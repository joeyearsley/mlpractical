{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-da5671615c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialising data providers...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mvalid_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtest_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/dataset.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dset, batch_size, max_num_batches, max_num_examples, randomize, augmentation, rng, conv_reshape)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_num_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_num_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mbufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextrasize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                     \u001b[0mreadsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_read_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Reached EOF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0muncompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0muncompress\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load Experiments/l1Experiment.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.001\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    ''' elif layer == 1:\n",
    "        odim = 600\n",
    "        model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=600, odim=600, irange=0.2, rng=rng))\n",
    "    elif layer == 2:\n",
    "        odim = 400\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        '''\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['l1F'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 6.759. Accuracy is 9.28%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 6.743. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 5.062. Accuracy is 85.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 4.562. Accuracy is 92.61%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 63 seconds. Training speed 876 pps. Validation speed 1661 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 4.605. Accuracy is 92.07%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 4.563. Accuracy is 93.39%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 63 seconds. Training speed 869 pps. Validation speed 1818 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 4.592. Accuracy is 93.55%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 4.562. Accuracy is 94.69%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 61 seconds. Training speed 896 pps. Validation speed 1816 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 4.593. Accuracy is 94.47%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 4.575. Accuracy is 95.41%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 62 seconds. Training speed 889 pps. Validation speed 1798 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 4.598. Accuracy is 95.29%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 4.585. Accuracy is 96.01%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 61 seconds. Training speed 904 pps. Validation speed 1832 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 4.608. Accuracy is 95.96%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 4.606. Accuracy is 96.04%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 63 seconds. Training speed 868 pps. Validation speed 1774 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 4.620. Accuracy is 96.43%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 4.621. Accuracy is 96.59%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 65 seconds. Training speed 849 pps. Validation speed 1732 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 4.634. Accuracy is 96.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 4.647. Accuracy is 96.55%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 62 seconds. Training speed 887 pps. Validation speed 1802 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 4.647. Accuracy is 97.13%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 4.653. Accuracy is 96.98%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 62 seconds. Training speed 895 pps. Validation speed 1767 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 4.661. Accuracy is 97.45%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 4.674. Accuracy is 97.07%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 62 seconds. Training speed 889 pps. Validation speed 1824 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 4.676. Accuracy is 97.58%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 4.691. Accuracy is 97.23%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 61 seconds. Training speed 894 pps. Validation speed 1823 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 4.689. Accuracy is 97.81%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 4.709. Accuracy is 97.12%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 61 seconds. Training speed 894 pps. Validation speed 1815 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 4.703. Accuracy is 98.00%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 4.727. Accuracy is 97.31%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 62 seconds. Training speed 890 pps. Validation speed 1831 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 4.717. Accuracy is 98.19%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 4.740. Accuracy is 97.42%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 65 seconds. Training speed 848 pps. Validation speed 1741 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 4.729. Accuracy is 98.35%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 4.756. Accuracy is 97.55%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 66 seconds. Training speed 833 pps. Validation speed 1792 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 4.742. Accuracy is 98.46%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 4.769. Accuracy is 97.53%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 64 seconds. Training speed 856 pps. Validation speed 1756 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 4.754. Accuracy is 98.52%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 4.783. Accuracy is 97.62%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 64 seconds. Training speed 852 pps. Validation speed 1793 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 4.766. Accuracy is 98.64%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 4.800. Accuracy is 97.65%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 63 seconds. Training speed 866 pps. Validation speed 1770 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 4.778. Accuracy is 98.77%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 4.813. Accuracy is 97.60%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 64 seconds. Training speed 858 pps. Validation speed 1664 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 4.788. Accuracy is 98.88%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 4.823. Accuracy is 97.64%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 64 seconds. Training speed 854 pps. Validation speed 1816 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 4.799. Accuracy is 98.88%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 4.839. Accuracy is 97.55%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 63 seconds. Training speed 873 pps. Validation speed 1741 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 4.809. Accuracy is 99.04%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 4.847. Accuracy is 97.61%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 65 seconds. Training speed 846 pps. Validation speed 1833 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 4.819. Accuracy is 99.09%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 4.861. Accuracy is 97.55%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 62 seconds. Training speed 889 pps. Validation speed 1781 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 4.828. Accuracy is 99.15%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 4.870. Accuracy is 97.63%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 63 seconds. Training speed 876 pps. Validation speed 1810 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 4.837. Accuracy is 99.27%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 4.879. Accuracy is 97.70%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 61 seconds. Training speed 898 pps. Validation speed 1846 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 4.845. Accuracy is 99.32%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 4.893. Accuracy is 97.67%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 62 seconds. Training speed 889 pps. Validation speed 1818 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 4.854. Accuracy is 99.37%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 4.898. Accuracy is 97.76%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 62 seconds. Training speed 892 pps. Validation speed 1827 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 4.861. Accuracy is 99.41%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 4.908. Accuracy is 97.76%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 63 seconds. Training speed 877 pps. Validation speed 1805 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 4.869. Accuracy is 99.47%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 4.916. Accuracy is 97.76%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 62 seconds. Training speed 888 pps. Validation speed 1807 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 4.876. Accuracy is 99.53%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 4.924. Accuracy is 97.79%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 61 seconds. Training speed 896 pps. Validation speed 1807 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 4.883. Accuracy is 99.54%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 4.931. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 61 seconds. Training speed 900 pps. Validation speed 1791 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 4.889. Accuracy is 99.60%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 4.940. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 61 seconds. Training speed 897 pps. Validation speed 1803 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 4.895. Accuracy is 99.61%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 4.946. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 61 seconds. Training speed 896 pps. Validation speed 1816 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 4.900. Accuracy is 99.69%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 4.952. Accuracy is 97.71%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 61 seconds. Training speed 901 pps. Validation speed 1753 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 4.906. Accuracy is 99.70%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 4.959. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 62 seconds. Training speed 892 pps. Validation speed 1816 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 4.911. Accuracy is 99.73%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 4.964. Accuracy is 97.81%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 64 seconds. Training speed 849 pps. Validation speed 1819 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 4.915. Accuracy is 99.74%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 4.968. Accuracy is 97.92%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 61 seconds. Training speed 895 pps. Validation speed 1812 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 4.920. Accuracy is 99.76%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 4.974. Accuracy is 97.86%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 61 seconds. Training speed 895 pps. Validation speed 1822 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 4.923. Accuracy is 99.81%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 4.978. Accuracy is 97.92%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 62 seconds. Training speed 891 pps. Validation speed 1821 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 4.927. Accuracy is 99.78%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 4.981. Accuracy is 97.89%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 65 seconds. Training speed 846 pps. Validation speed 1698 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 4.930. Accuracy is 99.80%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 4.986. Accuracy is 97.91%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 64 seconds. Training speed 864 pps. Validation speed 1778 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 4.933. Accuracy is 99.85%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 4.990. Accuracy is 97.90%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 64 seconds. Training speed 857 pps. Validation speed 1832 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 4.936. Accuracy is 99.85%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 4.993. Accuracy is 97.91%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 62 seconds. Training speed 878 pps. Validation speed 1810 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 4.939. Accuracy is 99.85%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 4.995. Accuracy is 97.94%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 64 seconds. Training speed 861 pps. Validation speed 1769 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 4.941. Accuracy is 99.87%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 4.998. Accuracy is 97.96%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 64 seconds. Training speed 854 pps. Validation speed 1778 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 4.943. Accuracy is 99.89%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 5.001. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 65 seconds. Training speed 848 pps. Validation speed 1791 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 4.944. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 5.004. Accuracy is 97.89%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 65 seconds. Training speed 848 pps. Validation speed 1762 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 4.946. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 5.004. Accuracy is 98.02%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 65 seconds. Training speed 851 pps. Validation speed 1718 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 4.947. Accuracy is 99.91%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 5.006. Accuracy is 97.95%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 64 seconds. Training speed 861 pps. Validation speed 1784 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 4.948. Accuracy is 99.93%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 5.007. Accuracy is 97.94%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 64 seconds. Training speed 851 pps. Validation speed 1807 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 98.01 %, cost (ce) is 0.062\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 8.016. Accuracy is 9.62%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 8.015. Accuracy is 9.69%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 6.376. Accuracy is 81.03%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 5.963. Accuracy is 91.80%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 81 seconds. Training speed 673 pps. Validation speed 1510 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 6.018. Accuracy is 91.14%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 5.954. Accuracy is 93.35%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 84 seconds. Training speed 646 pps. Validation speed 1507 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 6.004. Accuracy is 92.99%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 5.971. Accuracy is 94.22%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 83 seconds. Training speed 653 pps. Validation speed 1493 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 6.004. Accuracy is 94.11%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 5.990. Accuracy is 94.69%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 81 seconds. Training speed 673 pps. Validation speed 1507 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 6.013. Accuracy is 94.87%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 6.002. Accuracy is 95.58%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 83 seconds. Training speed 653 pps. Validation speed 1474 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 6.024. Accuracy is 95.64%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 6.019. Accuracy is 95.92%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 82 seconds. Training speed 663 pps. Validation speed 1490 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 6.038. Accuracy is 96.07%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 6.033. Accuracy is 96.63%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 83 seconds. Training speed 655 pps. Validation speed 1458 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 6.053. Accuracy is 96.48%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 6.051. Accuracy is 96.74%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 84 seconds. Training speed 649 pps. Validation speed 1448 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 6.067. Accuracy is 96.81%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 6.071. Accuracy is 96.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 83 seconds. Training speed 652 pps. Validation speed 1481 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 6.082. Accuracy is 97.07%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 6.100. Accuracy is 96.65%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 85 seconds. Training speed 644 pps. Validation speed 1412 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 6.096. Accuracy is 97.39%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 6.114. Accuracy is 96.88%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 80 seconds. Training speed 678 pps. Validation speed 1505 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 6.110. Accuracy is 97.61%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 6.124. Accuracy is 97.20%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 79 seconds. Training speed 689 pps. Validation speed 1510 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 6.123. Accuracy is 97.77%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 6.146. Accuracy is 97.23%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 80 seconds. Training speed 684 pps. Validation speed 1483 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 6.137. Accuracy is 98.00%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 6.156. Accuracy is 97.44%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 81 seconds. Training speed 673 pps. Validation speed 1470 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 6.148. Accuracy is 98.18%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 6.176. Accuracy is 97.29%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 84 seconds. Training speed 648 pps. Validation speed 1408 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 6.160. Accuracy is 98.29%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 6.185. Accuracy is 97.59%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 80 seconds. Training speed 681 pps. Validation speed 1516 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 6.172. Accuracy is 98.43%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 6.206. Accuracy is 97.28%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 80 seconds. Training speed 680 pps. Validation speed 1517 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 6.183. Accuracy is 98.53%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 6.218. Accuracy is 97.39%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 79 seconds. Training speed 685 pps. Validation speed 1532 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 6.193. Accuracy is 98.65%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 6.237. Accuracy is 97.31%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 80 seconds. Training speed 680 pps. Validation speed 1545 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 6.203. Accuracy is 98.77%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 6.236. Accuracy is 97.65%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 81 seconds. Training speed 670 pps. Validation speed 1464 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 6.213. Accuracy is 98.84%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 6.251. Accuracy is 97.63%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 81 seconds. Training speed 675 pps. Validation speed 1498 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 6.222. Accuracy is 98.97%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 6.260. Accuracy is 97.76%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 81 seconds. Training speed 671 pps. Validation speed 1521 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 6.230. Accuracy is 99.06%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 6.271. Accuracy is 97.74%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 82 seconds. Training speed 659 pps. Validation speed 1517 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 6.238. Accuracy is 99.11%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 6.279. Accuracy is 97.83%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 79 seconds. Training speed 687 pps. Validation speed 1530 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 6.245. Accuracy is 99.21%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 6.290. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 80 seconds. Training speed 683 pps. Validation speed 1507 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 6.252. Accuracy is 99.29%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 6.299. Accuracy is 97.80%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 80 seconds. Training speed 681 pps. Validation speed 1516 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 6.258. Accuracy is 99.36%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 6.309. Accuracy is 97.68%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 80 seconds. Training speed 684 pps. Validation speed 1511 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 6.264. Accuracy is 99.39%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 6.311. Accuracy is 97.92%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 81 seconds. Training speed 677 pps. Validation speed 1487 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 6.269. Accuracy is 99.49%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 6.320. Accuracy is 97.86%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 79 seconds. Training speed 686 pps. Validation speed 1521 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 6.274. Accuracy is 99.51%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 6.323. Accuracy is 97.99%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 79 seconds. Training speed 689 pps. Validation speed 1509 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 6.278. Accuracy is 99.53%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 6.330. Accuracy is 97.94%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 80 seconds. Training speed 685 pps. Validation speed 1527 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 6.282. Accuracy is 99.59%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 6.332. Accuracy is 97.97%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 80 seconds. Training speed 686 pps. Validation speed 1510 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 6.285. Accuracy is 99.64%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 6.339. Accuracy is 97.95%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 80 seconds. Training speed 680 pps. Validation speed 1532 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 6.287. Accuracy is 99.68%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 6.344. Accuracy is 97.89%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 80 seconds. Training speed 687 pps. Validation speed 1486 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 6.290. Accuracy is 99.70%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 6.352. Accuracy is 97.67%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 78 seconds. Training speed 696 pps. Validation speed 1517 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 6.291. Accuracy is 99.77%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 6.347. Accuracy is 98.02%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 80 seconds. Training speed 686 pps. Validation speed 1500 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 6.292. Accuracy is 99.77%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 6.351. Accuracy is 97.93%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 80 seconds. Training speed 681 pps. Validation speed 1511 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 6.293. Accuracy is 99.81%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 6.349. Accuracy is 98.02%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 79 seconds. Training speed 695 pps. Validation speed 1510 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 6.292. Accuracy is 99.85%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 6.352. Accuracy is 97.97%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 80 seconds. Training speed 684 pps. Validation speed 1522 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 6.292. Accuracy is 99.86%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 6.351. Accuracy is 97.98%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 79 seconds. Training speed 686 pps. Validation speed 1523 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 6.291. Accuracy is 99.87%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 6.350. Accuracy is 98.07%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 80 seconds. Training speed 679 pps. Validation speed 1501 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 6.290. Accuracy is 99.88%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 6.352. Accuracy is 98.04%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 79 seconds. Training speed 688 pps. Validation speed 1525 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 6.288. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 6.349. Accuracy is 98.04%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 80 seconds. Training speed 686 pps. Validation speed 1493 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 6.286. Accuracy is 99.91%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 6.347. Accuracy is 98.06%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 79 seconds. Training speed 689 pps. Validation speed 1533 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 6.283. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 6.346. Accuracy is 98.00%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 79 seconds. Training speed 692 pps. Validation speed 1504 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 6.280. Accuracy is 99.93%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 6.344. Accuracy is 98.03%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 80 seconds. Training speed 677 pps. Validation speed 1536 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 6.277. Accuracy is 99.95%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 6.341. Accuracy is 98.02%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 79 seconds. Training speed 688 pps. Validation speed 1482 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 6.274. Accuracy is 99.93%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 6.338. Accuracy is 98.07%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 79 seconds. Training speed 691 pps. Validation speed 1526 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 6.270. Accuracy is 99.95%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 6.335. Accuracy is 98.09%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 80 seconds. Training speed 680 pps. Validation speed 1517 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 6.265. Accuracy is 99.96%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 6.330. Accuracy is 98.06%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 80 seconds. Training speed 680 pps. Validation speed 1538 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 98.05 %, cost (ce) is 0.065\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 6.894. Accuracy is 9.68%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 6.899. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 5.505. Accuracy is 64.69%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 4.756. Accuracy is 89.35%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 62 seconds. Training speed 888 pps. Validation speed 1763 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 4.806. Accuracy is 89.80%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 4.730. Accuracy is 92.24%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 63 seconds. Training speed 870 pps. Validation speed 1660 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 4.785. Accuracy is 92.19%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 4.734. Accuracy is 93.60%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 63 seconds. Training speed 879 pps. Validation speed 1750 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 4.784. Accuracy is 93.35%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 4.747. Accuracy is 94.96%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 63 seconds. Training speed 877 pps. Validation speed 1750 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 4.796. Accuracy is 94.24%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 4.766. Accuracy is 95.21%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 63 seconds. Training speed 868 pps. Validation speed 1741 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 4.808. Accuracy is 95.07%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 4.807. Accuracy is 94.99%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 63 seconds. Training speed 874 pps. Validation speed 1729 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 4.826. Accuracy is 95.55%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 4.810. Accuracy is 96.33%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 63 seconds. Training speed 877 pps. Validation speed 1763 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 4.840. Accuracy is 96.00%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 4.846. Accuracy is 96.07%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 63 seconds. Training speed 866 pps. Validation speed 1773 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 4.857. Accuracy is 96.50%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 4.867. Accuracy is 96.19%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 64 seconds. Training speed 862 pps. Validation speed 1768 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 4.876. Accuracy is 96.80%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 4.882. Accuracy is 96.87%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 62 seconds. Training speed 884 pps. Validation speed 1781 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 4.891. Accuracy is 97.08%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 4.904. Accuracy is 96.64%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 63 seconds. Training speed 878 pps. Validation speed 1758 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 4.906. Accuracy is 97.42%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 4.920. Accuracy is 97.12%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 62 seconds. Training speed 881 pps. Validation speed 1743 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 4.923. Accuracy is 97.64%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 4.957. Accuracy is 96.63%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 63 seconds. Training speed 879 pps. Validation speed 1704 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 4.940. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 4.957. Accuracy is 97.39%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 63 seconds. Training speed 869 pps. Validation speed 1783 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 4.956. Accuracy is 97.91%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 4.976. Accuracy is 97.42%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 63 seconds. Training speed 870 pps. Validation speed 1753 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 4.970. Accuracy is 98.18%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 5.004. Accuracy is 97.32%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 63 seconds. Training speed 872 pps. Validation speed 1744 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 4.986. Accuracy is 98.25%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 5.021. Accuracy is 97.40%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 63 seconds. Training speed 876 pps. Validation speed 1777 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 4.999. Accuracy is 98.43%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 5.032. Accuracy is 97.49%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 63 seconds. Training speed 871 pps. Validation speed 1765 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 5.013. Accuracy is 98.56%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 5.051. Accuracy is 97.50%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 64 seconds. Training speed 862 pps. Validation speed 1749 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 5.024. Accuracy is 98.72%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 5.070. Accuracy is 97.47%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 62 seconds. Training speed 882 pps. Validation speed 1756 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 5.038. Accuracy is 98.90%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 5.089. Accuracy is 97.39%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 63 seconds. Training speed 873 pps. Validation speed 1722 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 5.050. Accuracy is 98.93%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 5.111. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 63 seconds. Training speed 869 pps. Validation speed 1771 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 5.060. Accuracy is 99.10%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 5.108. Accuracy is 97.57%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 63 seconds. Training speed 880 pps. Validation speed 1764 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 5.070. Accuracy is 99.13%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 5.126. Accuracy is 97.59%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 63 seconds. Training speed 870 pps. Validation speed 1778 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 5.080. Accuracy is 99.26%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 5.129. Accuracy is 97.91%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 62 seconds. Training speed 881 pps. Validation speed 1795 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 5.090. Accuracy is 99.26%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 5.139. Accuracy is 97.95%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 64 seconds. Training speed 860 pps. Validation speed 1724 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 5.097. Accuracy is 99.42%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 5.160. Accuracy is 97.63%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 63 seconds. Training speed 875 pps. Validation speed 1746 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 5.104. Accuracy is 99.48%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 5.173. Accuracy is 97.62%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 64 seconds. Training speed 859 pps. Validation speed 1727 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 5.110. Accuracy is 99.55%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 5.167. Accuracy is 98.02%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 64 seconds. Training speed 862 pps. Validation speed 1705 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 5.116. Accuracy is 99.61%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 5.176. Accuracy is 97.93%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 62 seconds. Training speed 894 pps. Validation speed 1778 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 5.119. Accuracy is 99.68%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 5.187. Accuracy is 97.95%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 63 seconds. Training speed 871 pps. Validation speed 1742 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 5.123. Accuracy is 99.71%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 5.263. Accuracy is 95.96%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 63 seconds. Training speed 870 pps. Validation speed 1767 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 5.128. Accuracy is 99.70%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 5.193. Accuracy is 97.98%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 63 seconds. Training speed 871 pps. Validation speed 1747 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 5.129. Accuracy is 99.79%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 5.225. Accuracy is 97.22%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 63 seconds. Training speed 868 pps. Validation speed 1730 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 5.132. Accuracy is 99.80%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 5.199. Accuracy is 98.01%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 64 seconds. Training speed 858 pps. Validation speed 1744 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 5.132. Accuracy is 99.87%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 5.201. Accuracy is 98.05%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 67 seconds. Training speed 821 pps. Validation speed 1551 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 5.132. Accuracy is 99.87%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 5.202. Accuracy is 98.06%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 69 seconds. Training speed 792 pps. Validation speed 1789 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 5.131. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 5.202. Accuracy is 98.07%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 63 seconds. Training speed 873 pps. Validation speed 1767 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 5.129. Accuracy is 99.91%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 5.205. Accuracy is 97.81%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 63 seconds. Training speed 871 pps. Validation speed 1711 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 5.128. Accuracy is 99.94%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 5.203. Accuracy is 98.08%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 63 seconds. Training speed 876 pps. Validation speed 1729 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 5.125. Accuracy is 99.93%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 5.200. Accuracy is 98.11%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 64 seconds. Training speed 863 pps. Validation speed 1742 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 5.121. Accuracy is 99.97%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 5.201. Accuracy is 97.97%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 63 seconds. Training speed 872 pps. Validation speed 1707 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 5.118. Accuracy is 99.95%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 5.195. Accuracy is 98.05%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 63 seconds. Training speed 879 pps. Validation speed 1754 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 5.114. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 5.191. Accuracy is 98.08%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 63 seconds. Training speed 870 pps. Validation speed 1747 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 5.109. Accuracy is 99.97%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 5.187. Accuracy is 98.06%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 63 seconds. Training speed 866 pps. Validation speed 1748 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 5.105. Accuracy is 99.96%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 5.187. Accuracy is 97.97%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 62 seconds. Training speed 881 pps. Validation speed 1755 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 5.100. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 5.180. Accuracy is 98.04%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 63 seconds. Training speed 871 pps. Validation speed 1754 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 5.094. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 5.172. Accuracy is 98.10%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 63 seconds. Training speed 872 pps. Validation speed 1788 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 5.088. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 5.167. Accuracy is 98.10%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 63 seconds. Training speed 874 pps. Validation speed 1788 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 5.082. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 5.161. Accuracy is 98.09%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 64 seconds. Training speed 864 pps. Validation speed 1752 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.98 %, cost (ce) is 0.075\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 5.721. Accuracy is 9.86%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 5.719. Accuracy is 9.91%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 5.133. Accuracy is 43.80%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 4.367. Accuracy is 72.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 54 seconds. Training speed 1022 pps. Validation speed 1906 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 4.076. Accuracy is 87.14%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 4.012. Accuracy is 89.33%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 55 seconds. Training speed 996 pps. Validation speed 1927 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 4.017. Accuracy is 90.78%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 3.952. Accuracy is 92.68%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 55 seconds. Training speed 1003 pps. Validation speed 1920 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 4.013. Accuracy is 92.51%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 3.994. Accuracy is 93.22%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 56 seconds. Training speed 991 pps. Validation speed 1908 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 4.020. Accuracy is 93.71%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 3.980. Accuracy is 95.13%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 55 seconds. Training speed 997 pps. Validation speed 1882 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 4.034. Accuracy is 94.57%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 4.014. Accuracy is 95.37%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 55 seconds. Training speed 1009 pps. Validation speed 1884 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 4.051. Accuracy is 95.25%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 4.034. Accuracy is 95.85%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 56 seconds. Training speed 987 pps. Validation speed 1873 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 4.066. Accuracy is 95.89%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 4.060. Accuracy is 96.09%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 55 seconds. Training speed 1011 pps. Validation speed 1903 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 4.087. Accuracy is 96.17%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 4.080. Accuracy is 96.60%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 55 seconds. Training speed 1002 pps. Validation speed 1868 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 4.100. Accuracy is 96.80%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 4.102. Accuracy is 96.76%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 55 seconds. Training speed 1003 pps. Validation speed 1843 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 4.120. Accuracy is 96.94%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 4.137. Accuracy is 96.72%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 55 seconds. Training speed 1001 pps. Validation speed 1870 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 4.138. Accuracy is 97.20%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 4.144. Accuracy is 97.19%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 55 seconds. Training speed 1014 pps. Validation speed 1882 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 4.154. Accuracy is 97.62%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 4.181. Accuracy is 96.93%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 55 seconds. Training speed 1003 pps. Validation speed 1862 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 4.171. Accuracy is 97.72%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 4.195. Accuracy is 97.11%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 55 seconds. Training speed 1008 pps. Validation speed 1885 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 4.187. Accuracy is 97.91%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 4.210. Accuracy is 97.29%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 55 seconds. Training speed 996 pps. Validation speed 1884 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 4.203. Accuracy is 98.10%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 4.227. Accuracy is 97.56%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 56 seconds. Training speed 996 pps. Validation speed 1885 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 4.219. Accuracy is 98.22%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 4.252. Accuracy is 97.36%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 56 seconds. Training speed 990 pps. Validation speed 1902 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 4.235. Accuracy is 98.34%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 4.271. Accuracy is 97.39%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 56 seconds. Training speed 987 pps. Validation speed 1892 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 4.250. Accuracy is 98.46%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 4.292. Accuracy is 97.51%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 55 seconds. Training speed 1002 pps. Validation speed 1873 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 4.263. Accuracy is 98.68%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 4.306. Accuracy is 97.64%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 55 seconds. Training speed 998 pps. Validation speed 1872 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 4.276. Accuracy is 98.79%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 4.323. Accuracy is 97.53%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 55 seconds. Training speed 1004 pps. Validation speed 1912 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 4.287. Accuracy is 98.94%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 4.341. Accuracy is 97.51%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 55 seconds. Training speed 1001 pps. Validation speed 1886 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 4.304. Accuracy is 98.86%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 4.370. Accuracy is 97.19%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 55 seconds. Training speed 1016 pps. Validation speed 1886 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 4.316. Accuracy is 99.04%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 4.379. Accuracy is 97.36%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 56 seconds. Training speed 991 pps. Validation speed 1875 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 4.324. Accuracy is 99.23%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 4.383. Accuracy is 97.71%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 56 seconds. Training speed 994 pps. Validation speed 1910 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 4.333. Accuracy is 99.30%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 4.397. Accuracy is 97.74%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 55 seconds. Training speed 1004 pps. Validation speed 1925 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 4.340. Accuracy is 99.42%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 4.406. Accuracy is 97.84%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 56 seconds. Training speed 991 pps. Validation speed 1876 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 4.450. Accuracy is 98.43%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 4.428. Accuracy is 97.73%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 55 seconds. Training speed 1007 pps. Validation speed 1879 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 4.379. Accuracy is 99.46%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 4.447. Accuracy is 97.47%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 56 seconds. Training speed 986 pps. Validation speed 1882 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 4.382. Accuracy is 99.62%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 4.452. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 56 seconds. Training speed 979 pps. Validation speed 1861 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 4.387. Accuracy is 99.64%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 4.456. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 55 seconds. Training speed 1002 pps. Validation speed 1915 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 4.388. Accuracy is 99.79%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 4.467. Accuracy is 97.68%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 55 seconds. Training speed 1000 pps. Validation speed 1937 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 4.390. Accuracy is 99.80%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 4.468. Accuracy is 97.83%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 55 seconds. Training speed 1010 pps. Validation speed 1874 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 4.393. Accuracy is 99.79%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 4.485. Accuracy is 97.56%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 55 seconds. Training speed 1003 pps. Validation speed 1853 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 4.492. Accuracy is 98.73%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 4.492. Accuracy is 97.72%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 56 seconds. Training speed 988 pps. Validation speed 1877 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 4.433. Accuracy is 99.63%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 4.500. Accuracy is 97.69%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 55 seconds. Training speed 1006 pps. Validation speed 1903 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 4.430. Accuracy is 99.81%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 4.505. Accuracy is 97.88%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 55 seconds. Training speed 1003 pps. Validation speed 1916 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 4.429. Accuracy is 99.89%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 4.507. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 56 seconds. Training speed 997 pps. Validation speed 1827 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 4.428. Accuracy is 99.91%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 4.508. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 56 seconds. Training speed 995 pps. Validation speed 1861 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 4.426. Accuracy is 99.93%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 4.509. Accuracy is 97.81%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 55 seconds. Training speed 1001 pps. Validation speed 1911 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 4.423. Accuracy is 99.95%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 4.506. Accuracy is 97.88%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 56 seconds. Training speed 988 pps. Validation speed 1894 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 4.420. Accuracy is 99.95%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 4.505. Accuracy is 97.92%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 55 seconds. Training speed 1007 pps. Validation speed 1868 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 4.416. Accuracy is 99.97%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 4.506. Accuracy is 97.81%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 56 seconds. Training speed 998 pps. Validation speed 1853 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 4.412. Accuracy is 99.97%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 4.500. Accuracy is 97.84%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 55 seconds. Training speed 999 pps. Validation speed 1892 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 4.407. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 4.495. Accuracy is 97.90%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 55 seconds. Training speed 1001 pps. Validation speed 1881 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 4.402. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 4.491. Accuracy is 97.89%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 55 seconds. Training speed 1005 pps. Validation speed 1876 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 4.397. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 4.487. Accuracy is 97.92%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 56 seconds. Training speed 989 pps. Validation speed 1868 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 4.391. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 4.484. Accuracy is 97.86%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 56 seconds. Training speed 996 pps. Validation speed 1879 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 4.386. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 4.477. Accuracy is 97.92%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 55 seconds. Training speed 1010 pps. Validation speed 1883 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 4.379. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 4.471. Accuracy is 97.91%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 56 seconds. Training speed 993 pps. Validation speed 1876 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.82 %, cost (ce) is 0.088\n",
      "INFO:root:Saving Data\n"
     ]
    }
   ],
   "source": [
    "# %load Experiments/l2Experiment.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.001\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['l2F'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout fixed Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.570. Accuracy is 9.28%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.969. Accuracy is 73.68%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.329. Accuracy is 90.79%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 66 seconds. Training speed 824 pps. Validation speed 1851 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.543. Accuracy is 82.75%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.291. Accuracy is 91.78%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 68 seconds. Training speed 808 pps. Validation speed 1754 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.487. Accuracy is 84.79%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.255. Accuracy is 93.42%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 68 seconds. Training speed 804 pps. Validation speed 1845 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.439. Accuracy is 86.40%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.240. Accuracy is 93.20%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 67 seconds. Training speed 813 pps. Validation speed 1822 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.406. Accuracy is 87.37%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.213. Accuracy is 94.14%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 67 seconds. Training speed 810 pps. Validation speed 1839 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.378. Accuracy is 88.29%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.198. Accuracy is 94.62%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 66 seconds. Training speed 821 pps. Validation speed 1830 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.356. Accuracy is 88.95%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.183. Accuracy is 95.14%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 67 seconds. Training speed 807 pps. Validation speed 1832 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.336. Accuracy is 89.63%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.170. Accuracy is 95.48%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 67 seconds. Training speed 812 pps. Validation speed 1825 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.317. Accuracy is 90.19%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.158. Accuracy is 95.93%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 67 seconds. Training speed 806 pps. Validation speed 1840 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.304. Accuracy is 90.63%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.153. Accuracy is 95.77%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 67 seconds. Training speed 811 pps. Validation speed 1838 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.288. Accuracy is 91.10%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.144. Accuracy is 96.11%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 68 seconds. Training speed 806 pps. Validation speed 1795 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.277. Accuracy is 91.37%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.138. Accuracy is 96.26%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 66 seconds. Training speed 825 pps. Validation speed 1819 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.267. Accuracy is 91.73%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.135. Accuracy is 96.38%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 67 seconds. Training speed 817 pps. Validation speed 1822 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.262. Accuracy is 91.96%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.129. Accuracy is 96.38%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 66 seconds. Training speed 824 pps. Validation speed 1808 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.254. Accuracy is 92.16%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.126. Accuracy is 96.54%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 67 seconds. Training speed 816 pps. Validation speed 1814 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.245. Accuracy is 92.45%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.123. Accuracy is 96.58%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 66 seconds. Training speed 825 pps. Validation speed 1846 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.239. Accuracy is 92.59%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.118. Accuracy is 96.81%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 67 seconds. Training speed 813 pps. Validation speed 1836 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.233. Accuracy is 92.84%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.115. Accuracy is 96.91%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 67 seconds. Training speed 808 pps. Validation speed 1836 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.231. Accuracy is 92.83%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.115. Accuracy is 96.77%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 65 seconds. Training speed 835 pps. Validation speed 1818 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.222. Accuracy is 93.10%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.111. Accuracy is 96.88%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 67 seconds. Training speed 810 pps. Validation speed 1807 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.217. Accuracy is 93.27%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.110. Accuracy is 96.86%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 67 seconds. Training speed 814 pps. Validation speed 1829 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.214. Accuracy is 93.31%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.105. Accuracy is 97.09%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 68 seconds. Training speed 799 pps. Validation speed 1826 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.212. Accuracy is 93.41%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.106. Accuracy is 96.94%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 67 seconds. Training speed 814 pps. Validation speed 1840 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.207. Accuracy is 93.53%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.101. Accuracy is 97.23%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 66 seconds. Training speed 820 pps. Validation speed 1826 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.202. Accuracy is 93.65%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.100. Accuracy is 97.19%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 66 seconds. Training speed 829 pps. Validation speed 1826 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.199. Accuracy is 93.82%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.101. Accuracy is 97.14%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 66 seconds. Training speed 820 pps. Validation speed 1824 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.197. Accuracy is 93.88%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.098. Accuracy is 97.32%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 66 seconds. Training speed 830 pps. Validation speed 1807 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.191. Accuracy is 93.97%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.094. Accuracy is 97.32%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 67 seconds. Training speed 818 pps. Validation speed 1813 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.190. Accuracy is 94.09%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.096. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 67 seconds. Training speed 809 pps. Validation speed 1799 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.187. Accuracy is 94.10%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.093. Accuracy is 97.56%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 67 seconds. Training speed 818 pps. Validation speed 1820 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.185. Accuracy is 94.25%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.094. Accuracy is 97.29%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 67 seconds. Training speed 813 pps. Validation speed 1817 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.182. Accuracy is 94.26%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.092. Accuracy is 97.38%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 67 seconds. Training speed 812 pps. Validation speed 1810 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.181. Accuracy is 94.31%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.091. Accuracy is 97.40%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 67 seconds. Training speed 810 pps. Validation speed 1818 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.180. Accuracy is 94.39%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.090. Accuracy is 97.37%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 65 seconds. Training speed 836 pps. Validation speed 1810 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.174. Accuracy is 94.49%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.090. Accuracy is 97.37%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 67 seconds. Training speed 811 pps. Validation speed 1850 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.171. Accuracy is 94.63%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.090. Accuracy is 97.40%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 67 seconds. Training speed 807 pps. Validation speed 1800 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.171. Accuracy is 94.64%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.089. Accuracy is 97.31%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 67 seconds. Training speed 813 pps. Validation speed 1811 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.170. Accuracy is 94.65%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.086. Accuracy is 97.54%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 67 seconds. Training speed 806 pps. Validation speed 1827 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.168. Accuracy is 94.78%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.086. Accuracy is 97.52%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 67 seconds. Training speed 816 pps. Validation speed 1819 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.164. Accuracy is 94.82%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.083. Accuracy is 97.60%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 65 seconds. Training speed 836 pps. Validation speed 1810 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.165. Accuracy is 94.74%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.086. Accuracy is 97.59%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 67 seconds. Training speed 812 pps. Validation speed 1822 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.162. Accuracy is 94.97%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.086. Accuracy is 97.59%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 66 seconds. Training speed 822 pps. Validation speed 1804 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.161. Accuracy is 94.85%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.083. Accuracy is 97.63%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 66 seconds. Training speed 829 pps. Validation speed 1839 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.159. Accuracy is 95.01%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.084. Accuracy is 97.57%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 67 seconds. Training speed 815 pps. Validation speed 1830 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.159. Accuracy is 94.95%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.081. Accuracy is 97.66%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 67 seconds. Training speed 817 pps. Validation speed 1831 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.157. Accuracy is 95.06%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.082. Accuracy is 97.60%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 67 seconds. Training speed 816 pps. Validation speed 1799 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.153. Accuracy is 95.11%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.083. Accuracy is 97.49%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 65 seconds. Training speed 833 pps. Validation speed 1839 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.153. Accuracy is 95.12%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.082. Accuracy is 97.58%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 67 seconds. Training speed 810 pps. Validation speed 1837 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.154. Accuracy is 95.09%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.079. Accuracy is 97.70%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 66 seconds. Training speed 830 pps. Validation speed 1824 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.152. Accuracy is 95.17%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.080. Accuracy is 97.69%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 66 seconds. Training speed 827 pps. Validation speed 1818 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.46 %, cost (ce) is 0.082\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.471. Accuracy is 9.62%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.470. Accuracy is 9.69%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.263. Accuracy is 59.80%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.399. Accuracy is 88.68%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 83 seconds. Training speed 650 pps. Validation speed 1561 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.696. Accuracy is 77.00%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.334. Accuracy is 90.43%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 85 seconds. Training speed 634 pps. Validation speed 1540 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.602. Accuracy is 80.68%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.294. Accuracy is 91.45%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 86 seconds. Training speed 633 pps. Validation speed 1525 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.554. Accuracy is 82.22%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.263. Accuracy is 92.61%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 85 seconds. Training speed 637 pps. Validation speed 1546 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.513. Accuracy is 83.57%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.249. Accuracy is 92.76%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 84 seconds. Training speed 646 pps. Validation speed 1533 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.482. Accuracy is 84.58%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.228. Accuracy is 93.36%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 84 seconds. Training speed 644 pps. Validation speed 1522 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.456. Accuracy is 85.50%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.217. Accuracy is 93.69%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 84 seconds. Training speed 647 pps. Validation speed 1554 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.431. Accuracy is 86.21%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.201. Accuracy is 94.13%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 84 seconds. Training speed 643 pps. Validation speed 1525 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.412. Accuracy is 86.80%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.195. Accuracy is 94.30%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 84 seconds. Training speed 644 pps. Validation speed 1562 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.392. Accuracy is 87.44%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.184. Accuracy is 94.60%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 85 seconds. Training speed 640 pps. Validation speed 1541 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.380. Accuracy is 87.92%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.172. Accuracy is 95.09%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 84 seconds. Training speed 645 pps. Validation speed 1565 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.365. Accuracy is 88.34%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.166. Accuracy is 95.24%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 83 seconds. Training speed 652 pps. Validation speed 1538 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.352. Accuracy is 88.82%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.157. Accuracy is 95.50%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 85 seconds. Training speed 637 pps. Validation speed 1560 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.344. Accuracy is 89.04%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.154. Accuracy is 95.49%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 85 seconds. Training speed 636 pps. Validation speed 1522 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.329. Accuracy is 89.52%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.148. Accuracy is 95.64%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 85 seconds. Training speed 639 pps. Validation speed 1558 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.325. Accuracy is 89.66%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.141. Accuracy is 95.90%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 84 seconds. Training speed 645 pps. Validation speed 1523 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.314. Accuracy is 90.06%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.142. Accuracy is 95.63%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 84 seconds. Training speed 642 pps. Validation speed 1542 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.302. Accuracy is 90.39%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.135. Accuracy is 96.03%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 84 seconds. Training speed 648 pps. Validation speed 1521 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.298. Accuracy is 90.46%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.129. Accuracy is 96.37%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 84 seconds. Training speed 647 pps. Validation speed 1538 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.291. Accuracy is 90.60%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.127. Accuracy is 96.32%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 85 seconds. Training speed 641 pps. Validation speed 1526 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.283. Accuracy is 90.99%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.123. Accuracy is 96.45%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 85 seconds. Training speed 638 pps. Validation speed 1519 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.277. Accuracy is 91.09%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.123. Accuracy is 96.31%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 85 seconds. Training speed 641 pps. Validation speed 1520 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.273. Accuracy is 91.28%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.119. Accuracy is 96.53%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 85 seconds. Training speed 637 pps. Validation speed 1551 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.268. Accuracy is 91.51%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.116. Accuracy is 96.60%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 85 seconds. Training speed 640 pps. Validation speed 1525 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.263. Accuracy is 91.53%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.115. Accuracy is 96.59%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 85 seconds. Training speed 638 pps. Validation speed 1531 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.254. Accuracy is 91.90%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.114. Accuracy is 96.62%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 85 seconds. Training speed 638 pps. Validation speed 1541 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.258. Accuracy is 91.76%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.113. Accuracy is 96.66%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 83 seconds. Training speed 651 pps. Validation speed 1548 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.248. Accuracy is 92.10%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.109. Accuracy is 96.70%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 85 seconds. Training speed 637 pps. Validation speed 1563 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.246. Accuracy is 92.22%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.105. Accuracy is 96.90%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 84 seconds. Training speed 643 pps. Validation speed 1542 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.242. Accuracy is 92.22%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.104. Accuracy is 97.06%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 84 seconds. Training speed 644 pps. Validation speed 1539 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.240. Accuracy is 92.39%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.104. Accuracy is 96.98%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 84 seconds. Training speed 648 pps. Validation speed 1516 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.236. Accuracy is 92.46%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.103. Accuracy is 97.00%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 84 seconds. Training speed 649 pps. Validation speed 1526 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.233. Accuracy is 92.56%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.100. Accuracy is 97.08%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 85 seconds. Training speed 640 pps. Validation speed 1531 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.232. Accuracy is 92.58%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.105. Accuracy is 96.93%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 83 seconds. Training speed 650 pps. Validation speed 1534 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.226. Accuracy is 92.76%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.098. Accuracy is 97.02%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 84 seconds. Training speed 645 pps. Validation speed 1543 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.223. Accuracy is 92.88%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.099. Accuracy is 97.06%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 84 seconds. Training speed 643 pps. Validation speed 1545 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.223. Accuracy is 92.84%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.097. Accuracy is 97.09%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 84 seconds. Training speed 644 pps. Validation speed 1555 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.218. Accuracy is 92.99%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.096. Accuracy is 97.13%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 84 seconds. Training speed 643 pps. Validation speed 1538 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.213. Accuracy is 93.21%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.094. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 84 seconds. Training speed 644 pps. Validation speed 1539 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.212. Accuracy is 93.30%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.093. Accuracy is 97.23%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 84 seconds. Training speed 646 pps. Validation speed 1563 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.209. Accuracy is 93.33%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.092. Accuracy is 97.29%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 84 seconds. Training speed 646 pps. Validation speed 1550 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.208. Accuracy is 93.32%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.093. Accuracy is 97.21%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 85 seconds. Training speed 640 pps. Validation speed 1546 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.203. Accuracy is 93.49%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.092. Accuracy is 97.20%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 85 seconds. Training speed 640 pps. Validation speed 1506 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.203. Accuracy is 93.47%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.093. Accuracy is 97.35%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 84 seconds. Training speed 646 pps. Validation speed 1551 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.205. Accuracy is 93.46%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.088. Accuracy is 97.36%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 85 seconds. Training speed 635 pps. Validation speed 1535 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.201. Accuracy is 93.59%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.092. Accuracy is 97.22%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 84 seconds. Training speed 644 pps. Validation speed 1527 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.201. Accuracy is 93.59%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.086. Accuracy is 97.45%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 84 seconds. Training speed 646 pps. Validation speed 1547 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.194. Accuracy is 93.70%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.090. Accuracy is 97.24%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 84 seconds. Training speed 643 pps. Validation speed 1554 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.195. Accuracy is 93.73%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.087. Accuracy is 97.36%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 84 seconds. Training speed 644 pps. Validation speed 1559 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.195. Accuracy is 93.78%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.087. Accuracy is 97.38%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 85 seconds. Training speed 638 pps. Validation speed 1533 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.08 %, cost (ce) is 0.088\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.667. Accuracy is 9.68%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.671. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.901. Accuracy is 32.30%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.788. Accuracy is 73.24%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 69 seconds. Training speed 788 pps. Validation speed 1760 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.003. Accuracy is 64.66%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.456. Accuracy is 86.56%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 70 seconds. Training speed 783 pps. Validation speed 1746 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.793. Accuracy is 73.27%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.387. Accuracy is 88.69%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 70 seconds. Training speed 782 pps. Validation speed 1766 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.698. Accuracy is 76.78%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.337. Accuracy is 90.36%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 69 seconds. Training speed 790 pps. Validation speed 1704 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.638. Accuracy is 79.16%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.300. Accuracy is 90.98%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 70 seconds. Training speed 782 pps. Validation speed 1765 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.590. Accuracy is 80.88%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.280. Accuracy is 91.79%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 69 seconds. Training speed 790 pps. Validation speed 1757 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.555. Accuracy is 82.28%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.256. Accuracy is 92.14%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 69 seconds. Training speed 790 pps. Validation speed 1758 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.524. Accuracy is 83.21%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.243. Accuracy is 92.20%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 70 seconds. Training speed 784 pps. Validation speed 1730 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.498. Accuracy is 84.21%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.234. Accuracy is 92.37%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 69 seconds. Training speed 794 pps. Validation speed 1754 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.473. Accuracy is 84.81%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.216. Accuracy is 93.35%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 69 seconds. Training speed 786 pps. Validation speed 1741 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.449. Accuracy is 85.83%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.206. Accuracy is 93.58%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 69 seconds. Training speed 788 pps. Validation speed 1730 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.440. Accuracy is 86.14%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.197. Accuracy is 93.90%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 69 seconds. Training speed 791 pps. Validation speed 1750 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.421. Accuracy is 86.67%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.188. Accuracy is 94.18%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 69 seconds. Training speed 795 pps. Validation speed 1749 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.406. Accuracy is 87.34%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.178. Accuracy is 94.60%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 68 seconds. Training speed 799 pps. Validation speed 1760 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.395. Accuracy is 87.65%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.176. Accuracy is 94.48%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 69 seconds. Training speed 793 pps. Validation speed 1773 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.384. Accuracy is 87.88%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.170. Accuracy is 94.74%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 69 seconds. Training speed 793 pps. Validation speed 1742 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.373. Accuracy is 88.35%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.163. Accuracy is 94.92%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 68 seconds. Training speed 798 pps. Validation speed 1754 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.363. Accuracy is 88.55%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.159. Accuracy is 95.00%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 69 seconds. Training speed 793 pps. Validation speed 1772 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.356. Accuracy is 88.86%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.157. Accuracy is 94.96%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 68 seconds. Training speed 797 pps. Validation speed 1766 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.348. Accuracy is 89.07%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.150. Accuracy is 95.31%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 69 seconds. Training speed 797 pps. Validation speed 1727 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.342. Accuracy is 89.35%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.146. Accuracy is 95.46%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 69 seconds. Training speed 794 pps. Validation speed 1745 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.334. Accuracy is 89.41%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.143. Accuracy is 95.59%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 68 seconds. Training speed 798 pps. Validation speed 1757 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.325. Accuracy is 89.84%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.145. Accuracy is 95.50%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 69 seconds. Training speed 790 pps. Validation speed 1764 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.320. Accuracy is 89.94%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.137. Accuracy is 95.77%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 69 seconds. Training speed 794 pps. Validation speed 1760 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.315. Accuracy is 90.07%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.134. Accuracy is 96.00%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 69 seconds. Training speed 793 pps. Validation speed 1747 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.306. Accuracy is 90.45%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.131. Accuracy is 95.98%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 69 seconds. Training speed 791 pps. Validation speed 1736 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.305. Accuracy is 90.54%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.131. Accuracy is 96.07%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 69 seconds. Training speed 790 pps. Validation speed 1759 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.298. Accuracy is 90.67%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.125. Accuracy is 96.18%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 69 seconds. Training speed 795 pps. Validation speed 1758 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.294. Accuracy is 90.77%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.122. Accuracy is 96.34%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 69 seconds. Training speed 790 pps. Validation speed 1762 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.290. Accuracy is 91.00%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.127. Accuracy is 96.17%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 70 seconds. Training speed 784 pps. Validation speed 1718 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.284. Accuracy is 91.08%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.117. Accuracy is 96.45%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 69 seconds. Training speed 791 pps. Validation speed 1740 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.282. Accuracy is 91.14%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.118. Accuracy is 96.44%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 69 seconds. Training speed 791 pps. Validation speed 1747 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.276. Accuracy is 91.36%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.117. Accuracy is 96.50%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 69 seconds. Training speed 788 pps. Validation speed 1747 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.272. Accuracy is 91.56%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.114. Accuracy is 96.49%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 69 seconds. Training speed 793 pps. Validation speed 1729 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.271. Accuracy is 91.57%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.116. Accuracy is 96.44%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 69 seconds. Training speed 793 pps. Validation speed 1774 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.267. Accuracy is 91.77%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.116. Accuracy is 96.39%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 69 seconds. Training speed 789 pps. Validation speed 1746 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.264. Accuracy is 91.70%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.111. Accuracy is 96.61%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 69 seconds. Training speed 786 pps. Validation speed 1720 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.262. Accuracy is 91.80%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.109. Accuracy is 96.67%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 68 seconds. Training speed 799 pps. Validation speed 1746 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.261. Accuracy is 91.81%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.108. Accuracy is 96.79%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 68 seconds. Training speed 796 pps. Validation speed 1770 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.255. Accuracy is 92.06%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.109. Accuracy is 96.78%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 69 seconds. Training speed 793 pps. Validation speed 1751 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.251. Accuracy is 92.12%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.105. Accuracy is 96.77%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 69 seconds. Training speed 784 pps. Validation speed 1763 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.247. Accuracy is 92.38%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.105. Accuracy is 96.73%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 69 seconds. Training speed 786 pps. Validation speed 1750 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.245. Accuracy is 92.35%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.105. Accuracy is 96.82%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 69 seconds. Training speed 789 pps. Validation speed 1744 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.244. Accuracy is 92.43%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.101. Accuracy is 97.01%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 69 seconds. Training speed 786 pps. Validation speed 1752 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.242. Accuracy is 92.49%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.102. Accuracy is 96.87%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 69 seconds. Training speed 792 pps. Validation speed 1760 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.241. Accuracy is 92.47%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.103. Accuracy is 96.83%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 68 seconds. Training speed 797 pps. Validation speed 1748 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.238. Accuracy is 92.57%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.101. Accuracy is 96.89%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 68 seconds. Training speed 801 pps. Validation speed 1729 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.238. Accuracy is 92.63%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.099. Accuracy is 97.01%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 69 seconds. Training speed 793 pps. Validation speed 1762 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.235. Accuracy is 92.68%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.098. Accuracy is 97.15%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 70 seconds. Training speed 784 pps. Validation speed 1738 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.233. Accuracy is 92.76%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.096. Accuracy is 97.14%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 69 seconds. Training speed 787 pps. Validation speed 1763 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 96.58 %, cost (ce) is 0.106\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.348. Accuracy is 9.86%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.345. Accuracy is 9.91%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.367. Accuracy is 10.89%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.284. Accuracy is 10.64%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 61 seconds. Training speed 892 pps. Validation speed 1918 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.264. Accuracy is 14.97%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.910. Accuracy is 33.97%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 62 seconds. Training speed 887 pps. Validation speed 1915 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.672. Accuracy is 38.01%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.933. Accuracy is 63.54%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 62 seconds. Training speed 884 pps. Validation speed 1894 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.195. Accuracy is 56.58%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.750. Accuracy is 71.10%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 61 seconds. Training speed 890 pps. Validation speed 1917 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.045. Accuracy is 63.28%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.616. Accuracy is 79.38%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 62 seconds. Training speed 885 pps. Validation speed 1945 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.928. Accuracy is 68.35%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.508. Accuracy is 85.89%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 61 seconds. Training speed 899 pps. Validation speed 1916 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.838. Accuracy is 72.00%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.450. Accuracy is 87.44%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 62 seconds. Training speed 883 pps. Validation speed 1928 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.772. Accuracy is 74.72%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.410. Accuracy is 88.22%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 62 seconds. Training speed 881 pps. Validation speed 1929 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.725. Accuracy is 76.62%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.372. Accuracy is 89.75%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 61 seconds. Training speed 890 pps. Validation speed 1931 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.673. Accuracy is 78.56%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.330. Accuracy is 90.72%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 61 seconds. Training speed 889 pps. Validation speed 1916 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.642. Accuracy is 79.60%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.304. Accuracy is 91.22%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 61 seconds. Training speed 892 pps. Validation speed 1974 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.603. Accuracy is 81.06%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.276. Accuracy is 92.19%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 61 seconds. Training speed 896 pps. Validation speed 1923 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.568. Accuracy is 82.31%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.255. Accuracy is 92.82%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 61 seconds. Training speed 890 pps. Validation speed 1936 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.546. Accuracy is 83.02%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.235. Accuracy is 93.22%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 62 seconds. Training speed 884 pps. Validation speed 1947 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.516. Accuracy is 84.07%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.224. Accuracy is 93.54%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 61 seconds. Training speed 891 pps. Validation speed 1905 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.492. Accuracy is 84.89%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.211. Accuracy is 93.86%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 62 seconds. Training speed 884 pps. Validation speed 1921 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.473. Accuracy is 85.47%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.203. Accuracy is 94.02%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 61 seconds. Training speed 901 pps. Validation speed 1935 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.460. Accuracy is 85.96%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.196. Accuracy is 94.06%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 62 seconds. Training speed 887 pps. Validation speed 1913 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.441. Accuracy is 86.47%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.183. Accuracy is 94.48%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 62 seconds. Training speed 888 pps. Validation speed 1915 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.430. Accuracy is 86.95%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.176. Accuracy is 94.75%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 62 seconds. Training speed 883 pps. Validation speed 1941 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.416. Accuracy is 87.41%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.171. Accuracy is 94.85%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 61 seconds. Training speed 887 pps. Validation speed 1945 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.409. Accuracy is 87.62%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.166. Accuracy is 95.03%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 62 seconds. Training speed 887 pps. Validation speed 1933 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.397. Accuracy is 87.90%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.159. Accuracy is 95.14%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 61 seconds. Training speed 890 pps. Validation speed 1920 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.385. Accuracy is 88.23%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.157. Accuracy is 95.27%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 62 seconds. Training speed 884 pps. Validation speed 1922 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.375. Accuracy is 88.64%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.151. Accuracy is 95.45%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 62 seconds. Training speed 883 pps. Validation speed 1938 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.367. Accuracy is 88.84%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.149. Accuracy is 95.49%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 61 seconds. Training speed 892 pps. Validation speed 1927 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.363. Accuracy is 89.11%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.148. Accuracy is 95.52%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 62 seconds. Training speed 886 pps. Validation speed 1900 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.354. Accuracy is 89.25%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.146. Accuracy is 95.45%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 61 seconds. Training speed 887 pps. Validation speed 1962 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.349. Accuracy is 89.35%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.142. Accuracy is 95.67%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 61 seconds. Training speed 897 pps. Validation speed 1907 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.344. Accuracy is 89.64%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.140. Accuracy is 95.63%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 61 seconds. Training speed 890 pps. Validation speed 1911 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.337. Accuracy is 89.83%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.134. Accuracy is 95.87%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 62 seconds. Training speed 883 pps. Validation speed 1913 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.331. Accuracy is 89.98%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.135. Accuracy is 95.79%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 61 seconds. Training speed 891 pps. Validation speed 1914 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.329. Accuracy is 90.06%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.130. Accuracy is 96.00%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 61 seconds. Training speed 889 pps. Validation speed 1932 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.320. Accuracy is 90.42%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.130. Accuracy is 96.00%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 61 seconds. Training speed 891 pps. Validation speed 1932 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.318. Accuracy is 90.53%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.128. Accuracy is 96.08%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 62 seconds. Training speed 887 pps. Validation speed 1920 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.314. Accuracy is 90.60%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.126. Accuracy is 96.06%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 62 seconds. Training speed 889 pps. Validation speed 1898 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.310. Accuracy is 90.63%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.122. Accuracy is 96.24%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 61 seconds. Training speed 894 pps. Validation speed 1910 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.308. Accuracy is 90.68%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.125. Accuracy is 96.01%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 62 seconds. Training speed 881 pps. Validation speed 1915 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.303. Accuracy is 90.92%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.120. Accuracy is 96.22%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 62 seconds. Training speed 888 pps. Validation speed 1908 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.298. Accuracy is 90.97%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.119. Accuracy is 96.34%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 61 seconds. Training speed 890 pps. Validation speed 1917 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.296. Accuracy is 90.97%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.121. Accuracy is 96.20%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 62 seconds. Training speed 879 pps. Validation speed 1952 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.291. Accuracy is 91.20%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.118. Accuracy is 96.27%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 62 seconds. Training speed 885 pps. Validation speed 1930 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.288. Accuracy is 91.36%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.115. Accuracy is 96.36%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 62 seconds. Training speed 886 pps. Validation speed 1944 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.289. Accuracy is 91.35%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.115. Accuracy is 96.31%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 61 seconds. Training speed 890 pps. Validation speed 1946 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.285. Accuracy is 91.33%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.114. Accuracy is 96.41%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 61 seconds. Training speed 891 pps. Validation speed 1968 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.282. Accuracy is 91.58%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.112. Accuracy is 96.48%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 62 seconds. Training speed 878 pps. Validation speed 1887 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.281. Accuracy is 91.58%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.113. Accuracy is 96.42%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 62 seconds. Training speed 885 pps. Validation speed 1923 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.277. Accuracy is 91.77%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.111. Accuracy is 96.55%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 62 seconds. Training speed 887 pps. Validation speed 1940 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.275. Accuracy is 91.78%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.108. Accuracy is 96.58%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 61 seconds. Training speed 892 pps. Validation speed 1919 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.272. Accuracy is 91.82%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.110. Accuracy is 96.53%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 61 seconds. Training speed 892 pps. Validation speed 1922 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 96.36 %, cost (ce) is 0.122\n",
      "INFO:root:Saving Data\n"
     ]
    }
   ],
   "source": [
    "# %load Experiments/dropNExperiment.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutFixed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = DropoutFixed(0.5, 0.5)\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)    \n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropNF'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout Annealed Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.570. Accuracy is 9.28%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.943. Accuracy is 73.85%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.322. Accuracy is 91.16%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 67 seconds. Training speed 807 pps. Validation speed 1817 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.535. Accuracy is 83.08%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.290. Accuracy is 91.65%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 66 seconds. Training speed 820 pps. Validation speed 1844 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.470. Accuracy is 85.38%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.255. Accuracy is 93.08%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 67 seconds. Training speed 811 pps. Validation speed 1839 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.429. Accuracy is 86.67%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.230. Accuracy is 93.79%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 66 seconds. Training speed 830 pps. Validation speed 1823 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.386. Accuracy is 88.04%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.208. Accuracy is 94.35%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 67 seconds. Training speed 808 pps. Validation speed 1830 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.357. Accuracy is 89.05%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.189. Accuracy is 94.97%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 68 seconds. Training speed 804 pps. Validation speed 1806 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.327. Accuracy is 90.02%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.176. Accuracy is 95.29%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 67 seconds. Training speed 806 pps. Validation speed 1832 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.306. Accuracy is 90.61%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.164. Accuracy is 95.53%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 67 seconds. Training speed 815 pps. Validation speed 1842 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.287. Accuracy is 91.18%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.150. Accuracy is 95.99%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 66 seconds. Training speed 830 pps. Validation speed 1807 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.270. Accuracy is 91.69%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.146. Accuracy is 95.94%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 65 seconds. Training speed 834 pps. Validation speed 1814 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.257. Accuracy is 92.11%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.135. Accuracy is 96.40%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 66 seconds. Training speed 827 pps. Validation speed 1815 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.243. Accuracy is 92.64%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.129. Accuracy is 96.38%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 67 seconds. Training speed 807 pps. Validation speed 1846 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.229. Accuracy is 93.03%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.123. Accuracy is 96.59%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 67 seconds. Training speed 809 pps. Validation speed 1832 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.218. Accuracy is 93.34%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.119. Accuracy is 96.68%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 67 seconds. Training speed 814 pps. Validation speed 1840 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.207. Accuracy is 93.71%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.114. Accuracy is 96.73%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 67 seconds. Training speed 814 pps. Validation speed 1830 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.202. Accuracy is 93.80%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.110. Accuracy is 96.95%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 67 seconds. Training speed 814 pps. Validation speed 1820 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.191. Accuracy is 94.13%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.105. Accuracy is 97.12%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 66 seconds. Training speed 821 pps. Validation speed 1846 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.184. Accuracy is 94.36%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.103. Accuracy is 97.21%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 66 seconds. Training speed 825 pps. Validation speed 1822 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.176. Accuracy is 94.62%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.100. Accuracy is 97.21%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 67 seconds. Training speed 812 pps. Validation speed 1821 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.169. Accuracy is 94.75%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.098. Accuracy is 97.21%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 68 seconds. Training speed 802 pps. Validation speed 1841 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.163. Accuracy is 94.89%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.098. Accuracy is 97.18%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 67 seconds. Training speed 810 pps. Validation speed 1821 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.157. Accuracy is 95.10%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.092. Accuracy is 97.36%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 67 seconds. Training speed 812 pps. Validation speed 1818 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.152. Accuracy is 95.29%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.091. Accuracy is 97.35%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 68 seconds. Training speed 805 pps. Validation speed 1844 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.148. Accuracy is 95.41%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.088. Accuracy is 97.46%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 68 seconds. Training speed 805 pps. Validation speed 1836 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.142. Accuracy is 95.59%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.085. Accuracy is 97.55%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 67 seconds. Training speed 813 pps. Validation speed 1835 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.137. Accuracy is 95.72%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.087. Accuracy is 97.56%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 66 seconds. Training speed 823 pps. Validation speed 1823 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.132. Accuracy is 95.89%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.084. Accuracy is 97.47%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 68 seconds. Training speed 803 pps. Validation speed 1828 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.129. Accuracy is 95.99%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.081. Accuracy is 97.73%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 67 seconds. Training speed 810 pps. Validation speed 1819 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.124. Accuracy is 96.09%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.080. Accuracy is 97.71%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 67 seconds. Training speed 819 pps. Validation speed 1822 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.121. Accuracy is 96.23%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.078. Accuracy is 97.88%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 67 seconds. Training speed 810 pps. Validation speed 1832 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.118. Accuracy is 96.30%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.077. Accuracy is 97.81%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 67 seconds. Training speed 815 pps. Validation speed 1833 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.112. Accuracy is 96.54%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.077. Accuracy is 97.78%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 67 seconds. Training speed 817 pps. Validation speed 1813 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.108. Accuracy is 96.56%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.075. Accuracy is 97.88%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 66 seconds. Training speed 829 pps. Validation speed 1800 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.105. Accuracy is 96.72%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.072. Accuracy is 97.89%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 66 seconds. Training speed 821 pps. Validation speed 1854 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.102. Accuracy is 96.79%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.071. Accuracy is 97.97%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 67 seconds. Training speed 815 pps. Validation speed 1845 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.100. Accuracy is 96.87%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.073. Accuracy is 97.84%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 66 seconds. Training speed 831 pps. Validation speed 1784 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.097. Accuracy is 96.94%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.072. Accuracy is 97.94%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 65 seconds. Training speed 835 pps. Validation speed 1827 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.095. Accuracy is 96.98%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.068. Accuracy is 98.05%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 67 seconds. Training speed 815 pps. Validation speed 1821 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.090. Accuracy is 97.18%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.068. Accuracy is 98.05%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 66 seconds. Training speed 823 pps. Validation speed 1864 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.088. Accuracy is 97.25%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.066. Accuracy is 98.09%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 67 seconds. Training speed 818 pps. Validation speed 1857 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.085. Accuracy is 97.32%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.068. Accuracy is 98.00%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 67 seconds. Training speed 813 pps. Validation speed 1880 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.082. Accuracy is 97.41%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.067. Accuracy is 98.08%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 66 seconds. Training speed 820 pps. Validation speed 1940 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.079. Accuracy is 97.52%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.064. Accuracy is 98.18%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 67 seconds. Training speed 806 pps. Validation speed 1936 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.078. Accuracy is 97.53%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.065. Accuracy is 98.07%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 67 seconds. Training speed 811 pps. Validation speed 1953 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.075. Accuracy is 97.62%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.064. Accuracy is 98.17%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 66 seconds. Training speed 828 pps. Validation speed 1896 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.072. Accuracy is 97.72%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.065. Accuracy is 98.10%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 66 seconds. Training speed 824 pps. Validation speed 1893 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.071. Accuracy is 97.75%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.064. Accuracy is 98.14%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 65 seconds. Training speed 835 pps. Validation speed 1924 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.068. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.062. Accuracy is 98.17%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 66 seconds. Training speed 819 pps. Validation speed 1900 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.066. Accuracy is 97.92%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.062. Accuracy is 98.15%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 66 seconds. Training speed 823 pps. Validation speed 1906 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.065. Accuracy is 97.93%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.062. Accuracy is 98.15%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 65 seconds. Training speed 841 pps. Validation speed 1894 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 98.03 %, cost (ce) is 0.060\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.471. Accuracy is 9.62%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.470. Accuracy is 9.69%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.859. Accuracy is 74.93%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.311. Accuracy is 90.91%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 82 seconds. Training speed 658 pps. Validation speed 1552 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.430. Accuracy is 86.62%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.262. Accuracy is 92.16%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 84 seconds. Training speed 648 pps. Validation speed 1556 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.366. Accuracy is 88.59%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.224. Accuracy is 93.42%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 84 seconds. Training speed 647 pps. Validation speed 1555 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.318. Accuracy is 90.22%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.199. Accuracy is 94.33%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 84 seconds. Training speed 646 pps. Validation speed 1532 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.280. Accuracy is 91.37%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.177. Accuracy is 95.06%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 82 seconds. Training speed 665 pps. Validation speed 1524 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.253. Accuracy is 92.22%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.158. Accuracy is 95.45%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 83 seconds. Training speed 656 pps. Validation speed 1550 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.231. Accuracy is 92.88%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.142. Accuracy is 95.85%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 82 seconds. Training speed 664 pps. Validation speed 1588 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.212. Accuracy is 93.47%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.131. Accuracy is 96.28%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 83 seconds. Training speed 651 pps. Validation speed 1566 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.193. Accuracy is 94.01%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.124. Accuracy is 96.43%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 83 seconds. Training speed 655 pps. Validation speed 1580 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.184. Accuracy is 94.24%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.120. Accuracy is 96.69%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 82 seconds. Training speed 660 pps. Validation speed 1567 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.168. Accuracy is 94.92%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.108. Accuracy is 96.97%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 83 seconds. Training speed 655 pps. Validation speed 1581 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.158. Accuracy is 95.11%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.107. Accuracy is 96.90%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 83 seconds. Training speed 655 pps. Validation speed 1598 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.146. Accuracy is 95.42%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.103. Accuracy is 97.08%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 83 seconds. Training speed 650 pps. Validation speed 1571 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.140. Accuracy is 95.60%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.095. Accuracy is 97.30%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 83 seconds. Training speed 651 pps. Validation speed 1580 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.130. Accuracy is 95.85%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.095. Accuracy is 97.29%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 83 seconds. Training speed 655 pps. Validation speed 1554 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.123. Accuracy is 96.16%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.088. Accuracy is 97.52%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 82 seconds. Training speed 660 pps. Validation speed 1520 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.118. Accuracy is 96.31%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.089. Accuracy is 97.29%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 84 seconds. Training speed 648 pps. Validation speed 1529 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.111. Accuracy is 96.49%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.084. Accuracy is 97.46%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 83 seconds. Training speed 650 pps. Validation speed 1535 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.105. Accuracy is 96.73%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.080. Accuracy is 97.67%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 82 seconds. Training speed 664 pps. Validation speed 1509 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.099. Accuracy is 96.90%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.079. Accuracy is 97.62%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 82 seconds. Training speed 659 pps. Validation speed 1564 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.094. Accuracy is 97.05%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.077. Accuracy is 97.69%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 82 seconds. Training speed 663 pps. Validation speed 1538 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.091. Accuracy is 97.19%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.075. Accuracy is 97.74%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 82 seconds. Training speed 656 pps. Validation speed 1592 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.086. Accuracy is 97.31%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.073. Accuracy is 97.86%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 83 seconds. Training speed 655 pps. Validation speed 1593 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.080. Accuracy is 97.47%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.072. Accuracy is 97.81%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 82 seconds. Training speed 659 pps. Validation speed 1585 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.077. Accuracy is 97.54%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.070. Accuracy is 97.95%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 82 seconds. Training speed 660 pps. Validation speed 1588 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.074. Accuracy is 97.65%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.069. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 83 seconds. Training speed 651 pps. Validation speed 1604 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.068. Accuracy is 97.84%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.071. Accuracy is 97.89%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 83 seconds. Training speed 648 pps. Validation speed 1595 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.066. Accuracy is 97.94%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.067. Accuracy is 97.95%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 83 seconds. Training speed 654 pps. Validation speed 1603 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.062. Accuracy is 98.01%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.067. Accuracy is 98.12%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 82 seconds. Training speed 657 pps. Validation speed 1558 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.058. Accuracy is 98.18%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.065. Accuracy is 98.04%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 82 seconds. Training speed 663 pps. Validation speed 1599 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.054. Accuracy is 98.29%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.066. Accuracy is 98.05%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 83 seconds. Training speed 654 pps. Validation speed 1605 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.053. Accuracy is 98.34%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.063. Accuracy is 98.11%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 83 seconds. Training speed 650 pps. Validation speed 1600 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.049. Accuracy is 98.45%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.062. Accuracy is 98.10%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 83 seconds. Training speed 654 pps. Validation speed 1604 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.046. Accuracy is 98.57%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.064. Accuracy is 97.99%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 83 seconds. Training speed 650 pps. Validation speed 1593 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.044. Accuracy is 98.62%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.061. Accuracy is 98.16%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 82 seconds. Training speed 657 pps. Validation speed 1600 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.041. Accuracy is 98.72%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.061. Accuracy is 98.14%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 82 seconds. Training speed 659 pps. Validation speed 1586 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.039. Accuracy is 98.81%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.063. Accuracy is 98.12%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 82 seconds. Training speed 658 pps. Validation speed 1617 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.036. Accuracy is 98.85%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.060. Accuracy is 98.24%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 81 seconds. Training speed 673 pps. Validation speed 1595 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.033. Accuracy is 99.02%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.062. Accuracy is 98.24%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 84 seconds. Training speed 646 pps. Validation speed 1563 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.031. Accuracy is 99.03%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.059. Accuracy is 98.31%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 83 seconds. Training speed 656 pps. Validation speed 1591 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.029. Accuracy is 99.13%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.059. Accuracy is 98.25%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 82 seconds. Training speed 661 pps. Validation speed 1603 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.026. Accuracy is 99.23%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.060. Accuracy is 98.22%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 82 seconds. Training speed 662 pps. Validation speed 1590 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.024. Accuracy is 99.34%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.061. Accuracy is 98.26%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 83 seconds. Training speed 650 pps. Validation speed 1613 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.022. Accuracy is 99.43%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.059. Accuracy is 98.45%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 83 seconds. Training speed 654 pps. Validation speed 1588 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.020. Accuracy is 99.45%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.059. Accuracy is 98.32%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 82 seconds. Training speed 661 pps. Validation speed 1595 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.019. Accuracy is 99.52%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.060. Accuracy is 98.23%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 82 seconds. Training speed 658 pps. Validation speed 1589 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.017. Accuracy is 99.62%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.061. Accuracy is 98.31%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 82 seconds. Training speed 658 pps. Validation speed 1595 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.015. Accuracy is 99.67%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.060. Accuracy is 98.31%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 82 seconds. Training speed 662 pps. Validation speed 1571 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.013. Accuracy is 99.73%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.062. Accuracy is 98.24%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 82 seconds. Training speed 664 pps. Validation speed 1589 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.011. Accuracy is 99.78%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.061. Accuracy is 98.32%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 77 seconds. Training speed 706 pps. Validation speed 1584 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 98.30 %, cost (ce) is 0.054\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.667. Accuracy is 9.68%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.671. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.092. Accuracy is 64.70%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.343. Accuracy is 89.36%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 63 seconds. Training speed 868 pps. Validation speed 1826 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.332. Accuracy is 89.83%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.256. Accuracy is 92.25%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 63 seconds. Training speed 863 pps. Validation speed 1793 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.263. Accuracy is 92.20%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.211. Accuracy is 93.62%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 63 seconds. Training speed 873 pps. Validation speed 1717 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.218. Accuracy is 93.36%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.181. Accuracy is 94.97%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 63 seconds. Training speed 876 pps. Validation speed 1775 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.188. Accuracy is 94.25%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.159. Accuracy is 95.21%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 63 seconds. Training speed 871 pps. Validation speed 1768 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.163. Accuracy is 95.08%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.162. Accuracy is 95.00%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 62 seconds. Training speed 882 pps. Validation speed 1773 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.145. Accuracy is 95.57%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.130. Accuracy is 96.33%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 62 seconds. Training speed 880 pps. Validation speed 1759 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.127. Accuracy is 96.05%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.133. Accuracy is 96.09%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 62 seconds. Training speed 882 pps. Validation speed 1769 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.115. Accuracy is 96.52%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.125. Accuracy is 96.23%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 62 seconds. Training speed 883 pps. Validation speed 1769 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.105. Accuracy is 96.83%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.111. Accuracy is 96.87%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 62 seconds. Training speed 883 pps. Validation speed 1769 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.094. Accuracy is 97.11%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.108. Accuracy is 96.66%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 62 seconds. Training speed 880 pps. Validation speed 1773 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.085. Accuracy is 97.46%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.099. Accuracy is 97.12%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 62 seconds. Training speed 889 pps. Validation speed 1756 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.077. Accuracy is 97.68%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.111. Accuracy is 96.63%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 62 seconds. Training speed 885 pps. Validation speed 1758 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.072. Accuracy is 97.86%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.089. Accuracy is 97.39%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 63 seconds. Training speed 877 pps. Validation speed 1779 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.066. Accuracy is 97.96%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.087. Accuracy is 97.45%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 62 seconds. Training speed 884 pps. Validation speed 1788 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.059. Accuracy is 98.21%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.094. Accuracy is 97.36%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 62 seconds. Training speed 890 pps. Validation speed 1775 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.056. Accuracy is 98.28%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.092. Accuracy is 97.40%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 63 seconds. Training speed 873 pps. Validation speed 1810 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.051. Accuracy is 98.49%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.085. Accuracy is 97.53%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 63 seconds. Training speed 873 pps. Validation speed 1793 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.046. Accuracy is 98.62%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.085. Accuracy is 97.51%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 62 seconds. Training speed 883 pps. Validation speed 1760 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.041. Accuracy is 98.76%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.088. Accuracy is 97.51%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 62 seconds. Training speed 881 pps. Validation speed 1775 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.038. Accuracy is 98.92%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.090. Accuracy is 97.44%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 62 seconds. Training speed 886 pps. Validation speed 1786 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.035. Accuracy is 98.99%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.095. Accuracy is 97.30%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 62 seconds. Training speed 881 pps. Validation speed 1771 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.031. Accuracy is 99.13%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.080. Accuracy is 97.61%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 62 seconds. Training speed 890 pps. Validation speed 1804 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.028. Accuracy is 99.16%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.086. Accuracy is 97.58%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 62 seconds. Training speed 883 pps. Validation speed 1801 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.026. Accuracy is 99.29%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.076. Accuracy is 97.88%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 63 seconds. Training speed 874 pps. Validation speed 1742 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.024. Accuracy is 99.30%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.075. Accuracy is 97.97%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 62 seconds. Training speed 880 pps. Validation speed 1800 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.021. Accuracy is 99.48%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.088. Accuracy is 97.59%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 62 seconds. Training speed 884 pps. Validation speed 1785 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.019. Accuracy is 99.52%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.090. Accuracy is 97.65%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 63 seconds. Training speed 881 pps. Validation speed 1736 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.016. Accuracy is 99.59%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.076. Accuracy is 98.01%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 61 seconds. Training speed 898 pps. Validation speed 1767 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.015. Accuracy is 99.63%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.079. Accuracy is 97.91%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 63 seconds. Training speed 877 pps. Validation speed 1770 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.013. Accuracy is 99.73%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.084. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 62 seconds. Training speed 887 pps. Validation speed 1733 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.012. Accuracy is 99.75%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.149. Accuracy is 96.08%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 62 seconds. Training speed 881 pps. Validation speed 1761 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.011. Accuracy is 99.74%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.079. Accuracy is 97.99%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 62 seconds. Training speed 882 pps. Validation speed 1788 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.009. Accuracy is 99.85%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.096. Accuracy is 97.54%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 63 seconds. Training speed 879 pps. Validation speed 1724 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.009. Accuracy is 99.84%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.079. Accuracy is 97.94%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 62 seconds. Training speed 888 pps. Validation speed 1804 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.007. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.079. Accuracy is 98.02%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 62 seconds. Training speed 889 pps. Validation speed 1771 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.007. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.079. Accuracy is 98.05%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 62 seconds. Training speed 883 pps. Validation speed 1732 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.006. Accuracy is 99.92%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.079. Accuracy is 98.08%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 62 seconds. Training speed 888 pps. Validation speed 1768 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.005. Accuracy is 99.93%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.082. Accuracy is 98.00%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 62 seconds. Training speed 894 pps. Validation speed 1779 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.005. Accuracy is 99.96%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.082. Accuracy is 98.05%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 62 seconds. Training speed 885 pps. Validation speed 1718 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.004. Accuracy is 99.95%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.082. Accuracy is 98.07%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 61 seconds. Training speed 896 pps. Validation speed 1763 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.004. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.085. Accuracy is 98.01%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 63 seconds. Training speed 877 pps. Validation speed 1768 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.003. Accuracy is 99.96%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.083. Accuracy is 98.06%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 62 seconds. Training speed 892 pps. Validation speed 1708 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.003. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.083. Accuracy is 98.07%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 62 seconds. Training speed 884 pps. Validation speed 1785 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.003. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.084. Accuracy is 98.03%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 62 seconds. Training speed 884 pps. Validation speed 1788 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.003. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.087. Accuracy is 98.02%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 62 seconds. Training speed 886 pps. Validation speed 1730 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.002. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.086. Accuracy is 98.00%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 63 seconds. Training speed 873 pps. Validation speed 1798 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.002. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.084. Accuracy is 98.08%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 62 seconds. Training speed 891 pps. Validation speed 1779 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.002. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.084. Accuracy is 98.09%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 63 seconds. Training speed 867 pps. Validation speed 1737 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.002. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.084. Accuracy is 98.12%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 61 seconds. Training speed 894 pps. Validation speed 1807 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.92 %, cost (ce) is 0.078\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.348. Accuracy is 9.86%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.345. Accuracy is 9.91%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.568. Accuracy is 43.98%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.806. Accuracy is 72.94%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 55 seconds. Training speed 1007 pps. Validation speed 1959 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.423. Accuracy is 87.15%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.360. Accuracy is 89.29%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 55 seconds. Training speed 1006 pps. Validation speed 1962 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.307. Accuracy is 90.79%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.242. Accuracy is 92.72%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 56 seconds. Training speed 987 pps. Validation speed 1940 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.249. Accuracy is 92.53%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.230. Accuracy is 93.23%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 55 seconds. Training speed 995 pps. Validation speed 1928 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.207. Accuracy is 93.73%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.167. Accuracy is 95.15%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 55 seconds. Training speed 1001 pps. Validation speed 1949 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.177. Accuracy is 94.59%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.157. Accuracy is 95.40%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 54 seconds. Training speed 1018 pps. Validation speed 1957 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.155. Accuracy is 95.26%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.138. Accuracy is 95.88%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 55 seconds. Training speed 994 pps. Validation speed 1949 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.135. Accuracy is 95.92%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.129. Accuracy is 96.11%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 55 seconds. Training speed 1008 pps. Validation speed 1936 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.122. Accuracy is 96.21%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.116. Accuracy is 96.61%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 55 seconds. Training speed 998 pps. Validation speed 1948 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.107. Accuracy is 96.82%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.109. Accuracy is 96.76%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 55 seconds. Training speed 1004 pps. Validation speed 1942 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.098. Accuracy is 96.99%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.116. Accuracy is 96.72%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 55 seconds. Training speed 1004 pps. Validation speed 1946 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.089. Accuracy is 97.24%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.096. Accuracy is 97.21%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 56 seconds. Training speed 991 pps. Validation speed 1923 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.080. Accuracy is 97.65%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.108. Accuracy is 96.92%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 55 seconds. Training speed 1006 pps. Validation speed 1928 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.073. Accuracy is 97.74%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.098. Accuracy is 97.12%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 55 seconds. Training speed 1003 pps. Validation speed 1930 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.067. Accuracy is 97.95%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.091. Accuracy is 97.35%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 55 seconds. Training speed 1000 pps. Validation speed 1937 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.061. Accuracy is 98.13%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.086. Accuracy is 97.54%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 54 seconds. Training speed 1015 pps. Validation speed 1926 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.057. Accuracy is 98.27%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.090. Accuracy is 97.37%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 55 seconds. Training speed 999 pps. Validation speed 1930 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.052. Accuracy is 98.37%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.089. Accuracy is 97.45%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 55 seconds. Training speed 1006 pps. Validation speed 1897 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.048. Accuracy is 98.51%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.091. Accuracy is 97.56%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 55 seconds. Training speed 997 pps. Validation speed 1933 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.042. Accuracy is 98.72%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.087. Accuracy is 97.66%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 56 seconds. Training speed 990 pps. Validation speed 1937 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.039. Accuracy is 98.84%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.087. Accuracy is 97.56%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 55 seconds. Training speed 1006 pps. Validation speed 1925 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.034. Accuracy is 98.99%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.089. Accuracy is 97.50%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 55 seconds. Training speed 1002 pps. Validation speed 1944 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.033. Accuracy is 98.95%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.097. Accuracy is 97.35%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 55 seconds. Training speed 1012 pps. Validation speed 1907 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.030. Accuracy is 99.05%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.094. Accuracy is 97.35%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 55 seconds. Training speed 1000 pps. Validation speed 1934 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.026. Accuracy is 99.26%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.087. Accuracy is 97.74%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 56 seconds. Training speed 993 pps. Validation speed 1928 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.023. Accuracy is 99.35%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.089. Accuracy is 97.78%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 55 seconds. Training speed 1007 pps. Validation speed 1913 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.019. Accuracy is 99.48%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.087. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 55 seconds. Training speed 1011 pps. Validation speed 1928 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.151. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.078. Accuracy is 97.65%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 55 seconds. Training speed 1006 pps. Validation speed 1883 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.025. Accuracy is 99.34%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.084. Accuracy is 97.51%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 55 seconds. Training speed 1010 pps. Validation speed 1937 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.017. Accuracy is 99.62%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.083. Accuracy is 97.79%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 55 seconds. Training speed 993 pps. Validation speed 1970 pps.\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.014. Accuracy is 99.65%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 0.082. Accuracy is 97.84%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 55 seconds. Training speed 1003 pps. Validation speed 1956 pps.\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.011. Accuracy is 99.79%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 0.088. Accuracy is 97.70%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 55 seconds. Training speed 1005 pps. Validation speed 1942 pps.\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.010. Accuracy is 99.82%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 0.088. Accuracy is 97.76%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 55 seconds. Training speed 1001 pps. Validation speed 1912 pps.\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.009. Accuracy is 99.81%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 0.098. Accuracy is 97.62%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 55 seconds. Training speed 1008 pps. Validation speed 1954 pps.\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.008. Accuracy is 99.86%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 0.086. Accuracy is 97.96%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 55 seconds. Training speed 1000 pps. Validation speed 1946 pps.\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.006. Accuracy is 99.88%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 0.090. Accuracy is 97.74%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 54 seconds. Training speed 1016 pps. Validation speed 1959 pps.\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.005. Accuracy is 99.91%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 0.090. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 55 seconds. Training speed 1006 pps. Validation speed 1966 pps.\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.005. Accuracy is 99.92%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 0.093. Accuracy is 97.75%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 56 seconds. Training speed 984 pps. Validation speed 1886 pps.\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.004. Accuracy is 99.95%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 0.093. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 55 seconds. Training speed 1003 pps. Validation speed 1910 pps.\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.003. Accuracy is 99.96%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 0.095. Accuracy is 97.76%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 56 seconds. Training speed 992 pps. Validation speed 1962 pps.\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.003. Accuracy is 99.97%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 0.093. Accuracy is 97.86%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 55 seconds. Training speed 1009 pps. Validation speed 1980 pps.\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.003. Accuracy is 99.97%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 0.096. Accuracy is 97.92%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 55 seconds. Training speed 1000 pps. Validation speed 1978 pps.\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.002. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 0.098. Accuracy is 97.79%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 55 seconds. Training speed 1005 pps. Validation speed 1904 pps.\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.002. Accuracy is 99.98%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 0.096. Accuracy is 97.83%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 55 seconds. Training speed 1007 pps. Validation speed 1950 pps.\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.002. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 0.096. Accuracy is 97.86%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 55 seconds. Training speed 993 pps. Validation speed 1968 pps.\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.002. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 0.098. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 55 seconds. Training speed 1007 pps. Validation speed 1961 pps.\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.002. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 0.098. Accuracy is 97.94%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 55 seconds. Training speed 1000 pps. Validation speed 1948 pps.\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.001. Accuracy is 99.99%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 0.101. Accuracy is 97.84%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 55 seconds. Training speed 1008 pps. Validation speed 1913 pps.\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.001. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 0.099. Accuracy is 97.90%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 55 seconds. Training speed 997 pps. Validation speed 1971 pps.\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.001. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 0.100. Accuracy is 97.90%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 56 seconds. Training speed 991 pps. Validation speed 1984 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.85 %, cost (ce) is 0.093\n",
      "INFO:root:Saving Data\n"
     ]
    }
   ],
   "source": [
    "# %load Experiments/dropAExperiment.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Reset\n",
    "    dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.005)\n",
    "\n",
    "\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropNF'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-22e58d315386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training started...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtr_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing the model on test set:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/optimisers.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_iterator, valid_iterator)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# do the initial validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mtr_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         logger.info('Epoch %i: Training cost (%s) for initial model is %.3f. Accuracy is %.2f%%'\n\u001b[1;32m    176\u001b[0m                     % (self.lr_scheduler.epoch, cost_name, tr_nll, tr_acc * 100.))\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/optimisers.pyc\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, model, valid_iterator, l1_weight, l2_weight)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0macc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnll_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mnll_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0macc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/layers.pyc\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/layers.pyc\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# compute the linear outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0;31m# apply numerical stabilisation by subtracting max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# from each row (not required for the coursework)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/layers.pyc\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# here f() is an identity function, so just return a linear transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load Experiments/noDropExp.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['noDLF'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9803\n"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "print shelve_r['noDLF1'][2][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reg Exp Annealed Dropout for 10 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.570. Accuracy is 9.28%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.910. Accuracy is 75.77%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.310. Accuracy is 91.36%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 70 seconds. Training speed 779 pps. Validation speed 1784 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.464. Accuracy is 85.64%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.273. Accuracy is 92.02%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 70 seconds. Training speed 772 pps. Validation speed 1834 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.379. Accuracy is 88.45%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.229. Accuracy is 93.75%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 71 seconds. Training speed 769 pps. Validation speed 1624 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.317. Accuracy is 90.41%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.199. Accuracy is 94.53%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 78 seconds. Training speed 693 pps. Validation speed 1616 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.265. Accuracy is 92.07%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.175. Accuracy is 95.30%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 75 seconds. Training speed 729 pps. Validation speed 1618 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.221. Accuracy is 93.44%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.156. Accuracy is 95.84%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 79 seconds. Training speed 688 pps. Validation speed 1600 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.188. Accuracy is 94.51%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.143. Accuracy is 96.15%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 75 seconds. Training speed 730 pps. Validation speed 1540 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.157. Accuracy is 95.40%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.132. Accuracy is 96.41%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 75 seconds. Training speed 730 pps. Validation speed 1546 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.133. Accuracy is 96.21%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.116. Accuracy is 96.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 76 seconds. Training speed 720 pps. Validation speed 1598 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.111. Accuracy is 96.82%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.111. Accuracy is 96.96%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 69 seconds. Training speed 798 pps. Validation speed 1594 pps.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e08b62282926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training started...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mtr_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing the model on test set:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/optimisers.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_iterator, valid_iterator)\u001b[0m\n\u001b[1;32m    190\u001b[0m             tr_nll, tr_acc = self.train_epoch(model=model,\n\u001b[1;32m    191\u001b[0m                                               \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                                               learning_rate=self.lr_scheduler.get_rate())\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mtstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mtr_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/optimisers.pyc\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, model, train_iterator, learning_rate)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# get the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdp_scheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdp_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/layers.pyc\u001b[0m in \u001b[0;36mfprop_dropout\u001b[0;34m(self, x, dp_scheduler)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_inp_scaler\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md_inp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0md_hid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/layers.pyc\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# a[a > 30.] = 30.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m30.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load Experiments/dropAExperiment10E.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    \n",
    "    #Reset\n",
    "    dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.05)\n",
    "    \n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropA10EF'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reg Exp Annealed Dropout for 25 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load Experiments/dropAExperiment25E.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    \n",
    "    #Reset\n",
    "    dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.02)\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropA25EF'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reg Exp Annealed Dropout for 40 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load Experiments/dropAExperiment40E.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Reset\n",
    "    dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.0125)\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropA40EF'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reg Exp Annealed Dropout for 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load Experiments/dropAExperiment50E.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    \n",
    "    #Reset for new run\n",
    "    dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.01)\n",
    "    \n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropA50EF'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reg Exp Annealed Dropout for 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load Experiments/dropAExperiment100E.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 50\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    \n",
    "    #Reset for new run\n",
    "    dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.005)\n",
    "    \n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 3:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropA100EF'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
