{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlp.utils import test_conv_linear_fprop\n",
    "from mlp.conv import ConvLinear\n",
    "\n",
    "#from mlp.conv import ConvLinear\n",
    "\n",
    "layer = ConvLinear(3,2,kernel_shape=(2,2))\n",
    "test_conv_linear_fprop(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlp.utils import test_conv_linear_bprop\n",
    "from mlp.conv import ConvLinear\n",
    "\n",
    "layer = ConvLinear(3,2, kernel_shape=(2,2))\n",
    "test_conv_linear_bprop(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlp.utils import test_conv_linear_pgrads\n",
    "from mlp.conv import ConvLinear\n",
    "\n",
    "layer = ConvLinear(3,2, kernel_shape=(2,2))\n",
    "test_conv_linear_pgrads(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  1.   0.   2.]\n",
      "   [  3.   4.   4.]\n",
      "   [  3.   4.   4.]]\n",
      "\n",
      "  [[  7.   0.   8.]\n",
      "   [  3.   5.   5.]\n",
      "   [  6.   6.   5.]]]\n",
      "\n",
      "\n",
      " [[[ 10.   0.   2.]\n",
      "   [  3.   4.   4.]\n",
      "   [  3.   4.   4.]]\n",
      "\n",
      "  [[  8.   0.  11.]\n",
      "   [  9.   9.   5.]\n",
      "   [ 10.   9.   5.]]]]\n",
      "[[[[  1.   0.   2.]\n",
      "   [  3.   4.   4.]\n",
      "   [  3.   4.   4.]]\n",
      "\n",
      "  [[  7.   0.   8.]\n",
      "   [  3.   5.   5.]\n",
      "   [  6.   6.   5.]]]\n",
      "\n",
      "\n",
      " [[[ 10.   0.   2.]\n",
      "   [  3.   4.   4.]\n",
      "   [  3.   4.   4.]]\n",
      "\n",
      "  [[  8.   0.  11.]\n",
      "   [  9.   9.   5.]\n",
      "   [ 10.   9.   5.]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlp.utils import test_maxpool_fprop,test_maxpool_generic_fprop\n",
    "from mlp.conv import ConvMaxPool2D\n",
    "\n",
    "layer = ConvMaxPool2D(2, (4,4), pool_shape=(2,2), pool_stride=(2,2))\n",
    "test_maxpool_fprop(layer)\n",
    "layer = ConvMaxPool2D(2, (4,4), pool_shape=(2,2), pool_stride=(1,1))\n",
    "test_maxpool_generic_fprop(layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlp.utils import test_maxpool_bprop, test_maxpool_generic_bprop\n",
    "from mlp.conv import ConvMaxPool2D\n",
    "\n",
    "layer = ConvMaxPool2D(2,(24,24), pool_stride=(2,2))\n",
    "test_maxpool_bprop(layer)\n",
    "\n",
    "layer = ConvMaxPool2D(2,(24,24), pool_stride=(1,1))\n",
    "test_maxpool_generic_bprop(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.  1.]\n",
      "   [ 1.  1.]]]]\n",
      "[[[[ 2.  2.]\n",
      "   [ 2.  2.]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nVerify Gradients\\nMake checks - Done maxpool Checks\\nMake maxpool generic, just change bprop?\\n\\nCython \\nExperiments\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "x = numpy.ones((1,1,2,2))\n",
    "print x\n",
    "\n",
    "print x+1\n",
    "\n",
    "'''\n",
    "Verify Gradients - WHY THE FUCK WON'T IT WORK\n",
    "\n",
    "Cython \n",
    "Experiments\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do back prop check for maxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do sigmoid checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do Relu checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mlp.layers import max_and_argmax\n",
    "import numpy\n",
    "\n",
    "y = numpy.array(\n",
    "      [[[[  496.,   500.,   436.],\n",
    "         [  376.,   500.,   316.],\n",
    "         [  256.,   226.,   500.]],\n",
    "        [[ 1385.,  1403.,  1421.],\n",
    "         [ 1457.,  1475.,  1493.],\n",
    "         [ 1529.,  1547.,  1565.]]],\n",
    "       [[[ -944.,  -974., -1004.],\n",
    "         [-1064., -1094., -1124.],\n",
    "         [-1184., -1214., -1244.]],\n",
    "        [[ 2249.,  2267.,  2285.],\n",
    "         [ 2321.,  2339.,  2357.],\n",
    "         [ 2393.,  2411.,  2429.]]]])\n",
    "\n",
    "\n",
    "mx, mxi = max_and_argmax(y[0,0,:,:],keepdims_argmax=True)\n",
    "G = numpy.zeros(y.shape)\n",
    "G[0,0,mxi[0],mxi[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = []  # remove any existing handlers\n",
    "file_handler = logging.FileHandler('scl.log')\n",
    "file_handler.setFormatter(logging.Formatter(\n",
    "    '%(name)s:%(levelname)s %(message)s'))\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext cython\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy\n",
    "cimport numpy\n",
    "\n",
    "from mlp.layers import max_and_argmax\n",
    "\n",
    "from types import *\n",
    "\n",
    "\n",
    "DTYPE = numpy.float32\n",
    "ctypedef numpy.float32_t DTYPE_t\n",
    "\n",
    "\n",
    "DTYPE2 = numpy.int\n",
    "ctypedef numpy.int_t DTYPE2_t\n",
    "\n",
    "def my1_conv2d(numpy.ndarray[DTYPE_t, ndim=4] image,numpy.ndarray[DTYPE_t, ndim=4] kernels, strides=(1, 1), pool=False):\n",
    "    \"\"\"\n",
    "    Implements a 2d valid convolution of kernels with the image\n",
    "    Note: filter means the same as kernel and convolution (correlation) of those with the input space\n",
    "    produces feature maps (sometimes refered to also as receptive fields). Also note, that\n",
    "    feature maps are synonyms here to channels, and as such num_inp_channels == num_inp_feat_maps\n",
    "    :param image: 4D tensor of sizes (batch_size, num_input_channels, img_shape_x, img_shape_y)\n",
    "    :param filters: 4D tensor of filters of size (num_inp_feat_maps, num_out_feat_maps, kernel_shape_x, kernel_shape_y)\n",
    "    :param strides: a tuple (stride_x, stride_y), specifying the shift of the kernels in x and y dimensions\n",
    "    :return: 4D tensor of size (batch_size, num_out_feature_maps, feature_map_shape_x, feature_map_shape_y)\n",
    "    \"\"\"\n",
    "   \n",
    "    #Get batch_size out\n",
    "    cdef int batch_size = image.shape[0]\n",
    "    \n",
    "    #Get the kernels size\n",
    "    cdef int out = kernels.shape[1]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Image dims - Used equation from stanford lectures: http://cs231n.github.io/convolutional-networks/\n",
    "    if((InputSize-FilterSize+2Padding)/Stride+1) is valid int then carry on else throw and error, first calculate\n",
    "    '''\n",
    "    \n",
    "    #Padding not implemented\n",
    "    cdef int padding = 0\n",
    "    \n",
    "    cdef int kxdim = kernels.shape[2]\n",
    "    cdef int kydim = kernels.shape[3]\n",
    "    #Get feature map size out\n",
    "    cdef int num_out_feat_maps = kernels.shape[1]\n",
    "    #Get input feature size\n",
    "    cdef int num_inp_feat_maps = kernels.shape[0]\n",
    "        \n",
    "    \n",
    "    assert kxdim <= image.shape[2], 'Kernel can be max the size of the image'\n",
    "    assert kydim <= image.shape[3], 'Kernel can be max the size of the image'\n",
    "    \n",
    "    #Can calculate here as this is all we are going to go to anyway - xdims is how many times kernel can move...\n",
    "    #These only tell us if we can use strides\n",
    "    cdef int strides_pos_x = ((image.shape[2] - kxdim + (2*padding))/(strides[0])) +1\n",
    "    cdef int strides_pos_y = ((image.shape[3] - kydim + (2*padding))/(strides[1])) +1\n",
    "    \n",
    "    #Do assertions to ensure passed in the correct type, strides pos also correspond to the output size.\n",
    "    assert strides_pos_x.dtype == DTYPE2,\"Can't make feature map with x-stride: %r\" % strides[0]\n",
    "    assert strides_pos_y.dtype == DTYPE2,\"Can't make feature map with y-stride: %r\" % strides[1]\n",
    "    \n",
    "    #Create G matrix\n",
    "    cdef numpy.ndarray G = numpy.zeros((image.shape[0],image.shape[1],image.shape[2],image.shape[3]))\n",
    "    \n",
    "    #Actual number of dimensions to traverse, +1 to count for range function\n",
    "    cdef int xdims = (image.shape[2] - kxdim) + 1\n",
    "    cdef int ydims = (image.shape[3] - kydim) + 1\n",
    "    \n",
    "    #Create empty 4D tensor\n",
    "    cdef numpy.ndarray output =  numpy.zeros((batch_size,out,strides_pos_x,strides_pos_y), dtype=DTYPE)\n",
    "    cdef numpy.ndarray[DTYPE_t, ndim=4] imgSlice\n",
    "    cdef numpy.ndarray[DTYPE_t, ndim=4] kernel\n",
    "    \n",
    "    cdef img,fm,x,y\n",
    "    #For each image in batch\n",
    "    for img in xrange(batch_size):\n",
    "        #For each feature map (output map)\n",
    "        for fm in xrange(num_out_feat_maps):\n",
    "            #For each x-dim in output\n",
    "            '''\n",
    "                Striding is taken care of by going through all input x dimensions with a stride, then putting them into the\n",
    "                output file \n",
    "            '''\n",
    "            for x in xrange(0,xdims,strides[0]):\n",
    "                #For each y-dim in output\n",
    "                for y in xrange(0,ydims,strides[1]):\n",
    "                    #Get image slice from entire image, corresponds to kernel size, accross all channels.\n",
    "                    if(pool == True):\n",
    "                        imgSlice = image[img, fm, x:x+kxdim, y:y+kydim]\n",
    "                        maxi, maxInd = max_and_argmax(imgSlice, keepdims_argmax=True)\n",
    "                        #No need to div by strides\n",
    "                        output[img, fm, x/strides[0], y/strides[1]] = maxi\n",
    "                        #Add first corresponding max into G mat\n",
    "                        G[img,fm,x+maxInd[0],y+maxInd[1]] = 1\n",
    "                        \n",
    "                    else:\n",
    "                        imgSlice = image[img, :, x:x+kxdim, y:y+kydim]\n",
    "                        #Get kernels accross all channels.\n",
    "                        kernel = kernels[:, fm, :, :]\n",
    "                        \n",
    "                        '''\n",
    "                            Do the dot product to get the position.\n",
    "                            Divide by strides to get actual output position\n",
    "                        '''\n",
    "                        output[img, fm, x/strides[0], y/strides[1]] = numpy.dot(imgSlice.flatten(),kernel.flatten())\n",
    "                \n",
    "    return output,G\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Standard Conv Linear for 10 epochs\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.385. Accuracy is 10.60%\n"
     ]
    }
   ],
   "source": [
    "#Baseline experiment\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Standard Conv Linear for 10 epochs')\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True, conv_reshape = True )\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=1000, max_num_batches=-10, randomize=False, conv_reshape = True)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=1000, max_num_batches=-10, randomize=False, conv_reshape = True)\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.conv import ConvLinear, ConvMaxPool2D\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"naiveLinConV\", writeback = True)\n",
    "\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "\n",
    "train_dp.reset()\n",
    "valid_dp.reset()\n",
    "test_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(ConvLinear(num_inp_feat_maps = 1, num_out_feat_maps = 2, irange=0.2, rng=rng))\n",
    "model.add_layer(ConvMaxPool2D(num_feat_maps = 2, conv_shape = (24,24)))\n",
    "model.add_layer(Sigmoid(idim=2*12*12, odim=2*12*12, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=2*12*12, odim=10, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "shelve_r['results'] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "shelve_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:INFO Standard Conv Linear for 10 epochs\n",
      "root:INFO Initialising data providers...\n",
      "root:INFO Standard Conv Linear for 10 epochs\n",
      "root:INFO Initialising data providers...\n",
      "root:INFO Training started...\n",
      "mlp.optimisers:INFO Epoch 0: Training cost (ce) for initial model is 2.352. Accuracy is 10.60%\n",
      "mlp.optimisers:INFO Epoch 0: Validation cost (ce) for initial model is 2.369. Accuracy is 9.91%\n",
      "mlp.optimisers:INFO Epoch 1: Training cost (ce) is 2.818. Accuracy is 8.80%\n",
      "mlp.optimisers:INFO Epoch 1: Validation cost (ce) is 2.310. Accuracy is 9.90%\n",
      "mlp.optimisers:INFO Epoch 1: Took 296 seconds. Training speed 23 pps. Validation speed 40 pps.\n",
      "mlp.optimisers:INFO Epoch 2: Training cost (ce) is 2.314. Accuracy is 10.30%\n",
      "mlp.optimisers:INFO Epoch 2: Validation cost (ce) is 2.303. Accuracy is 12.94%\n",
      "mlp.optimisers:INFO Epoch 2: Took 296 seconds. Training speed 23 pps. Validation speed 40 pps.\n",
      "mlp.optimisers:INFO Epoch 3: Training cost (ce) is 2.183. Accuracy is 17.60%\n",
      "mlp.optimisers:INFO Epoch 3: Validation cost (ce) is 1.779. Accuracy is 33.70%\n",
      "mlp.optimisers:INFO Epoch 3: Took 296 seconds. Training speed 23 pps. Validation speed 40 pps.\n",
      "mlp.optimisers:INFO Epoch 4: Training cost (ce) is 1.701. Accuracy is 40.00%\n",
      "mlp.optimisers:INFO Epoch 4: Validation cost (ce) is 1.414. Accuracy is 53.66%\n",
      "mlp.optimisers:INFO Epoch 4: Took 301 seconds. Training speed 22 pps. Validation speed 39 pps.\n",
      "mlp.optimisers:INFO Epoch 5: Training cost (ce) is 1.615. Accuracy is 41.10%\n",
      "mlp.optimisers:INFO Epoch 5: Validation cost (ce) is 1.352. Accuracy is 49.75%\n",
      "mlp.optimisers:INFO Epoch 5: Took 301 seconds. Training speed 22 pps. Validation speed 39 pps.\n",
      "mlp.optimisers:INFO Epoch 6: Training cost (ce) is 1.622. Accuracy is 41.80%\n",
      "mlp.optimisers:INFO Epoch 6: Validation cost (ce) is 1.411. Accuracy is 45.66%\n",
      "mlp.optimisers:INFO Epoch 6: Took 302 seconds. Training speed 22 pps. Validation speed 39 pps.\n",
      "mlp.optimisers:INFO Epoch 7: Training cost (ce) is 1.633. Accuracy is 41.70%\n",
      "mlp.optimisers:INFO Epoch 7: Validation cost (ce) is 1.686. Accuracy is 39.25%\n",
      "mlp.optimisers:INFO Epoch 7: Took 301 seconds. Training speed 22 pps. Validation speed 39 pps.\n",
      "mlp.optimisers:INFO Epoch 8: Training cost (ce) is 1.755. Accuracy is 40.30%\n",
      "mlp.optimisers:INFO Epoch 8: Validation cost (ce) is 1.741. Accuracy is 40.20%\n",
      "mlp.optimisers:INFO Epoch 8: Took 301 seconds. Training speed 23 pps. Validation speed 39 pps.\n",
      "mlp.optimisers:INFO Epoch 9: Training cost (ce) is 1.829. Accuracy is 42.70%\n",
      "mlp.optimisers:INFO Epoch 9: Validation cost (ce) is 1.367. Accuracy is 49.63%\n",
      "mlp.optimisers:INFO Epoch 9: Took 299 seconds. Training speed 23 pps. Validation speed 39 pps.\n",
      "mlp.optimisers:INFO Epoch 10: Training cost (ce) is 1.722. Accuracy is 41.90%\n",
      "mlp.optimisers:INFO Epoch 10: Validation cost (ce) is 1.494. Accuracy is 49.85%\n",
      "mlp.optimisers:INFO Epoch 10: Took 300 seconds. Training speed 22 pps. Validation speed 39 pps.\n",
      "root:INFO Testing the model on test set:\n",
      "root:INFO MNIST test set accuracy is 50.45 %, cost (ce) is 1.473\n",
      "root:INFO Standard Conv Linear for 10 epochs\n",
      "root:INFO Initialising data providers...\n",
      "root:INFO Training started...\n",
      "mlp.optimisers:INFO Epoch 0: Training cost (ce) for initial model is 2.352. Accuracy is 10.60%\n",
      "mlp.optimisers:INFO Epoch 0: Validation cost (ce) for initial model is 2.369. Accuracy is 9.91%\n",
      "mlp.optimisers:INFO Epoch 1: Training cost (ce) is 2.818. Accuracy is 8.80%\n",
      "mlp.optimisers:INFO Epoch 1: Validation cost (ce) is 2.310. Accuracy is 9.90%\n",
      "mlp.optimisers:INFO Epoch 1: Took 236 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 2: Training cost (ce) is 2.314. Accuracy is 10.30%\n",
      "mlp.optimisers:INFO Epoch 2: Validation cost (ce) is 2.303. Accuracy is 12.94%\n",
      "mlp.optimisers:INFO Epoch 2: Took 235 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 3: Training cost (ce) is 2.183. Accuracy is 17.60%\n",
      "mlp.optimisers:INFO Epoch 3: Validation cost (ce) is 1.779. Accuracy is 33.70%\n",
      "mlp.optimisers:INFO Epoch 3: Took 236 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 4: Training cost (ce) is 1.701. Accuracy is 40.00%\n",
      "mlp.optimisers:INFO Epoch 4: Validation cost (ce) is 1.414. Accuracy is 53.66%\n",
      "mlp.optimisers:INFO Epoch 4: Took 236 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 5: Training cost (ce) is 1.615. Accuracy is 41.10%\n",
      "mlp.optimisers:INFO Epoch 5: Validation cost (ce) is 1.352. Accuracy is 49.75%\n",
      "mlp.optimisers:INFO Epoch 5: Took 236 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 6: Training cost (ce) is 1.622. Accuracy is 41.80%\n",
      "mlp.optimisers:INFO Epoch 6: Validation cost (ce) is 1.411. Accuracy is 45.66%\n",
      "mlp.optimisers:INFO Epoch 6: Took 234 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 7: Training cost (ce) is 1.633. Accuracy is 41.70%\n",
      "mlp.optimisers:INFO Epoch 7: Validation cost (ce) is 1.686. Accuracy is 39.25%\n",
      "mlp.optimisers:INFO Epoch 7: Took 235 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 8: Training cost (ce) is 1.755. Accuracy is 40.30%\n",
      "mlp.optimisers:INFO Epoch 8: Validation cost (ce) is 1.741. Accuracy is 40.20%\n",
      "mlp.optimisers:INFO Epoch 8: Took 236 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 9: Training cost (ce) is 1.829. Accuracy is 42.70%\n",
      "mlp.optimisers:INFO Epoch 9: Validation cost (ce) is 1.367. Accuracy is 49.63%\n",
      "mlp.optimisers:INFO Epoch 9: Took 234 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 10: Training cost (ce) is 1.722. Accuracy is 41.90%\n",
      "mlp.optimisers:INFO Epoch 10: Validation cost (ce) is 1.494. Accuracy is 49.85%\n",
      "mlp.optimisers:INFO Epoch 10: Took 234 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 11: Training cost (ce) is 1.826. Accuracy is 40.90%\n",
      "mlp.optimisers:INFO Epoch 11: Validation cost (ce) is 1.626. Accuracy is 46.36%\n",
      "mlp.optimisers:INFO Epoch 11: Took 236 seconds. Training speed 22 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 12: Training cost (ce) is 1.784. Accuracy is 42.90%\n",
      "mlp.optimisers:INFO Epoch 12: Validation cost (ce) is 1.472. Accuracy is 50.65%\n",
      "mlp.optimisers:INFO Epoch 12: Took 233 seconds. Training speed 24 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 13: Training cost (ce) is 1.777. Accuracy is 44.40%\n",
      "mlp.optimisers:INFO Epoch 13: Validation cost (ce) is 1.832. Accuracy is 43.97%\n",
      "mlp.optimisers:INFO Epoch 13: Took 234 seconds. Training speed 24 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 14: Training cost (ce) is 1.736. Accuracy is 44.20%\n",
      "mlp.optimisers:INFO Epoch 14: Validation cost (ce) is 1.956. Accuracy is 48.29%\n",
      "mlp.optimisers:INFO Epoch 14: Took 235 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 15: Training cost (ce) is 1.900. Accuracy is 42.70%\n",
      "mlp.optimisers:INFO Epoch 15: Validation cost (ce) is 2.331. Accuracy is 42.36%\n",
      "mlp.optimisers:INFO Epoch 15: Took 236 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 16: Training cost (ce) is 2.031. Accuracy is 41.60%\n",
      "mlp.optimisers:INFO Epoch 16: Validation cost (ce) is 2.319. Accuracy is 44.42%\n",
      "mlp.optimisers:INFO Epoch 16: Took 235 seconds. Training speed 24 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 17: Training cost (ce) is 2.142. Accuracy is 42.60%\n",
      "mlp.optimisers:INFO Epoch 17: Validation cost (ce) is 2.218. Accuracy is 43.12%\n",
      "mlp.optimisers:INFO Epoch 17: Took 235 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 18: Training cost (ce) is 2.108. Accuracy is 42.40%\n",
      "mlp.optimisers:INFO Epoch 18: Validation cost (ce) is 1.852. Accuracy is 45.90%\n",
      "mlp.optimisers:INFO Epoch 18: Took 233 seconds. Training speed 23 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 19: Training cost (ce) is 2.041. Accuracy is 44.40%\n",
      "mlp.optimisers:INFO Epoch 19: Validation cost (ce) is 1.695. Accuracy is 47.73%\n",
      "mlp.optimisers:INFO Epoch 19: Took 233 seconds. Training speed 24 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 20: Training cost (ce) is 2.077. Accuracy is 44.80%\n",
      "mlp.optimisers:INFO Epoch 20: Validation cost (ce) is 1.929. Accuracy is 51.75%\n",
      "mlp.optimisers:INFO Epoch 20: Took 234 seconds. Training speed 23 pps. Validation speed 53 pps.\n",
      "mlp.optimisers:INFO Epoch 21: Training cost (ce) is 2.051. Accuracy is 43.70%\n",
      "mlp.optimisers:INFO Epoch 21: Validation cost (ce) is 1.947. Accuracy is 47.97%\n",
      "mlp.optimisers:INFO Epoch 21: Took 236 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 22: Training cost (ce) is 2.298. Accuracy is 41.30%\n",
      "mlp.optimisers:INFO Epoch 22: Validation cost (ce) is 2.360. Accuracy is 45.50%\n",
      "mlp.optimisers:INFO Epoch 22: Took 234 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 23: Training cost (ce) is 2.066. Accuracy is 41.40%\n",
      "mlp.optimisers:INFO Epoch 23: Validation cost (ce) is 2.141. Accuracy is 45.02%\n",
      "mlp.optimisers:INFO Epoch 23: Took 236 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 24: Training cost (ce) is 2.081. Accuracy is 43.30%\n",
      "mlp.optimisers:INFO Epoch 24: Validation cost (ce) is 1.482. Accuracy is 57.37%\n",
      "mlp.optimisers:INFO Epoch 24: Took 235 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 25: Training cost (ce) is 1.939. Accuracy is 45.10%\n",
      "mlp.optimisers:INFO Epoch 25: Validation cost (ce) is 2.096. Accuracy is 43.65%\n",
      "mlp.optimisers:INFO Epoch 25: Took 237 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 26: Training cost (ce) is 1.979. Accuracy is 42.40%\n",
      "mlp.optimisers:INFO Epoch 26: Validation cost (ce) is 1.725. Accuracy is 48.45%\n",
      "mlp.optimisers:INFO Epoch 26: Took 236 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 27: Training cost (ce) is 1.913. Accuracy is 45.90%\n",
      "mlp.optimisers:INFO Epoch 27: Validation cost (ce) is 1.785. Accuracy is 48.01%\n",
      "mlp.optimisers:INFO Epoch 27: Took 236 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 28: Training cost (ce) is 1.787. Accuracy is 48.20%\n",
      "mlp.optimisers:INFO Epoch 28: Validation cost (ce) is 1.774. Accuracy is 45.23%\n",
      "mlp.optimisers:INFO Epoch 28: Took 235 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 29: Training cost (ce) is 1.854. Accuracy is 45.40%\n",
      "mlp.optimisers:INFO Epoch 29: Validation cost (ce) is 2.227. Accuracy is 42.09%\n",
      "mlp.optimisers:INFO Epoch 29: Took 237 seconds. Training speed 24 pps. Validation speed 52 pps.\n",
      "mlp.optimisers:INFO Epoch 30: Training cost (ce) is 1.917. Accuracy is 44.90%\n",
      "mlp.optimisers:INFO Epoch 30: Validation cost (ce) is 1.931. Accuracy is 50.85%\n",
      "mlp.optimisers:INFO Epoch 30: Took 236 seconds. Training speed 23 pps. Validation speed 52 pps.\n",
      "root:INFO Testing the model on test set:\n",
      "root:INFO MNIST test set accuracy is 51.41 %, cost (ce) is 1.929\n",
      "root:INFO Standard Conv Linear for 10 epochs\n",
      "root:INFO Initialising data providers...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('scl.log', 'r') as log_file:\n",
    "    print log_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
