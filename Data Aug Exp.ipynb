{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Aug Experiments - Mention increase in batch size to 500, as one will inflate batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.570. Accuracy is 9.32%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.805. Accuracy is 85.26%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.337. Accuracy is 89.83%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 39 seconds. Training speed 752 pps. Validation speed 1643 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.277. Accuracy is 91.73%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.239. Accuracy is 92.90%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 40 seconds. Training speed 748 pps. Validation speed 1601 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.222. Accuracy is 93.52%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.202. Accuracy is 94.34%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 39 seconds. Training speed 763 pps. Validation speed 1547 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.183. Accuracy is 94.58%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.172. Accuracy is 95.24%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 40 seconds. Training speed 737 pps. Validation speed 1580 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.156. Accuracy is 95.38%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.171. Accuracy is 95.46%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 41 seconds. Training speed 735 pps. Validation speed 1488 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.133. Accuracy is 96.15%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.142. Accuracy is 96.15%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 40 seconds. Training speed 732 pps. Validation speed 1700 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.116. Accuracy is 96.66%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.146. Accuracy is 95.83%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 41 seconds. Training speed 726 pps. Validation speed 1550 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.101. Accuracy is 97.08%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.124. Accuracy is 96.61%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 40 seconds. Training speed 739 pps. Validation speed 1600 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.089. Accuracy is 97.42%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.121. Accuracy is 96.81%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 41 seconds. Training speed 726 pps. Validation speed 1593 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.080. Accuracy is 97.77%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.117. Accuracy is 96.69%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 41 seconds. Training speed 732 pps. Validation speed 1572 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.070. Accuracy is 98.10%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.114. Accuracy is 96.86%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 40 seconds. Training speed 739 pps. Validation speed 1630 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.062. Accuracy is 98.32%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.112. Accuracy is 96.84%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 39 seconds. Training speed 757 pps. Validation speed 1623 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.056. Accuracy is 98.53%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.105. Accuracy is 97.01%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 39 seconds. Training speed 751 pps. Validation speed 1626 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.050. Accuracy is 98.74%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.102. Accuracy is 97.16%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 41 seconds. Training speed 732 pps. Validation speed 1571 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.045. Accuracy is 98.91%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.105. Accuracy is 97.04%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 38 seconds. Training speed 778 pps. Validation speed 1606 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.041. Accuracy is 99.02%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.102. Accuracy is 97.20%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 38 seconds. Training speed 797 pps. Validation speed 1600 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.036. Accuracy is 99.17%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.099. Accuracy is 97.33%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 39 seconds. Training speed 769 pps. Validation speed 1587 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.032. Accuracy is 99.30%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.097. Accuracy is 97.27%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 38 seconds. Training speed 776 pps. Validation speed 1607 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.030. Accuracy is 99.41%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.095. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 39 seconds. Training speed 766 pps. Validation speed 1553 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.027. Accuracy is 99.55%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.096. Accuracy is 97.48%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 38 seconds. Training speed 775 pps. Validation speed 1648 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.024. Accuracy is 99.58%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.096. Accuracy is 97.44%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 39 seconds. Training speed 761 pps. Validation speed 1666 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.022. Accuracy is 99.64%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.095. Accuracy is 97.43%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 38 seconds. Training speed 780 pps. Validation speed 1557 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.020. Accuracy is 99.72%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.097. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 39 seconds. Training speed 771 pps. Validation speed 1581 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.018. Accuracy is 99.78%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.095. Accuracy is 97.36%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 39 seconds. Training speed 758 pps. Validation speed 1546 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.017. Accuracy is 99.78%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.097. Accuracy is 97.30%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 39 seconds. Training speed 772 pps. Validation speed 1571 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.016. Accuracy is 99.85%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.098. Accuracy is 97.43%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 39 seconds. Training speed 767 pps. Validation speed 1560 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.014. Accuracy is 99.87%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.097. Accuracy is 97.41%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 38 seconds. Training speed 782 pps. Validation speed 1595 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.013. Accuracy is 99.91%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.096. Accuracy is 97.35%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 39 seconds. Training speed 760 pps. Validation speed 1680 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.012. Accuracy is 99.91%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.095. Accuracy is 97.49%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 39 seconds. Training speed 761 pps. Validation speed 1618 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.011. Accuracy is 99.94%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.098. Accuracy is 97.38%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 39 seconds. Training speed 764 pps. Validation speed 1638 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.40 %, cost (ce) is 0.092\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.376. Accuracy is 9.91%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.380. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.951. Accuracy is 28.43%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.820. Accuracy is 72.89%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 35 seconds. Training speed 868 pps. Validation speed 1588 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.558. Accuracy is 81.94%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.357. Accuracy is 89.10%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 36 seconds. Training speed 852 pps. Validation speed 1603 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.349. Accuracy is 89.35%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.305. Accuracy is 90.43%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 35 seconds. Training speed 867 pps. Validation speed 1627 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.279. Accuracy is 91.32%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.245. Accuracy is 92.45%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 35 seconds. Training speed 853 pps. Validation speed 1771 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.229. Accuracy is 93.06%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.201. Accuracy is 94.00%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 35 seconds. Training speed 865 pps. Validation speed 1641 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.196. Accuracy is 94.02%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.171. Accuracy is 94.94%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 35 seconds. Training speed 868 pps. Validation speed 1621 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.167. Accuracy is 94.79%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.164. Accuracy is 95.17%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 36 seconds. Training speed 855 pps. Validation speed 1598 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.148. Accuracy is 95.42%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.161. Accuracy is 95.39%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 32 seconds. Training speed 917 pps. Validation speed 1980 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.130. Accuracy is 96.08%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.140. Accuracy is 95.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 31 seconds. Training speed 960 pps. Validation speed 1912 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.115. Accuracy is 96.53%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.171. Accuracy is 94.99%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 32 seconds. Training speed 925 pps. Validation speed 1861 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.101. Accuracy is 96.87%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.143. Accuracy is 95.78%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 33 seconds. Training speed 918 pps. Validation speed 1734 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.090. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.156. Accuracy is 95.33%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 33 seconds. Training speed 908 pps. Validation speed 1877 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.078. Accuracy is 97.60%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.126. Accuracy is 96.40%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 32 seconds. Training speed 938 pps. Validation speed 1892 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.071. Accuracy is 97.82%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.158. Accuracy is 95.79%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 33 seconds. Training speed 903 pps. Validation speed 1782 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.062. Accuracy is 98.18%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.128. Accuracy is 96.51%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 33 seconds. Training speed 920 pps. Validation speed 1825 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.056. Accuracy is 98.18%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.122. Accuracy is 96.72%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 33 seconds. Training speed 921 pps. Validation speed 1665 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.051. Accuracy is 98.40%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.125. Accuracy is 96.62%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 33 seconds. Training speed 910 pps. Validation speed 1883 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.046. Accuracy is 98.54%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.108. Accuracy is 97.11%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 33 seconds. Training speed 909 pps. Validation speed 1890 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.040. Accuracy is 98.82%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.115. Accuracy is 96.98%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 33 seconds. Training speed 911 pps. Validation speed 1961 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.035. Accuracy is 99.05%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.114. Accuracy is 96.99%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 32 seconds. Training speed 926 pps. Validation speed 1854 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.028. Accuracy is 99.24%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.131. Accuracy is 96.49%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 33 seconds. Training speed 896 pps. Validation speed 1836 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.027. Accuracy is 99.24%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.117. Accuracy is 96.99%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 32 seconds. Training speed 929 pps. Validation speed 1934 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.030. Accuracy is 99.15%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.130. Accuracy is 96.85%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 33 seconds. Training speed 918 pps. Validation speed 1842 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.021. Accuracy is 99.43%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.121. Accuracy is 96.89%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 33 seconds. Training speed 920 pps. Validation speed 1810 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.014. Accuracy is 99.66%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.136. Accuracy is 96.68%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 33 seconds. Training speed 890 pps. Validation speed 1868 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.012. Accuracy is 99.68%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.119. Accuracy is 97.34%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 32 seconds. Training speed 940 pps. Validation speed 1866 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.010. Accuracy is 99.77%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.136. Accuracy is 97.04%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 34 seconds. Training speed 891 pps. Validation speed 1827 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.007. Accuracy is 99.88%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.126. Accuracy is 97.16%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 32 seconds. Training speed 928 pps. Validation speed 1827 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.006. Accuracy is 99.88%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.126. Accuracy is 97.32%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 33 seconds. Training speed 895 pps. Validation speed 1874 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.006. Accuracy is 99.87%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.127. Accuracy is 97.18%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 33 seconds. Training speed 917 pps. Validation speed 1887 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.13 %, cost (ce) is 0.120\n",
      "INFO:root:Saving Data\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "#Set no aug\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=50, max_num_batches=500, randomize=True, augmentation = 0)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.00\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "    \n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['noAug'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(2.5703508233660521, 0.093239999999999837), (0.80523317568429009, 0.85255999999999876), (0.27677549487643593, 0.91727999999999832), (0.22214730183563638, 0.93519999999999825), (0.18305137563708659, 0.94583999999999857), (0.15569982539570845, 0.95375999999999872), (0.13300080456875954, 0.9615199999999996), (0.11569182332048453, 0.96659999999999957), (0.10137976111233382, 0.9708), (0.088637899782371021, 0.97419999999999984), (0.079508461866036467, 0.97772000000000003), (0.070032142996076019, 0.98096000000000105), (0.062405188919370608, 0.9832000000000013), (0.055755217898011723, 0.98532000000000119), (0.04967158365777917, 0.98744000000000121), (0.04458790914267511, 0.98912000000000144), (0.040571948966103173, 0.99020000000000119), (0.036383325059962189, 0.99172000000000138), (0.032378143263541291, 0.99300000000000133), (0.029633980765690616, 0.9940800000000013), (0.026752833110905212, 0.99552000000000074), (0.024419206568088901, 0.9958000000000008), (0.022283936445299839, 0.99644000000000099), (0.020426003503800961, 0.99720000000000075), (0.018373570548880142, 0.99776000000000065), (0.017081363931949669, 0.99776000000000076), (0.015679766499505924, 0.99852000000000074), (0.014156501216024237, 0.99868000000000001), (0.013498181197960828, 0.99908000000000063), (0.012289599282016629, 0.99912000000000056), (0.011351036727095292, 0.99936000000000014)], [(2.5535497258761009, 0.098400000000000001), (0.33671330578696718, 0.89829999999999999), (0.23891238190127584, 0.92900000000000005), (0.20236318925468633, 0.94340000000000002), (0.17230566591938834, 0.95240000000000002), (0.17095267353612659, 0.9546), (0.1422519712481129, 0.96150000000000002), (0.14553004047410278, 0.95830000000000004), (0.12432306946479585, 0.96609999999999996), (0.12125752531609912, 0.96809999999999996), (0.11700666866391364, 0.96689999999999998), (0.1141506231164771, 0.96860000000000002), (0.11161868278035454, 0.96840000000000004), (0.10505397281456885, 0.97009999999999996), (0.10204037316560445, 0.97160000000000002), (0.10487224348522473, 0.97040000000000004), (0.10182463345379623, 0.97199999999999998), (0.098589431970232941, 0.97330000000000005), (0.09665586316200929, 0.97270000000000001), (0.095186040241131267, 0.97260000000000002), (0.096356009879375093, 0.9748), (0.096063583129408839, 0.97440000000000004), (0.094546509742299445, 0.97430000000000005), (0.097064316785876792, 0.97260000000000002), (0.095231162429616989, 0.97360000000000002), (0.09701217357475346, 0.97299999999999998), (0.098328930756980043, 0.97430000000000005), (0.096675300312268023, 0.97409999999999997), (0.096110424882530784, 0.97350000000000003), (0.094810086611919961, 0.97489999999999999), (0.097963335103077448, 0.9738)], (0.091880895620131325, 0.97399999999999998))\n"
     ]
    }
   ],
   "source": [
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "print shelve_r['noAug0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guassian augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "#Set no aug\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=25, max_num_batches=250, randomize=True, augmentation = 1)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.00\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"gauAugExperimentsExp\", writeback = True)\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "    \n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['gauAug'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gauAug1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e494671cb3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshelve_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"augExperiments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mshelve_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gauAug1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shelve.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gauAug1'"
     ]
    }
   ],
   "source": [
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "print shelve_r['gauAug1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotation Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.556. Accuracy is 9.29%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.546. Accuracy is 70.56%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.531. Accuracy is 83.27%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 17 seconds. Training speed 528 pps. Validation speed 1839 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.522. Accuracy is 83.85%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.385. Accuracy is 88.80%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 18 seconds. Training speed 507 pps. Validation speed 1859 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.426. Accuracy is 87.08%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.328. Accuracy is 90.77%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 18 seconds. Training speed 516 pps. Validation speed 1692 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.353. Accuracy is 89.58%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.288. Accuracy is 91.76%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 18 seconds. Training speed 520 pps. Validation speed 1727 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.294. Accuracy is 91.42%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.268. Accuracy is 92.01%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 18 seconds. Training speed 513 pps. Validation speed 1690 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.251. Accuracy is 92.51%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.232. Accuracy is 93.48%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 17 seconds. Training speed 529 pps. Validation speed 1856 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.222. Accuracy is 93.33%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.224. Accuracy is 93.36%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 17 seconds. Training speed 526 pps. Validation speed 1800 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.199. Accuracy is 94.42%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.199. Accuracy is 94.25%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 17 seconds. Training speed 530 pps. Validation speed 1849 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.171. Accuracy is 94.99%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.214. Accuracy is 94.05%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 17 seconds. Training speed 544 pps. Validation speed 1835 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.157. Accuracy is 95.62%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.195. Accuracy is 94.46%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 18 seconds. Training speed 518 pps. Validation speed 1733 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.141. Accuracy is 96.01%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.185. Accuracy is 94.88%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 17 seconds. Training speed 516 pps. Validation speed 1894 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.128. Accuracy is 96.42%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.190. Accuracy is 94.35%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 18 seconds. Training speed 511 pps. Validation speed 1754 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.120. Accuracy is 96.70%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.162. Accuracy is 95.26%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 17 seconds. Training speed 523 pps. Validation speed 1818 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.111. Accuracy is 97.13%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.177. Accuracy is 94.71%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 18 seconds. Training speed 512 pps. Validation speed 1811 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.098. Accuracy is 97.31%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.173. Accuracy is 94.90%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 17 seconds. Training speed 548 pps. Validation speed 1851 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.090. Accuracy is 97.66%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.171. Accuracy is 94.70%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 17 seconds. Training speed 547 pps. Validation speed 1852 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.085. Accuracy is 97.66%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.162. Accuracy is 95.23%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 17 seconds. Training speed 555 pps. Validation speed 1848 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.081. Accuracy is 97.90%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.162. Accuracy is 95.23%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 17 seconds. Training speed 553 pps. Validation speed 1848 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.074. Accuracy is 98.17%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.162. Accuracy is 95.26%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 17 seconds. Training speed 555 pps. Validation speed 1838 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.069. Accuracy is 98.22%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.145. Accuracy is 96.01%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 17 seconds. Training speed 558 pps. Validation speed 1857 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.066. Accuracy is 98.38%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.147. Accuracy is 95.97%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 17 seconds. Training speed 550 pps. Validation speed 1840 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.063. Accuracy is 98.45%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.150. Accuracy is 95.69%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 17 seconds. Training speed 552 pps. Validation speed 1853 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.057. Accuracy is 98.71%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.146. Accuracy is 95.76%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 17 seconds. Training speed 563 pps. Validation speed 1826 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.058. Accuracy is 98.48%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.147. Accuracy is 95.91%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 17 seconds. Training speed 537 pps. Validation speed 1866 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.055. Accuracy is 98.65%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.142. Accuracy is 96.15%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 17 seconds. Training speed 548 pps. Validation speed 1866 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.047. Accuracy is 98.88%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.142. Accuracy is 96.05%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 17 seconds. Training speed 560 pps. Validation speed 1831 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.048. Accuracy is 98.83%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.141. Accuracy is 95.89%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 17 seconds. Training speed 546 pps. Validation speed 1851 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.044. Accuracy is 99.03%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.141. Accuracy is 96.05%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 17 seconds. Training speed 554 pps. Validation speed 1871 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.043. Accuracy is 99.02%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.144. Accuracy is 96.05%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 16 seconds. Training speed 566 pps. Validation speed 1871 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.039. Accuracy is 99.10%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.143. Accuracy is 96.15%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 17 seconds. Training speed 549 pps. Validation speed 1797 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 95.99 %, cost (ce) is 0.137\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.379. Accuracy is 9.71%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.380. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.381. Accuracy is 10.49%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.287. Accuracy is 9.83%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 15 seconds. Training speed 624 pps. Validation speed 1901 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.068. Accuracy is 23.06%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.410. Accuracy is 46.56%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 15 seconds. Training speed 625 pps. Validation speed 1908 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.152. Accuracy is 59.01%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.784. Accuracy is 71.64%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 15 seconds. Training speed 623 pps. Validation speed 1894 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.755. Accuracy is 75.10%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.505. Accuracy is 84.25%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 15 seconds. Training speed 634 pps. Validation speed 1817 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.615. Accuracy is 80.38%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.458. Accuracy is 85.46%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 15 seconds. Training speed 619 pps. Validation speed 1922 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.519. Accuracy is 83.53%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.378. Accuracy is 88.33%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 16 seconds. Training speed 609 pps. Validation speed 1905 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.460. Accuracy is 85.30%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.382. Accuracy is 87.81%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 15 seconds. Training speed 635 pps. Validation speed 1916 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.410. Accuracy is 86.93%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.302. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 16 seconds. Training speed 586 pps. Validation speed 1808 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.357. Accuracy is 88.44%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.335. Accuracy is 89.56%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 17 seconds. Training speed 559 pps. Validation speed 1843 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.325. Accuracy is 89.54%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.243. Accuracy is 92.51%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 16 seconds. Training speed 593 pps. Validation speed 1827 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.289. Accuracy is 91.01%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.270. Accuracy is 92.07%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 16 seconds. Training speed 583 pps. Validation speed 1900 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.260. Accuracy is 91.65%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.279. Accuracy is 91.58%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 16 seconds. Training speed 587 pps. Validation speed 1912 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.232. Accuracy is 92.53%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.220. Accuracy is 93.13%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 15 seconds. Training speed 641 pps. Validation speed 1902 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.210. Accuracy is 93.50%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.253. Accuracy is 92.13%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 16 seconds. Training speed 601 pps. Validation speed 1924 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.188. Accuracy is 93.97%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.252. Accuracy is 92.33%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 15 seconds. Training speed 622 pps. Validation speed 1924 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.185. Accuracy is 94.18%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.209. Accuracy is 93.56%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 15 seconds. Training speed 626 pps. Validation speed 1940 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.154. Accuracy is 95.11%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.181. Accuracy is 94.61%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 16 seconds. Training speed 611 pps. Validation speed 1822 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.146. Accuracy is 95.08%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.207. Accuracy is 93.58%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 16 seconds. Training speed 593 pps. Validation speed 1853 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.134. Accuracy is 95.70%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.256. Accuracy is 92.29%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 15 seconds. Training speed 632 pps. Validation speed 1916 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.125. Accuracy is 95.81%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.201. Accuracy is 94.18%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 15 seconds. Training speed 609 pps. Validation speed 1928 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.122. Accuracy is 96.32%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.189. Accuracy is 94.54%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 16 seconds. Training speed 612 pps. Validation speed 1871 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.106. Accuracy is 96.59%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.179. Accuracy is 94.75%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 16 seconds. Training speed 605 pps. Validation speed 1872 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.091. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.186. Accuracy is 94.77%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 16 seconds. Training speed 586 pps. Validation speed 1885 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.096. Accuracy is 97.10%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.208. Accuracy is 93.90%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 15 seconds. Training speed 611 pps. Validation speed 1908 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.092. Accuracy is 97.02%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.186. Accuracy is 94.87%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 15 seconds. Training speed 628 pps. Validation speed 1882 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.075. Accuracy is 97.66%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.176. Accuracy is 95.15%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 15 seconds. Training speed 617 pps. Validation speed 1885 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.072. Accuracy is 97.74%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.191. Accuracy is 94.88%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 16 seconds. Training speed 579 pps. Validation speed 1826 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.080. Accuracy is 97.47%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.168. Accuracy is 95.39%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 15 seconds. Training speed 625 pps. Validation speed 1890 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.065. Accuracy is 98.05%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.180. Accuracy is 95.12%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 15 seconds. Training speed 619 pps. Validation speed 1885 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.070. Accuracy is 97.84%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.166. Accuracy is 95.61%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 15 seconds. Training speed 619 pps. Validation speed 1933 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 95.48 %, cost (ce) is 0.164\n",
      "INFO:root:Saving Data\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "#Set no aug\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=25, max_num_batches=250, randomize=True, augmentation = 2)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.00\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "    \n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['rotAug'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(2.5562683854356489, 0.092879999999999921), (1.5462982589063765, 0.70559999999999989), (0.52189560304094995, 0.83848000000000067), (0.42601826261832176, 0.87079999999999957), (0.35337329232518222, 0.89584000000000008), (0.29377871768666169, 0.91415999999999931), (0.25147616865457145, 0.92511999999999917), (0.22159527630811471, 0.93327999999999944), (0.19857560650501502, 0.94423999999999864), (0.17065748879635703, 0.94991999999999932), (0.15739096503117883, 0.95615999999999868), (0.14088532762064696, 0.96007999999999927), (0.12800619988959416, 0.96423999999999932), (0.12028947523356515, 0.96695999999999938), (0.11102779961198238, 0.97127999999999903), (0.098303455248003366, 0.97311999999999876), (0.090107473644583214, 0.97663999999999851), (0.085240087924978794, 0.9765599999999991), (0.081285213966619668, 0.97903999999999891), (0.073936646915694307, 0.98167999999999878), (0.068661238422579068, 0.98223999999999922), (0.065987430741484179, 0.98383999999999849), (0.062576911035505542, 0.98447999999999836), (0.057157856532776434, 0.98711999999999878), (0.05836131241731192, 0.98479999999999834), (0.054861843143869965, 0.98647999999999847), (0.047313922558208675, 0.98879999999999912), (0.048282828617332518, 0.98831999999999864), (0.043959980024938103, 0.99031999999999909), (0.042513829937615939, 0.99015999999999904), (0.038962053094678914, 0.9910399999999987)], [(2.5535497258761009, 0.098400000000000001), (0.53095794079653513, 0.8327), (0.3847975785344892, 0.88800000000000001), (0.32811904856890112, 0.90769999999999995), (0.28833518032017907, 0.91759999999999997), (0.26849039268389641, 0.92010000000000003), (0.23190436179304252, 0.93479999999999996), (0.22357345145821295, 0.93359999999999999), (0.1988263761413088, 0.9425), (0.21354910981776962, 0.9405), (0.19521755192628301, 0.9446), (0.18535467485224727, 0.94879999999999998), (0.18957229794384883, 0.94350000000000001), (0.16209442276715202, 0.9526), (0.17689308919717014, 0.94710000000000005), (0.17309306076018746, 0.94899999999999995), (0.17128083105509678, 0.94699999999999995), (0.1616393732473787, 0.95230000000000004), (0.16191196993569271, 0.95230000000000004), (0.16190569925171841, 0.9526), (0.14470813146043601, 0.96009999999999995), (0.14665529683316686, 0.9597), (0.15002790943951697, 0.95689999999999997), (0.1459303194877232, 0.95760000000000001), (0.14678537446227194, 0.95909999999999995), (0.14214705362081301, 0.96150000000000002), (0.1421936739170602, 0.96050000000000002), (0.14088370097875541, 0.95889999999999997), (0.14141438870739392, 0.96050000000000002), (0.14364370884789432, 0.96050000000000002), (0.143336842679649, 0.96150000000000002)], (0.13665804875365636, 0.95989999999999998))\n"
     ]
    }
   ],
   "source": [
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "print shelve_r['rotAug0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "#Set no aug\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=25, max_num_batches=250, randomize=True, augmentation = 3)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.00\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"dpAugExperimentsExp\", writeback = True)\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "    \n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['dpAug'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dpAug0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9da473251b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshelve_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"augExperiments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mshelve_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dpAug0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shelve.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dpAug0'"
     ]
    }
   ],
   "source": [
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "print shelve_r['dpAug0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Shift Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.586. Accuracy is 8.68%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.770. Accuracy is 64.97%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.547. Accuracy is 83.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 18 seconds. Training speed 538 pps. Validation speed 1581 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.636. Accuracy is 80.63%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.416. Accuracy is 87.99%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 17 seconds. Training speed 577 pps. Validation speed 1630 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.496. Accuracy is 85.36%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.332. Accuracy is 91.27%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 17 seconds. Training speed 600 pps. Validation speed 1493 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.409. Accuracy is 88.20%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.291. Accuracy is 92.03%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 17 seconds. Training speed 594 pps. Validation speed 1600 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.348. Accuracy is 90.08%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.258. Accuracy is 92.76%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 17 seconds. Training speed 596 pps. Validation speed 1599 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.297. Accuracy is 91.32%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.232. Accuracy is 93.85%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 17 seconds. Training speed 587 pps. Validation speed 1539 pps.\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "#Set no aug\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=25, max_num_batches=250, randomize=True, augmentation = 4)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.00\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"siAugExperimentsExp\", writeback = True)\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "    \n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['siAug'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'siAug1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ddf041504d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshelve_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"augExperiments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mshelve_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'siAug1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shelve.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'siAug1'"
     ]
    }
   ],
   "source": [
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "print shelve_r['siAug1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.576. Accuracy is 8.94%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.561. Accuracy is 71.10%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.430. Accuracy is 87.57%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 15 seconds. Training speed 640 pps. Validation speed 1878 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.546. Accuracy is 84.19%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.366. Accuracy is 89.49%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 16 seconds. Training speed 629 pps. Validation speed 1762 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.445. Accuracy is 87.37%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.303. Accuracy is 90.87%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 16 seconds. Training speed 616 pps. Validation speed 1778 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.381. Accuracy is 89.10%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.300. Accuracy is 91.58%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 15 seconds. Training speed 636 pps. Validation speed 1807 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.335. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.261. Accuracy is 92.94%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 15 seconds. Training speed 635 pps. Validation speed 1892 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.281. Accuracy is 92.23%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.251. Accuracy is 93.03%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 16 seconds. Training speed 618 pps. Validation speed 1801 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.245. Accuracy is 93.29%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.227. Accuracy is 93.58%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 15 seconds. Training speed 645 pps. Validation speed 1833 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.209. Accuracy is 94.10%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.208. Accuracy is 94.06%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 16 seconds. Training speed 600 pps. Validation speed 1689 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.189. Accuracy is 94.70%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.183. Accuracy is 94.92%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 16 seconds. Training speed 618 pps. Validation speed 1826 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.170. Accuracy is 95.50%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.178. Accuracy is 95.15%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 16 seconds. Training speed 621 pps. Validation speed 1629 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.149. Accuracy is 95.90%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.161. Accuracy is 95.34%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 16 seconds. Training speed 609 pps. Validation speed 1804 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.139. Accuracy is 96.48%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.155. Accuracy is 95.60%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 16 seconds. Training speed 618 pps. Validation speed 1802 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.125. Accuracy is 96.56%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.159. Accuracy is 95.55%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 16 seconds. Training speed 633 pps. Validation speed 1753 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.113. Accuracy is 97.27%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.147. Accuracy is 95.76%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 16 seconds. Training speed 624 pps. Validation speed 1790 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.105. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.164. Accuracy is 95.12%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 16 seconds. Training speed 629 pps. Validation speed 1788 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.099. Accuracy is 97.60%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.148. Accuracy is 95.87%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 16 seconds. Training speed 625 pps. Validation speed 1781 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.093. Accuracy is 97.85%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.138. Accuracy is 96.14%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 16 seconds. Training speed 610 pps. Validation speed 1672 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.093. Accuracy is 97.86%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.136. Accuracy is 96.09%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 16 seconds. Training speed 580 pps. Validation speed 1789 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.079. Accuracy is 98.04%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.135. Accuracy is 96.02%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 16 seconds. Training speed 598 pps. Validation speed 1817 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.075. Accuracy is 98.26%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.141. Accuracy is 95.90%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 16 seconds. Training speed 623 pps. Validation speed 1761 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.073. Accuracy is 98.15%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.134. Accuracy is 96.20%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 15 seconds. Training speed 639 pps. Validation speed 1784 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.073. Accuracy is 98.14%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.133. Accuracy is 96.13%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 16 seconds. Training speed 610 pps. Validation speed 1730 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.067. Accuracy is 98.50%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.132. Accuracy is 96.08%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 15 seconds. Training speed 628 pps. Validation speed 1847 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.063. Accuracy is 98.46%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.129. Accuracy is 96.40%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 16 seconds. Training speed 612 pps. Validation speed 1767 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.059. Accuracy is 98.62%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.128. Accuracy is 96.41%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 16 seconds. Training speed 632 pps. Validation speed 1759 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.060. Accuracy is 98.57%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.129. Accuracy is 96.11%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 15 seconds. Training speed 633 pps. Validation speed 1797 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.056. Accuracy is 98.76%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.129. Accuracy is 96.27%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 16 seconds. Training speed 633 pps. Validation speed 1770 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.056. Accuracy is 98.70%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.129. Accuracy is 96.30%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 16 seconds. Training speed 608 pps. Validation speed 1803 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.051. Accuracy is 98.82%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.124. Accuracy is 96.37%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 15 seconds. Training speed 629 pps. Validation speed 1817 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.052. Accuracy is 98.81%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.125. Accuracy is 96.41%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 16 seconds. Training speed 627 pps. Validation speed 1790 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 96.47 %, cost (ce) is 0.120\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.379. Accuracy is 9.71%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.380. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.395. Accuracy is 10.72%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.303. Accuracy is 9.67%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 14 seconds. Training speed 718 pps. Validation speed 1873 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.292. Accuracy is 12.78%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 2.217. Accuracy is 27.07%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 14 seconds. Training speed 739 pps. Validation speed 1926 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.777. Accuracy is 34.82%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.193. Accuracy is 52.65%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 14 seconds. Training speed 711 pps. Validation speed 1889 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.020. Accuracy is 65.54%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.601. Accuracy is 81.07%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 14 seconds. Training speed 724 pps. Validation speed 1903 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.741. Accuracy is 76.72%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.449. Accuracy is 86.75%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 14 seconds. Training speed 721 pps. Validation speed 1839 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.608. Accuracy is 81.30%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.399. Accuracy is 87.93%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 14 seconds. Training speed 736 pps. Validation speed 1857 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.526. Accuracy is 83.87%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.372. Accuracy is 88.38%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 14 seconds. Training speed 715 pps. Validation speed 1816 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.461. Accuracy is 85.74%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.311. Accuracy is 90.63%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 14 seconds. Training speed 695 pps. Validation speed 1902 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.422. Accuracy is 86.63%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.301. Accuracy is 90.67%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 14 seconds. Training speed 724 pps. Validation speed 1842 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.383. Accuracy is 87.94%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.272. Accuracy is 91.68%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 14 seconds. Training speed 724 pps. Validation speed 1896 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.346. Accuracy is 89.32%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.274. Accuracy is 91.80%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 14 seconds. Training speed 709 pps. Validation speed 1856 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.296. Accuracy is 91.03%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.249. Accuracy is 92.33%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 14 seconds. Training speed 716 pps. Validation speed 1896 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.289. Accuracy is 91.00%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.263. Accuracy is 92.10%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 14 seconds. Training speed 711 pps. Validation speed 1895 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.253. Accuracy is 92.38%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.226. Accuracy is 93.09%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 14 seconds. Training speed 718 pps. Validation speed 1986 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.226. Accuracy is 93.08%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.214. Accuracy is 93.67%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 14 seconds. Training speed 722 pps. Validation speed 1906 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.212. Accuracy is 93.57%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.216. Accuracy is 93.72%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 14 seconds. Training speed 699 pps. Validation speed 1900 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.199. Accuracy is 94.02%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.174. Accuracy is 95.03%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 14 seconds. Training speed 718 pps. Validation speed 1873 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.165. Accuracy is 95.24%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.202. Accuracy is 94.23%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 14 seconds. Training speed 710 pps. Validation speed 1977 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.160. Accuracy is 95.20%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.175. Accuracy is 94.52%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 14 seconds. Training speed 710 pps. Validation speed 1886 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.136. Accuracy is 96.15%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.209. Accuracy is 94.20%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 14 seconds. Training speed 741 pps. Validation speed 1773 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.127. Accuracy is 96.24%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.184. Accuracy is 94.69%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 15 seconds. Training speed 674 pps. Validation speed 1788 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.116. Accuracy is 96.69%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.166. Accuracy is 95.17%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 14 seconds. Training speed 709 pps. Validation speed 1799 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.116. Accuracy is 96.54%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.161. Accuracy is 95.43%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 14 seconds. Training speed 686 pps. Validation speed 1894 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.102. Accuracy is 97.12%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.169. Accuracy is 95.29%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 14 seconds. Training speed 755 pps. Validation speed 1876 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.097. Accuracy is 97.02%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.170. Accuracy is 95.24%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 14 seconds. Training speed 751 pps. Validation speed 1884 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.092. Accuracy is 97.37%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.159. Accuracy is 95.57%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 14 seconds. Training speed 716 pps. Validation speed 1832 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.092. Accuracy is 97.30%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.173. Accuracy is 95.30%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 14 seconds. Training speed 750 pps. Validation speed 1827 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.080. Accuracy is 97.64%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.166. Accuracy is 95.44%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 14 seconds. Training speed 717 pps. Validation speed 1862 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.091. Accuracy is 97.19%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.208. Accuracy is 94.13%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 14 seconds. Training speed 722 pps. Validation speed 1890 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.073. Accuracy is 97.73%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.177. Accuracy is 95.17%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 15 seconds. Training speed 687 pps. Validation speed 1834 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 95.48 %, cost (ce) is 0.165\n",
      "INFO:root:Saving Data\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "#Set no aug\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=25, max_num_batches=250, randomize=True, augmentation = 5)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.00\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "    \n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['ranAug'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(2.5764682227846394, 0.089439999999999811), (1.5609751983696472, 0.71104000000000056), (0.54552691188002445, 0.84192000000000022), (0.44474780014951787, 0.87368000000000046), (0.38089699144193384, 0.89104000000000005), (0.33450930787410083, 0.90312000000000048), (0.28111618041248881, 0.92231999999999958), (0.2445538690417324, 0.93287999999999927), (0.20922008233164727, 0.94103999999999932), (0.18941796677622746, 0.9469599999999988), (0.16984181609687674, 0.95503999999999878), (0.14939115243557247, 0.95903999999999889), (0.13943360034734933, 0.96479999999999866), (0.12470625478090855, 0.96559999999999924), (0.11335153012250314, 0.97271999999999914), (0.10506183880711599, 0.97263999999999884), (0.099271112707728537, 0.97599999999999931), (0.092739087160186875, 0.97847999999999846), (0.092797675219459252, 0.97863999999999884), (0.079184470418435765, 0.98039999999999861), (0.075331608062100874, 0.98255999999999832), (0.072937061354576366, 0.98151999999999895), (0.07320546749959457, 0.98143999999999842), (0.066812475656684675, 0.98503999999999858), (0.063141379330105329, 0.98463999999999896), (0.059441830592227393, 0.98623999999999934), (0.059505719052917717, 0.98567999999999878), (0.055844066364537412, 0.98759999999999837), (0.05627042319058153, 0.98695999999999873), (0.050768521738684201, 0.98815999999999893), (0.051514689595047375, 0.98807999999999863)], [(2.5535497258761009, 0.098400000000000001), (0.42950846303533574, 0.87570000000000003), (0.36633853396523025, 0.89490000000000003), (0.30312223335450028, 0.90869999999999995), (0.29973977680142549, 0.91579999999999995), (0.26148935126052603, 0.9294), (0.25078757766645904, 0.93030000000000002), (0.2269921174407612, 0.93579999999999997), (0.20813730640478292, 0.94059999999999999), (0.18344012004481314, 0.94920000000000004), (0.17777984132471442, 0.95150000000000001), (0.16112643173359609, 0.95340000000000003), (0.15545742655515582, 0.95599999999999996), (0.15861021523544036, 0.95550000000000002), (0.14715460913297757, 0.95760000000000001), (0.1644013093522885, 0.95120000000000005), (0.14845442849221135, 0.9587), (0.13761880763340545, 0.96140000000000003), (0.13613548003037609, 0.96089999999999998), (0.13535366523322484, 0.96020000000000005), (0.14071446490030129, 0.95899999999999996), (0.13418408500476303, 0.96199999999999997), (0.13335496605237773, 0.96130000000000004), (0.13216752255465908, 0.96079999999999999), (0.12889259287657281, 0.96399999999999997), (0.12848034734130909, 0.96409999999999996), (0.1289382837082563, 0.96109999999999995), (0.1292296481322662, 0.9627), (0.12856114079054881, 0.96299999999999997), (0.1244286717764785, 0.9637), (0.12518512629619757, 0.96409999999999996)], (0.11993297480218293, 0.9647))\n"
     ]
    }
   ],
   "source": [
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "print shelve_r['ranAug0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.612. Accuracy is 8.54%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 3.339. Accuracy is 50.46%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.716. Accuracy is 78.08%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 9 seconds. Training speed 249 pps. Validation speed 1873 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.927. Accuracy is 71.14%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.526. Accuracy is 83.48%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 10 seconds. Training speed 236 pps. Validation speed 1753 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.740. Accuracy is 77.06%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.489. Accuracy is 85.75%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 10 seconds. Training speed 229 pps. Validation speed 1807 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.652. Accuracy is 80.84%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.441. Accuracy is 87.25%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 10 seconds. Training speed 233 pps. Validation speed 1817 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.536. Accuracy is 84.12%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.444. Accuracy is 85.97%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 10 seconds. Training speed 235 pps. Validation speed 1832 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.482. Accuracy is 85.78%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.449. Accuracy is 87.16%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 9 seconds. Training speed 230 pps. Validation speed 1951 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.430. Accuracy is 88.20%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.368. Accuracy is 89.67%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 10 seconds. Training speed 231 pps. Validation speed 1806 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.384. Accuracy is 88.98%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.346. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 10 seconds. Training speed 237 pps. Validation speed 1865 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.333. Accuracy is 91.12%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.378. Accuracy is 89.17%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 10 seconds. Training speed 228 pps. Validation speed 1835 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.311. Accuracy is 91.52%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.335. Accuracy is 90.34%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 10 seconds. Training speed 230 pps. Validation speed 1876 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.275. Accuracy is 92.04%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.356. Accuracy is 89.57%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 10 seconds. Training speed 237 pps. Validation speed 1840 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.251. Accuracy is 93.54%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.347. Accuracy is 89.71%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 10 seconds. Training speed 247 pps. Validation speed 1788 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.219. Accuracy is 94.12%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.305. Accuracy is 91.40%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 10 seconds. Training speed 229 pps. Validation speed 1815 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.198. Accuracy is 94.54%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.378. Accuracy is 88.91%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 10 seconds. Training speed 231 pps. Validation speed 1866 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.180. Accuracy is 95.38%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.322. Accuracy is 90.94%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 10 seconds. Training speed 238 pps. Validation speed 1821 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.183. Accuracy is 95.04%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.350. Accuracy is 90.09%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 10 seconds. Training speed 230 pps. Validation speed 1805 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.163. Accuracy is 95.70%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.289. Accuracy is 91.92%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 9 seconds. Training speed 242 pps. Validation speed 1899 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.165. Accuracy is 95.40%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.310. Accuracy is 91.45%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 10 seconds. Training speed 237 pps. Validation speed 1836 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.144. Accuracy is 96.12%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.318. Accuracy is 90.45%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 10 seconds. Training speed 237 pps. Validation speed 1798 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.143. Accuracy is 96.22%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.298. Accuracy is 91.11%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 10 seconds. Training speed 235 pps. Validation speed 1799 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.127. Accuracy is 96.86%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.290. Accuracy is 91.82%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 10 seconds. Training speed 235 pps. Validation speed 1812 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.132. Accuracy is 96.28%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.291. Accuracy is 91.50%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 10 seconds. Training speed 236 pps. Validation speed 1849 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.121. Accuracy is 96.82%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.270. Accuracy is 92.59%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 10 seconds. Training speed 247 pps. Validation speed 1826 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.115. Accuracy is 96.90%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.268. Accuracy is 92.57%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 10 seconds. Training speed 225 pps. Validation speed 1795 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.118. Accuracy is 97.10%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.286. Accuracy is 92.30%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 10 seconds. Training speed 237 pps. Validation speed 1811 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.102. Accuracy is 97.36%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.284. Accuracy is 91.83%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 10 seconds. Training speed 239 pps. Validation speed 1816 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.101. Accuracy is 97.26%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.277. Accuracy is 92.11%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 10 seconds. Training speed 229 pps. Validation speed 1833 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.100. Accuracy is 97.44%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.271. Accuracy is 92.50%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 9 seconds. Training speed 235 pps. Validation speed 1911 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.097. Accuracy is 97.54%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.285. Accuracy is 92.02%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 10 seconds. Training speed 229 pps. Validation speed 1831 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.088. Accuracy is 97.56%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.264. Accuracy is 92.72%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 10 seconds. Training speed 238 pps. Validation speed 1820 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 92.35 %, cost (ce) is 0.265\n",
      "INFO:root:Starting \n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.377. Accuracy is 9.10%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.380. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.542. Accuracy is 11.62%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.308. Accuracy is 10.64%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 9 seconds. Training speed 273 pps. Validation speed 1911 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.316. Accuracy is 9.76%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 2.307. Accuracy is 10.30%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 9 seconds. Training speed 264 pps. Validation speed 1968 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 2.314. Accuracy is 9.26%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 2.311. Accuracy is 10.64%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 9 seconds. Training speed 271 pps. Validation speed 1959 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 2.311. Accuracy is 11.14%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 2.307. Accuracy is 9.61%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 9 seconds. Training speed 270 pps. Validation speed 1918 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 2.306. Accuracy is 9.92%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 2.295. Accuracy is 18.82%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 9 seconds. Training speed 273 pps. Validation speed 1904 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 2.291. Accuracy is 12.94%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 2.269. Accuracy is 10.93%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 9 seconds. Training speed 274 pps. Validation speed 1877 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 2.202. Accuracy is 18.96%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 1.986. Accuracy is 22.25%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 9 seconds. Training speed 270 pps. Validation speed 1893 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 2.008. Accuracy is 25.52%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 1.702. Accuracy is 38.27%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 9 seconds. Training speed 290 pps. Validation speed 1860 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 1.695. Accuracy is 36.98%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 1.274. Accuracy is 50.53%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 9 seconds. Training speed 263 pps. Validation speed 1868 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 1.396. Accuracy is 49.06%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 1.066. Accuracy is 61.62%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 9 seconds. Training speed 269 pps. Validation speed 1923 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 1.228. Accuracy is 56.54%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.940. Accuracy is 68.50%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 9 seconds. Training speed 256 pps. Validation speed 1808 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 1.084. Accuracy is 63.26%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.772. Accuracy is 74.59%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 10 seconds. Training speed 242 pps. Validation speed 1806 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.996. Accuracy is 65.28%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.671. Accuracy is 78.12%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 10 seconds. Training speed 245 pps. Validation speed 1801 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.923. Accuracy is 69.54%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.669. Accuracy is 79.90%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 10 seconds. Training speed 233 pps. Validation speed 1720 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.847. Accuracy is 71.70%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.690. Accuracy is 77.24%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 10 seconds. Training speed 240 pps. Validation speed 1777 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.771. Accuracy is 74.38%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.656. Accuracy is 77.90%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 9 seconds. Training speed 258 pps. Validation speed 1868 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.737. Accuracy is 74.92%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.591. Accuracy is 81.47%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 9 seconds. Training speed 259 pps. Validation speed 1787 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.699. Accuracy is 76.44%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.496. Accuracy is 84.52%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 9 seconds. Training speed 251 pps. Validation speed 1857 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.607. Accuracy is 79.26%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.660. Accuracy is 80.76%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 9 seconds. Training speed 251 pps. Validation speed 1830 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.595. Accuracy is 80.56%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.567. Accuracy is 80.44%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 9 seconds. Training speed 263 pps. Validation speed 1869 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.529. Accuracy is 83.20%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.462. Accuracy is 85.58%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 10 seconds. Training speed 250 pps. Validation speed 1812 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.493. Accuracy is 84.46%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.454. Accuracy is 86.20%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 9 seconds. Training speed 273 pps. Validation speed 1904 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.474. Accuracy is 84.96%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.413. Accuracy is 87.61%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 9 seconds. Training speed 264 pps. Validation speed 1996 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.457. Accuracy is 85.46%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.420. Accuracy is 87.32%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 9 seconds. Training speed 272 pps. Validation speed 1901 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.389. Accuracy is 88.04%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.387. Accuracy is 88.20%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 9 seconds. Training speed 263 pps. Validation speed 1861 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.365. Accuracy is 89.34%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.441. Accuracy is 87.13%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 9 seconds. Training speed 250 pps. Validation speed 1837 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.356. Accuracy is 88.82%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.361. Accuracy is 89.10%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 9 seconds. Training speed 267 pps. Validation speed 1869 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.340. Accuracy is 89.58%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.346. Accuracy is 89.94%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 9 seconds. Training speed 270 pps. Validation speed 1738 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.317. Accuracy is 89.78%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.404. Accuracy is 88.18%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 9 seconds. Training speed 292 pps. Validation speed 1874 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.308. Accuracy is 90.62%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.413. Accuracy is 88.56%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 9 seconds. Training speed 265 pps. Validation speed 1855 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 87.89 %, cost (ce) is 0.422\n",
      "INFO:root:Saving Data\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "#Set no aug\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True, augmentation = 6)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.00\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 2\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,2):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer == 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer == 1:\n",
    "        odim = 300\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        model.add_layer(Sigmoid(idim=odim, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "    \n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['allAug'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(2.6121181065285213, 0.085399999999999934), (3.3386188054325641, 0.50460000000000005), (0.92744783493968019, 0.71140000000000014), (0.73978559840762348, 0.77060000000000017), (0.65191161521539442, 0.80840000000000001), (0.5356441589359745, 0.84120000000000017), (0.48156223556080574, 0.85779999999999967), (0.42952588115839468, 0.88200000000000012), (0.3837260789334026, 0.88980000000000004), (0.33331038473017471, 0.9111999999999999), (0.31116090080893472, 0.91519999999999957), (0.27474795056682294, 0.92039999999999988), (0.25136101986779752, 0.9353999999999999), (0.21947064786448864, 0.94119999999999937), (0.19793114700923031, 0.94539999999999946), (0.17974359308773238, 0.95379999999999943), (0.18305838239454297, 0.95039999999999936), (0.16257642352225748, 0.95699999999999985), (0.16464408652818271, 0.95400000000000007), (0.1438111786511668, 0.9611999999999995), (0.14314446972757594, 0.96219999999999939), (0.12716693398533691, 0.96859999999999946), (0.13241150587049533, 0.96279999999999977), (0.12068308820832811, 0.96819999999999917), (0.11546354675799775, 0.96900000000000008), (0.11805480551758075, 0.97099999999999942), (0.10242067540916651, 0.97359999999999924), (0.10141303594253075, 0.9725999999999998), (0.099547267069103912, 0.97439999999999982), (0.096912893551000162, 0.9753999999999996), (0.088059261918242648, 0.97559999999999969)], [(2.5535497258761009, 0.098400000000000001), (0.71594241389670832, 0.78080000000000005), (0.52622499808529122, 0.83479999999999999), (0.48876468753833607, 0.85750000000000004), (0.44148080812452029, 0.87250000000000005), (0.44448365445892357, 0.85970000000000002), (0.44902268720658839, 0.87160000000000004), (0.36844807101202837, 0.89670000000000005), (0.34600993066572661, 0.9032), (0.37798804525467922, 0.89170000000000005), (0.33489942131395717, 0.90339999999999998), (0.3563275315819055, 0.89570000000000005), (0.34651449538728785, 0.89710000000000001), (0.30476095189236235, 0.91400000000000003), (0.37769567026557049, 0.8891), (0.32192956326135275, 0.90939999999999999), (0.35008866557796592, 0.90090000000000003), (0.28869389968738374, 0.91920000000000002), (0.31036978609397559, 0.91449999999999998), (0.318354331305436, 0.90449999999999997), (0.29831928891037235, 0.91110000000000002), (0.29047910043313879, 0.91820000000000002), (0.29148025968604413, 0.91500000000000004), (0.27035655181512891, 0.92589999999999995), (0.26774179750055627, 0.92569999999999997), (0.2863644899202441, 0.92300000000000004), (0.28422448405616968, 0.91830000000000001), (0.27650801382157747, 0.92110000000000003), (0.27080538139917504, 0.92500000000000004), (0.28538326908369399, 0.92020000000000002), (0.2640530811081549, 0.92720000000000002)], (0.26513090460045308, 0.92349999999999999))\n"
     ]
    }
   ],
   "source": [
    "shelve_r = shelve.open(\"augExperiments\")\n",
    "\n",
    "print shelve_r['allAug0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
