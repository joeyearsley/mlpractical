{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial focuses on implementation of three reqularisaion techniques, two of them are norm based approaches which are added to optimised objective and the third technique, called *droput*, is a form of noise injection by random corruption of information carried by hidden units during training.\n",
    "\n",
    "\n",
    "## Virtual environments\n",
    "\n",
    "Before you proceed onwards, remember to activate your virtual environment:\n",
    "   * If you were in last week's Tuesday or Wednesday group type `activate_mlp` or `source ~/mlpractical/venv/bin/activate`\n",
    "   * If you were in the Monday group:\n",
    "      + and if you have chosen the **comfy** way type: `workon mlpractical`\n",
    "      + and if you have chosen the **generic** way, `source` your virutal environment using `source` and specyfing the path to the activate script (you need to localise it yourself, there were not any general recommendations w.r.t dir structure and people have installed it in different places, usually somewhere in the home directories. If you cannot easily find it by yourself, use something like: `find . -iname activate` ):\n",
    "\n",
    "## Syncing the git repository\n",
    "\n",
    "Look <a href=\"https://github.com/CSTR-Edinburgh/mlpractical/blob/master/gitFAQ.md\">here</a> for more details. But in short, we recommend to create a separate branch for this lab, as follows:\n",
    "\n",
    "1. Enter the mlpractical directory `cd ~/mlpractical/repo-mlp`\n",
    "2. List the branches and check which is currently active by typing: `git branch`\n",
    "3. If you have followed our recommendations, you should be in the `coursework1` branch, please commit your local changed to the repo index by typing:\n",
    "```\n",
    "git commit -am \"finished coursework\"\n",
    "```\n",
    "4. Now you can switch to `master` branch by typing: \n",
    "```\n",
    "git checkout master\n",
    " ```\n",
    "5. To update the repository (note, assuming master does not have any conflicts), if there are some, have a look <a href=\"https://github.com/CSTR-Edinburgh/mlpractical/blob/master/gitFAQ.md\">here</a>\n",
    "```\n",
    "git pull\n",
    "```\n",
    "6. And now, create the new branch & swith to it by typing:\n",
    "```\n",
    "git checkout -b lab4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation\n",
    "\n",
    "Regularisation add some *complexity term* to the cost function. It's purpose is to put some prior on the model's parameters. The most common prior is perhaps the one which assumes smoother solutions (the one which are not able to fit training data too well) are better as they are more likely to better generalise to unseen data. \n",
    "\n",
    "A way to incorporate such prior in the model is to add some term that penalise certain configurations of the parameters -- either from growing too large ($L_2$) or the one that prefers solution that could be modelled with less parameters ($L_1$), hence encouraging some parameters to become 0. One can, of course, combine many such priors when optimising the model, however, in the lab we shall use $L_1$ and/or $L_2$ priors.\n",
    "\n",
    "They can be easily incorporated into the training objective by adding some additive terms, as follows:\n",
    "\n",
    "(1) $\n",
    " \\begin{align*}\n",
    "        E^n &= \\underbrace{E^n_{\\text{train}}}_{\\text{data term}} + \n",
    "    \\underbrace{\\beta_{L_1} E^n_{L_1}}_{\\text{prior term}} + \\underbrace{\\beta_{L_2} E^n_{L_2}}_{\\text{prior term}}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $ E^n_{\\text{train}} = - \\sum_{k=1}^K t^n_k \\ln y^n_k $,  $\\beta_{L_1}$ and $\\beta_{L_2}$ some non-negative constants specified a priori (hyper-parameters) and $E^n_{L_1}$ and $E^n_{L_2}$ norm metric specifying certain properties of parameters:\n",
    "\n",
    "(2) $\n",
    " \\begin{align*}\n",
    " E^n_{L_p}(\\mathbf{W}) = \\left ( \\sum_{i,j \\in \\mathbf{W}} |w_{i,j}|^p \\right )^{\\frac{1}{p}}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $p$ denotes the norm-order (for regularisation either 1 or 2). (TODO: explain here why we usualy skip square root for p=2)\n",
    "\n",
    "## $L_{p=2}$ (Weight Decay)\n",
    "\n",
    "(3) $\n",
    " \\begin{align*}\n",
    "        E^n &= \\underbrace{E^n_{\\text{train}}}_{\\text{data term}} + \n",
    "    \\underbrace{\\beta E^n_{L_2}}_{\\text{prior term}} = E^n_{\\text{train}} + \\beta_{L_2} \\frac{1}{2}|w_i|^2\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "(4) $\n",
    "\\begin{align*}\\frac{\\partial E^n}{\\partial w_i} &= \\frac{\\partial (E^n_{\\text{train}} + \\beta_{L_2} E_{L_2}) }{\\partial w_i} \n",
    "  = \\left( \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_2} \\frac{\\partial\n",
    "      E_{L_2}}{\\partial w_i} \\right) \n",
    "  = \\left( \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_2} w_i \\right)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "(5) $\n",
    "\\begin{align*}\n",
    "  \\Delta w_i &= -\\eta \\left( \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_2} w_i \\right) \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $\\eta$ is learning rate.\n",
    "\n",
    "## $L_{p=1}$ (Sparsity)\n",
    "\n",
    "(6) $\n",
    " \\begin{align*}\n",
    "        E^n &= \\underbrace{E^n_{\\text{train}}}_{\\text{data term}} + \n",
    "    \\underbrace{\\beta E^n_{L_1}}_{\\text{prior term}} \n",
    "        = E^n_{\\text{train}} + \\beta_{L_1} |w_i|\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "(7) $\\begin{align*}\n",
    "  \\frac{\\partial E^n}{\\partial w_i} =  \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_1} \\frac{\\partial E_{L_1}}{\\partial w_i}  =  \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_1}  \\mbox{sgn}(w_i)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "(8) $\\begin{align*}\n",
    "  \\Delta w_i &= -\\eta \\left( \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_1} \\mbox{sgn}(w_i) \\right) \n",
    "\\end{align*}$\n",
    "\n",
    "Where $\\mbox{sgn}(w_i)$ is the sign of $w_i$: $\\mbox{sgn}(w_i) = 1$ if $w_i>0$ and $\\mbox{sgn}(w_i) = -1$ if $w_i<0$\n",
    "\n",
    "One can also apply those penalty terms for biases, however, this is usually not necessary as biases have secondary impact on smoothnes of the given solution.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout, for a given layer's output $\\mathbf{h}^i \\in \\mathbb{R}^{BxH^l}$ (where $B$ is batch size and $H^l$ is the $l$-th layer output dimensionality) implements the following transformation:\n",
    "\n",
    "(9) $\\mathbf{\\hat h}^l = \\mathbf{d}^l\\circ\\mathbf{h}^l$\n",
    "\n",
    "where $\\circ$ denotes an elementwise product and $\\mathbf{d}^l \\in \\{0,1\\}^{BxH^i}$ is a matrix in which $d^l_{ij}$ element is sampled from the Bernoulli distribution:\n",
    "\n",
    "(10) $d^l_{ij} \\sim \\mbox{Bernoulli}(p^l_d)$\n",
    "\n",
    "with $0<p^l_d<1$ denoting the probability the given unit is kept unchanged (dropping probability is thus $1-p^l_d$). We ignore here edge scenarios where $p^l_d=1$ and there is no dropout applied (and the training would be exactly the same as in standard SGD) and $p^l_d=0$ where all units would have been dropped, hence the model would not learn anything.\n",
    "\n",
    "The probability $p^l_d$ is a hyperparameter (like learning rate) meaning it needs to be provided before training and also very often tuned for the given task. As the notation suggest, it can be specified separately for each layer, including scenario where $l=0$ when some random input features (pixels in the image for MNIST) are being also ommitted.\n",
    "\n",
    "### Keeping the $l$-th layer output $\\mathbf{\\hat h}^l$ (input to the upper layer) appropiately scaled at test-time\n",
    "\n",
    "The other issue one needs to take into account is the mismatch that arises between training and test (runtime) stages when dropout is applied. It is due to the fact that droput is not applied when testing hence the average input to the unit in upper layer is going to be bigger when compared to training stage (where some inputs are set to 0), in average $1/p^l_d$ times bigger. \n",
    "\n",
    "So to account for this mismatch one could either:\n",
    "\n",
    "1. When training is finished scale the final weight matrices $\\mathbf{W}^l, l=1,\\ldots,L$ by $p^{l-1}_d$ (remember, $p^{0}_d$ is the probability related to the input features)\n",
    "2. Scale the activations in equation (9) during training, that is, for each mini-batch multiply $\\mathbf{\\hat h}^l$ by $1/p^l_d$ to compensate for dropped units and then at run-time use the model as usual, **without** scaling. Make sure the $1/p^l_d$ scaler is taken into account for both forward and backward passes.\n",
    "\n",
    "\n",
    "Our recommendation is option 2 as it will make some things easier from implementation perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True, augmentation=3)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.624. Accuracy is 8.60%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.851. Accuracy is 61.50%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.594. Accuracy is 81.14%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 7 seconds. Training speed 425 pps. Validation speed 2030 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.502. Accuracy is 84.10%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.460. Accuracy is 86.49%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 8 seconds. Training speed 371 pps. Validation speed 1920 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.334. Accuracy is 89.50%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.666. Accuracy is 80.03%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 8 seconds. Training speed 386 pps. Validation speed 1965 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.259. Accuracy is 92.30%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.432. Accuracy is 87.61%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 8 seconds. Training speed 369 pps. Validation speed 2020 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.188. Accuracy is 94.80%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.396. Accuracy is 88.48%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 8 seconds. Training speed 382 pps. Validation speed 2011 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.129. Accuracy is 97.20%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.409. Accuracy is 88.38%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 7 seconds. Training speed 449 pps. Validation speed 1973 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.090. Accuracy is 98.40%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.379. Accuracy is 89.74%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 7 seconds. Training speed 445 pps. Validation speed 2056 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.072. Accuracy is 98.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.386. Accuracy is 89.87%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 7 seconds. Training speed 388 pps. Validation speed 2038 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.054. Accuracy is 99.50%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.400. Accuracy is 89.56%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 7 seconds. Training speed 444 pps. Validation speed 2056 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.042. Accuracy is 99.30%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.396. Accuracy is 89.89%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 7 seconds. Training speed 441 pps. Validation speed 2020 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.032. Accuracy is 99.70%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.431. Accuracy is 88.88%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 7 seconds. Training speed 442 pps. Validation speed 1974 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.024. Accuracy is 99.70%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.408. Accuracy is 89.64%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 8 seconds. Training speed 366 pps. Validation speed 1992 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.020. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.420. Accuracy is 89.82%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 7 seconds. Training speed 441 pps. Validation speed 1972 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.017. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.420. Accuracy is 89.93%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 7 seconds. Training speed 451 pps. Validation speed 1943 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.014. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.432. Accuracy is 89.69%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 7 seconds. Training speed 446 pps. Validation speed 1910 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.013. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.430. Accuracy is 89.79%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 7 seconds. Training speed 383 pps. Validation speed 2052 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.011. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.424. Accuracy is 90.21%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 8 seconds. Training speed 373 pps. Validation speed 2060 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.010. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.429. Accuracy is 90.18%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 8 seconds. Training speed 386 pps. Validation speed 2034 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.009. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.439. Accuracy is 89.85%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 8 seconds. Training speed 383 pps. Validation speed 1934 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.008. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.437. Accuracy is 90.12%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 7 seconds. Training speed 385 pps. Validation speed 2058 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.008. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.441. Accuracy is 90.01%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 8 seconds. Training speed 382 pps. Validation speed 2016 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.007. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.445. Accuracy is 89.89%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 8 seconds. Training speed 384 pps. Validation speed 1983 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.007. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.445. Accuracy is 90.09%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 8 seconds. Training speed 381 pps. Validation speed 1975 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.006. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.449. Accuracy is 89.97%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 7 seconds. Training speed 385 pps. Validation speed 2065 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.006. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.450. Accuracy is 90.08%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 8 seconds. Training speed 385 pps. Validation speed 2024 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.006. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.449. Accuracy is 90.03%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 8 seconds. Training speed 376 pps. Validation speed 2040 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.005. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.453. Accuracy is 90.14%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 8 seconds. Training speed 394 pps. Validation speed 1994 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.005. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.455. Accuracy is 89.96%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 7 seconds. Training speed 389 pps. Validation speed 2057 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.005. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.460. Accuracy is 89.91%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 7 seconds. Training speed 387 pps. Validation speed 2058 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.005. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.456. Accuracy is 90.20%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 7 seconds. Training speed 389 pps. Validation speed 2039 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 89.44 %, cost (ce) is 0.464\n"
     ]
    }
   ],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Implement L1 based regularisation\n",
    "\n",
    "Implement L1 regularisation penalty (just for weight matrices, ignore biases). Test your solution on one hidden layer model similar to the one from coursework's Task 4 (800 hidden units) but limit training data to 10 000 (random) data-points (keep validation and test sets the same). First build and train not-regularised model as a basline. Then train regularised model starting with $\\beta_{L1}$ set to 0.001 and do some grid search for better values. Plot validation accuracies as a function of epochs for each model (each $\\beta_{L1}$ you tried).\n",
    "\n",
    "Implementation tips:\n",
    "* Have a look at the constructor of mlp.optimiser.SGDOptimiser class, it has been modified to take more optimisation-related arguments.\n",
    "* The best place to implement regularisation terms is `pgrads` method of mlp.layers.Layer (sub)-classes (look at equations (5) and (8) to see why). Some modifications are also required in `train_epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 8.934. Accuracy is 8.60%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 8.863. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 9.212. Accuracy is 61.50%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 6.955. Accuracy is 81.13%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 8 seconds. Training speed 299 pps. Validation speed 2015 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 6.870. Accuracy is 84.10%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 6.827. Accuracy is 86.48%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 8 seconds. Training speed 323 pps. Validation speed 2021 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 6.703. Accuracy is 89.50%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 7.033. Accuracy is 80.01%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 8 seconds. Training speed 302 pps. Validation speed 2019 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 6.628. Accuracy is 92.30%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 6.799. Accuracy is 87.58%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 8 seconds. Training speed 319 pps. Validation speed 2048 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 6.553. Accuracy is 94.70%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 6.759. Accuracy is 88.38%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 8 seconds. Training speed 305 pps. Validation speed 1977 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 6.486. Accuracy is 97.20%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 6.765. Accuracy is 88.34%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 8 seconds. Training speed 330 pps. Validation speed 2014 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 6.440. Accuracy is 98.40%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 6.727. Accuracy is 89.72%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 8 seconds. Training speed 310 pps. Validation speed 2036 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 6.413. Accuracy is 98.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 6.724. Accuracy is 89.87%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 8 seconds. Training speed 311 pps. Validation speed 2040 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 6.384. Accuracy is 99.50%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 6.729. Accuracy is 89.47%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 8 seconds. Training speed 326 pps. Validation speed 2027 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 6.361. Accuracy is 99.30%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 6.714. Accuracy is 89.90%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 8 seconds. Training speed 328 pps. Validation speed 1966 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 6.339. Accuracy is 99.70%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 6.739. Accuracy is 88.77%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 8 seconds. Training speed 307 pps. Validation speed 1919 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 6.320. Accuracy is 99.60%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 6.701. Accuracy is 89.60%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 8 seconds. Training speed 293 pps. Validation speed 1991 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 6.303. Accuracy is 99.80%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 6.700. Accuracy is 89.77%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 9 seconds. Training speed 294 pps. Validation speed 1913 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 6.286. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 6.687. Accuracy is 89.95%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 8 seconds. Training speed 291 pps. Validation speed 2028 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 6.270. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 6.688. Accuracy is 89.61%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 8 seconds. Training speed 293 pps. Validation speed 2017 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 6.256. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 6.671. Accuracy is 89.76%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 8 seconds. Training speed 292 pps. Validation speed 2005 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 6.241. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 6.651. Accuracy is 90.18%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 9 seconds. Training speed 290 pps. Validation speed 1927 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 6.226. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 6.642. Accuracy is 90.16%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 8 seconds. Training speed 290 pps. Validation speed 1983 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 6.211. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 6.640. Accuracy is 89.79%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 9 seconds. Training speed 289 pps. Validation speed 1945 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 6.196. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 6.621. Accuracy is 90.07%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 8 seconds. Training speed 291 pps. Validation speed 2010 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 6.182. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 6.612. Accuracy is 89.95%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 9 seconds. Training speed 284 pps. Validation speed 1968 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 6.167. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 6.602. Accuracy is 89.87%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 8 seconds. Training speed 287 pps. Validation speed 1999 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 6.153. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 6.587. Accuracy is 90.07%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 9 seconds. Training speed 286 pps. Validation speed 1954 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 6.138. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 6.577. Accuracy is 89.92%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 8 seconds. Training speed 290 pps. Validation speed 2048 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 6.124. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 6.564. Accuracy is 89.98%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 8 seconds. Training speed 290 pps. Validation speed 1985 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 6.109. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 6.548. Accuracy is 89.98%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 9 seconds. Training speed 292 pps. Validation speed 1949 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 6.095. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 6.538. Accuracy is 90.06%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 9 seconds. Training speed 290 pps. Validation speed 1975 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 6.081. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 6.525. Accuracy is 89.95%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 9 seconds. Training speed 290 pps. Validation speed 1982 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 6.066. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 6.517. Accuracy is 89.83%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 9 seconds. Training speed 287 pps. Validation speed 1990 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 6.052. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 6.497. Accuracy is 90.08%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 8 seconds. Training speed 290 pps. Validation speed 2018 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 89.36 %, cost (ce) is 0.459\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "l1_weight = 0.0001\n",
    "l2_weight = 0.0\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=None,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:  Implement L2 based regularisation\n",
    "\n",
    "Implement L2 regularisation method. Follow similar steps as in Exercise 1. Start with $\\beta_{L2}$ set to 0.001 and do some grid search for better values. Plot validation accuracies as a function of epochs for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.666. Accuracy is 8.60%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.595. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.798. Accuracy is 58.20%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.580. Accuracy is 83.75%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 8 seconds. Training speed 393 pps. Validation speed 1947 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.543. Accuracy is 83.40%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.513. Accuracy is 85.87%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 7 seconds. Training speed 388 pps. Validation speed 2049 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.369. Accuracy is 89.70%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.452. Accuracy is 87.73%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 8 seconds. Training speed 387 pps. Validation speed 2020 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.278. Accuracy is 92.60%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.490. Accuracy is 86.40%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 7 seconds. Training speed 392 pps. Validation speed 2043 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.219. Accuracy is 94.10%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.434. Accuracy is 88.83%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 8 seconds. Training speed 388 pps. Validation speed 2034 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.160. Accuracy is 97.30%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.460. Accuracy is 88.57%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 8 seconds. Training speed 389 pps. Validation speed 2026 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.133. Accuracy is 98.00%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.429. Accuracy is 89.41%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 8 seconds. Training speed 379 pps. Validation speed 1952 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.115. Accuracy is 98.60%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.455. Accuracy is 88.30%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 8 seconds. Training speed 389 pps. Validation speed 1934 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.093. Accuracy is 99.50%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.457. Accuracy is 88.88%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 8 seconds. Training speed 378 pps. Validation speed 1933 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.082. Accuracy is 99.50%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.431. Accuracy is 89.97%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 8 seconds. Training speed 388 pps. Validation speed 1927 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.078. Accuracy is 99.50%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.461. Accuracy is 89.28%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 8 seconds. Training speed 376 pps. Validation speed 1976 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.070. Accuracy is 99.60%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.445. Accuracy is 90.05%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 8 seconds. Training speed 382 pps. Validation speed 1974 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.064. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.456. Accuracy is 89.82%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 8 seconds. Training speed 385 pps. Validation speed 2010 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.060. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.462. Accuracy is 89.94%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 8 seconds. Training speed 386 pps. Validation speed 1959 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.060. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.465. Accuracy is 89.94%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 8 seconds. Training speed 389 pps. Validation speed 1917 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.058. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.472. Accuracy is 89.79%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 8 seconds. Training speed 384 pps. Validation speed 1915 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.056. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.478. Accuracy is 89.68%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 8 seconds. Training speed 383 pps. Validation speed 1891 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.055. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.472. Accuracy is 89.82%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 8 seconds. Training speed 385 pps. Validation speed 1961 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.054. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.478. Accuracy is 89.95%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 8 seconds. Training speed 385 pps. Validation speed 1960 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.053. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.473. Accuracy is 90.21%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 8 seconds. Training speed 390 pps. Validation speed 1938 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.053. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.480. Accuracy is 89.99%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 8 seconds. Training speed 380 pps. Validation speed 1971 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.052. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.484. Accuracy is 90.01%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 8 seconds. Training speed 376 pps. Validation speed 1970 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.052. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.482. Accuracy is 90.16%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 8 seconds. Training speed 377 pps. Validation speed 1903 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.051. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.486. Accuracy is 90.08%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 8 seconds. Training speed 385 pps. Validation speed 1969 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.051. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.488. Accuracy is 89.99%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 8 seconds. Training speed 383 pps. Validation speed 2013 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.051. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.490. Accuracy is 90.10%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 8 seconds. Training speed 386 pps. Validation speed 2000 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.051. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.491. Accuracy is 90.12%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 8 seconds. Training speed 379 pps. Validation speed 1921 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.050. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.493. Accuracy is 90.08%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 8 seconds. Training speed 385 pps. Validation speed 2028 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.050. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.494. Accuracy is 90.26%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 8 seconds. Training speed 384 pps. Validation speed 1979 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.050. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.495. Accuracy is 90.14%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 8 seconds. Training speed 382 pps. Validation speed 1954 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 89.37 %, cost (ce) is 0.461\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "l1_weight = 0\n",
    "l2_weight = 0.00001\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=None,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "    \n",
    "Droput applied to input features (turning on/off some random pixels) may be also viewed as a form of data augmentation -- as we effectively create images that differ in some way from training one but also model is tasked to properly classify imperfect data-points.\n",
    "\n",
    "Your task in this exercise is to pick a random digit from MNIST dataset (use MNISTDataProvider) and corrupt it pixel-wise with different levels of probabilities $p_{d} \\in \\{0.9, 0.7, 0.5, 0.2, 0.1\\}$ (reminder, dropout probability is $1-p_d$) that is, for each pixel $x_{i,j}$ in image $\\mathbf{X} \\in \\mathbb{R}^{W\\times H}$:\n",
    "\n",
    "$\\begin{align}\n",
    "d_{i,j} & \\sim\\ \\mbox{Bernoulli}(p_{d}) \\\\\n",
    "x_{i,j} &=\n",
    "\\begin{cases}\n",
    "     0     & \\quad \\text{if } d_{i,j} = 0\\\\\n",
    "     x_{i,j}       & \\quad \\text{if } d_{i,j} = 1\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Plot the solution as a 2x3 grid of images for each $p_d$ scenario, at position (0, 0) plot an original (uncorrupted) image.\n",
    "\n",
    "Tip: You may use numpy.random.binomial function to draw samples from Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-31d487a07e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/dataset.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mrval_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrval_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mrval_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrval_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_reshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/dataset.pyc\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m     31\u001b[0m            \u001b[0mRe\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msize\u001b[0m \u001b[0mback\u001b[0m \u001b[0mto\u001b[0m \u001b[0mnormal\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mflatten\u001b[0m \u001b[0mback\u001b[0m \u001b[0mto\u001b[0m \u001b[0;36m784.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     '''\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "import scipy.ndimage\n",
    "\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=6, max_num_batches=6, randomize=True, augmentation=2)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "train_dp.reset()\n",
    "x, t = train_dp.next()\n",
    "print x.shape\n",
    "\n",
    "img = x#[0].reshape(28,28)\n",
    "pds = [0.9, 0.7, 0.5, 0.2, 0.1]\n",
    "imgs = [None] * (len(pds)+1)\n",
    "imgs[0] = img\n",
    "\n",
    "print img.shape\n",
    "for i, pd in enumerate(pds):\n",
    "    d = self.rng.binomial(1, pd, img.shape)\n",
    "    imgs[i + 1] = img*d #scipy.ndimage.gaussian_filter(img, sigma=pd)\n",
    "\n",
    "temp = scipy.ndimage.interpolation.rotate(img, 20)[:28,:28].copy()\n",
    "\n",
    "fig, ax = plt.subplots(2,3)\n",
    "ax[0, 0].imshow(numpy.roll(imgs[0],-5), cmap=cm.Greys_r)\n",
    "ax[0, 1].imshow(imgs[1], cmap=cm.Greys_r)\n",
    "ax[0, 2].imshow(temp, cmap=cm.Greys_r)\n",
    "ax[1, 0].imshow(imgs[3], cmap=cm.Greys_r)\n",
    "ax[1, 1].imshow(imgs[4], cmap=cm.Greys_r)\n",
    "ax[1, 2].imshow(imgs[5], cmap=cm.Greys_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Implement Dropout \n",
    "\n",
    "Implement dropout regularisation technique. Then for the same initial configuration as in Exercise 1. investigate effectivness of different dropout rates applied to input features and/or hidden layers. Start with $p_{inp}=0.5$ and $p_{hid}=0.5$ and do some search for better settings.\n",
    "\n",
    "Implementation tips:\n",
    "* Add a function `fprop_dropout` to `mlp.layers.MLP` class which (on top of `inputs` argument) takes also dropout-related argument(s) and perform dropout forward propagation through the model.\n",
    "* One also would have to introduce required modificastions to `mlp.optimisers.SGDOptimiser.train_epoch()` function.\n",
    "* Design and implemnt dropout scheduler in a similar way to how learning rates are handled (that is, allowing for some implementation dependent schedule which is kept independent of implementation in `mlp.optimisers.SGDOptimiser.train()`). \n",
    "   +  For this exercise implement only fixed dropout scheduler - `DropoutFixed`, but implementation should allow to easily add other schedules in the future. \n",
    "   +  Dropout scheduler of any type should return a tuple of two numbers $(p_{inp},\\; p_{hid})$, the first one is dropout factor for input features (data-points), and the latter dropout factor for hidden layers (assumed the same for all hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-97aaea08fffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training started...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtr_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing the model on test set:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/optimisers.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_iterator, valid_iterator)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# do the initial validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mtr_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         logger.info('Epoch %i: Training cost (%s) for initial model is %.3f. Accuracy is %.2f%%'\n\u001b[1;32m    176\u001b[0m                     % (self.lr_scheduler.epoch, cost_name, tr_nll, tr_acc * 100.))\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/optimisers.pyc\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, model, valid_iterator, l1_weight, l2_weight)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0macc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnll_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mnll_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/dataset.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mret_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mret_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mret_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;31m#extend the original batch with all augmented versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mrval_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/dataset.pyc\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     30\u001b[0m                \u001b[0mRe\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msize\u001b[0m \u001b[0mback\u001b[0m \u001b[0mto\u001b[0m \u001b[0mnormal\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mflatten\u001b[0m \u001b[0mback\u001b[0m \u001b[0mto\u001b[0m \u001b[0;36m784.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     '''\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/ndimage/interpolation.pyc\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(input, angle, axes, reshape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    625\u001b[0m                              [m21, m22]], dtype=numpy.float64)\n\u001b[1;32m    626\u001b[0m     \u001b[0miy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m     \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         mtrx = numpy.array([[m11, -m21],\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed, DropoutAnnealed\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True, augmentation=2)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 50\n",
    "l1_weight = 0.0\n",
    "l2_weight = 0.0\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.005)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed, DropoutFixed\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 20\n",
    "l1_weight = 0.0\n",
    "l2_weight = 0.0\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    dp_scheduler = DropoutFixed(0.5, 0.5)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8ba282387cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# define the optimiser, here stochasitc gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# with fixed learning rate and max_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearningRateExponential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mdp_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropoutFixed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, DropoutFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 50\n",
    "l1_weight = 0.0\n",
    "l2_weight = 0.0\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateExponential(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    dp_scheduler = DropoutFixed(0.5, 0.5)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
