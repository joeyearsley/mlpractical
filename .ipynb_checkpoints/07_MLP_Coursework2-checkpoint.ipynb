{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please don't edit this cell!**\n",
    "\n",
    "# Marks and Feedback\n",
    "\n",
    "**Total Marks:**   XX/100\n",
    "\n",
    "**Overall comments:**\n",
    "\n",
    "\n",
    "## Part 1. Investigations into Neural Networks (35 marks)\n",
    "\n",
    "* **Task 1**:   *Experiments with learning rate schedules* - XX/5\n",
    "    * learning rate schedulers implemented\n",
    "    * experiments carried out\n",
    "    * further comments\n",
    "\n",
    "\n",
    "* **Task 2**:   *Experiments with regularisation* - XX/5\n",
    "    * L1 experiments\n",
    "    * L2 experiments\n",
    "    * dropout experiments\n",
    "    * annealed dropout implmented\n",
    "    * further experiments carried out\n",
    "    * further comments\n",
    "    \n",
    "\n",
    "* **Task 3**:   *Experiments with pretraining* - XX/15\n",
    "    * autoencoder pretraining implemented\n",
    "    * denoising autoencoder pretraining implemented\n",
    "    * CE layer-by-layer pretraining implemented\n",
    "    * experiments\n",
    "    * further comments\n",
    "\n",
    "\n",
    "* **Task 4**:   *Experiments with data augmentation* - XX/5\n",
    "    * training data augmneted using noise, rotation, ...\n",
    "    * any further augmnetations\n",
    "    * experiments \n",
    "    * further comments\n",
    "\n",
    "\n",
    "* **Task 5**:   *State of the art* - XX/5\n",
    "    * motivation for systems constructed\n",
    "    * experiments\n",
    "    * accuracy of best system\n",
    "    * further comments\n",
    "\n",
    "\n",
    "\n",
    "## Part 2. Convolutional Neural Networks (55 marks)\n",
    "\n",
    "* **Task 6**:   *Implement convolutional layer* - XX/20\n",
    "    * linear conv layer\n",
    "    * sigmoid conv layer\n",
    "    * relu conv layer\n",
    "    * any checks for correctness\n",
    "    * loop-based or vectorised implementations\n",
    "    * timing comparisons\n",
    "\n",
    "\n",
    "* **Task 7**:   *Implement maxpooling layer* - XX/10\n",
    "    * implementation of non-overlapping pooling\n",
    "    * generic implementation\n",
    "    * any checks for correctness\n",
    "\n",
    "\n",
    "* **Task 8**:   *Experiments with convolutional networks* - XX/25\n",
    "    * 1 conv layer (1 fmap)\n",
    "    * 1 conv layer (5 fmaps)\n",
    "    * 2 conv layers\n",
    "    * further experiments\n",
    "\n",
    "\n",
    "\n",
    "## Presentation (10 marks)\n",
    "\n",
    "* ** Marks:**   XX/10\n",
    "    * Concise description of each system constructed\n",
    "    * Experiment design and motivations for different systems\n",
    "    * Presentation of results - graphs, tables, diagrams\n",
    "    * Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework #2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "## Previous Tutorials\n",
    "\n",
    "Before starting this coursework make sure that you have completed the following labs:\n",
    "\n",
    "* [04_Regularisation.ipynb](https://github.com/CSTR-Edinburgh/mlpractical/blob/master/04_Regularisation.ipynb) - regularising the model\n",
    "* [05_Transfer_functions.ipynb](https://github.com/CSTR-Edinburgh/mlpractical/blob/master/05_Transfer_functions.ipynb) - building and training different activation functions\n",
    "* [06_MLP_Coursework2_Introduction.ipynb](https://github.com/CSTR-Edinburgh/mlpractical/blob/master/06_MLP_Coursework2_Introduction.ipynb) - Notes on numpy and tensors\n",
    "\n",
    "\n",
    "## Submission\n",
    "**Submission Deadline:  Thursday 14 January 2016, 16:00** \n",
    "\n",
    "Submit the coursework as an ipython notebook file, using the `submit` command in the terminal on a DICE machine. If your file is `06_MLP_Coursework1.ipynb` then you would enter:\n",
    "\n",
    "`submit mlp 2 06_MLP_Coursework1.ipynb` \n",
    "\n",
    "where `mlp 2` indicates this is the second coursework of MLP.\n",
    "\n",
    "After submitting, you should receive an email of acknowledgment from the system confirming that your submission has been received successfully. Keep the email as evidence of your coursework submission.\n",
    "\n",
    "**Please make sure you submit a single `ipynb` file (and nothing else)!**\n",
    "\n",
    "**Submission Deadline:  Thursday 14 January 2016, 16:00** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Please enter your student number and the date in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP Coursework 2\n",
    "#Student number: s1567688\n",
    "#Date: 10/12/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Investigations into Neural Networks (35 marks)\n",
    "\n",
    "In this part you are may choose exactly what you implement. However, you are expected to express your motivations, observations, and findings in a clear and cohesive way. Try to make it clear why you decided to do certain things. Use graphs and/or tables of results to show trends and other characteristics you think are important. \n",
    "\n",
    "For example, in Task 1 you could experiment with different schedulers in order to compare their convergence properties. In Task 2 you could look into (and visualise) what happens to weights when applying L1 and/or L2 regularisation when training. For instance, you could create sorted histograms of weight magnitudes in in each layer, etc..\n",
    "\n",
    "**Before submission, please collapse all the log entries into smaller boxes (by clicking on the bar on the left hand side)**\n",
    "\n",
    "### Task 1 - Experiments with learning rate schedules (5 marks)\n",
    "\n",
    "Investigate the effect of learning rate schedules on training and accuracy.  Implement at least one additional learning rate scheduler mentioned in the lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/IPython/nbformat.py:13: ShimWarning: The `IPython.nbformat` package has been deprecated. You should import from nbformat instead.\n",
      "  \"You should import from nbformat instead.\", ShimWarning)\n",
      "/Library/Python/2.7/site-packages/nbformat/current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) for initial model is 2.570. Accuracy is 9.28%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.761. Accuracy is 85.84%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.262. Accuracy is 92.61%\n",
      "INFO:mlp.schedulers:0.495024916875\n",
      "INFO:mlp.optimisers:Epoch 2: Took 30 seconds. Training speed 1782 pps. Validation speed 5868 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.269. Accuracy is 92.06%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.228. Accuracy is 93.41%\n",
      "INFO:mlp.schedulers:0.490099336653\n",
      "INFO:mlp.optimisers:Epoch 3: Took 30 seconds. Training speed 1779 pps. Validation speed 5889 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.222. Accuracy is 93.56%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.192. Accuracy is 94.68%\n",
      "INFO:mlp.schedulers:0.485222766774\n",
      "INFO:mlp.optimisers:Epoch 4: Took 30 seconds. Training speed 1793 pps. Validation speed 5779 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.189. Accuracy is 94.45%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.171. Accuracy is 95.40%\n",
      "INFO:mlp.schedulers:0.480394719576\n",
      "INFO:mlp.optimisers:Epoch 5: Took 29 seconds. Training speed 1843 pps. Validation speed 5766 pps.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a7c2df9d9641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training started...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtr_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing the model on test set:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/optimisers.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_iterator, valid_iterator)\u001b[0m\n\u001b[1;32m    187\u001b[0m             tr_nll, tr_acc = self.train_epoch(model=model,\n\u001b[1;32m    188\u001b[0m                                               \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                                               learning_rate=self.lr_scheduler.get_rate())\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mtstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mtr_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/optimisers.pyc\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, model, train_iterator, learning_rate)\u001b[0m\n\u001b[1;32m    145\u001b[0m                                                \u001b[0mdeltas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                                                \u001b[0ml1_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                                                l2_weight=self.l2_weight)\n\u001b[0m\u001b[1;32m    148\u001b[0m                 \u001b[0muparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/layers.pyc\u001b[0m in \u001b[0;36mpgrads\u001b[0;34m(self, inputs, deltas, l1_weight, l2_weight)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0ml1_b_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1_weight\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mgrad_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_W_penalty\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1_W_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_b_penalty\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1_b_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) for initial model is 2.570. Accuracy is 9.28%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.739. Accuracy is 86.09%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.310. Accuracy is 90.83%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 29 seconds. Training speed 1806 pps. Validation speed 5940 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.278. Accuracy is 91.68%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.206. Accuracy is 94.44%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 29 seconds. Training speed 1851 pps. Validation speed 5831 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.202. Accuracy is 94.25%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.192. Accuracy is 94.78%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 29 seconds. Training speed 1828 pps. Validation speed 5915 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.198. Accuracy is 94.37%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 30 seconds. Training speed 1748 pps. Validation speed 5793 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 28 seconds. Training speed 1902 pps. Validation speed 5827 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 27 seconds. Training speed 1988 pps. Validation speed 5857 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 30 seconds. Training speed 1756 pps. Validation speed 5943 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 29 seconds. Training speed 1848 pps. Validation speed 5959 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 29 seconds. Training speed 1861 pps. Validation speed 5920 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 28 seconds. Training speed 1906 pps. Validation speed 5796 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 27 seconds. Training speed 1995 pps. Validation speed 5992 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 29 seconds. Training speed 1817 pps. Validation speed 5892 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.198. Accuracy is 94.40%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.192. Accuracy is 94.86%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 28 seconds. Training speed 1886 pps. Validation speed 5874 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 94.37 %, cost (ce) is 0.199\n",
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Pre-Training started...\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 21.023. Accuracy is 0.80%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 11.791. Accuracy is 1.40%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 8.838. Accuracy is 1.20%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 7.072. Accuracy is 1.10%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 5.876. Accuracy is 0.70%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 5.021. Accuracy is 0.80%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 4.384. Accuracy is 0.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 3.893. Accuracy is 0.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 3.502. Accuracy is 0.90%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 3.182. Accuracy is 1.00%\n",
      "INFO:mlp.optimisers:activations 3\n",
      "INFO:mlp.optimisers:activations2 2\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 16.812. Accuracy is 11.10%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 8.182. Accuracy is 27.10%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 5.006. Accuracy is 37.40%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 3.509. Accuracy is 44.20%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 2.699. Accuracy is 50.40%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 2.216. Accuracy is 51.60%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.920. Accuracy is 52.00%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.675. Accuracy is 55.00%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 1.506. Accuracy is 56.40%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 1.381. Accuracy is 57.60%\n",
      "INFO:mlp.optimisers:activations 4\n",
      "INFO:mlp.optimisers:activations2 3\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 13.397. Accuracy is 12.40%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 7.578. Accuracy is 28.10%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 4.333. Accuracy is 37.30%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 2.644. Accuracy is 47.40%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.752. Accuracy is 55.10%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.251. Accuracy is 60.80%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.954. Accuracy is 64.90%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.770. Accuracy is 68.20%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.653. Accuracy is 70.10%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.574. Accuracy is 70.70%\n",
      "INFO:mlp.optimisers:activations 5\n",
      "INFO:mlp.optimisers:activations2 4\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.407. Accuracy is 9.70%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.413. Accuracy is 10.55%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.634. Accuracy is 50.00%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 1.033. Accuracy is 79.58%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 5 seconds. Training speed 421 pps. Validation speed 3994 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.874. Accuracy is 80.70%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.758. Accuracy is 80.32%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 5 seconds. Training speed 408 pps. Validation speed 4063 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.648. Accuracy is 85.10%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.545. Accuracy is 87.19%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 5 seconds. Training speed 411 pps. Validation speed 4017 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.537. Accuracy is 87.40%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.485. Accuracy is 87.76%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 5 seconds. Training speed 400 pps. Validation speed 4056 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.468. Accuracy is 88.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.452. Accuracy is 87.67%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 5 seconds. Training speed 409 pps. Validation speed 3990 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.424. Accuracy is 89.80%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.428. Accuracy is 88.08%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 5 seconds. Training speed 409 pps. Validation speed 4112 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.393. Accuracy is 89.70%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.421. Accuracy is 87.75%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 5 seconds. Training speed 409 pps. Validation speed 4058 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.365. Accuracy is 90.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.413. Accuracy is 87.93%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 5 seconds. Training speed 407 pps. Validation speed 4014 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.336. Accuracy is 91.50%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.388. Accuracy is 88.67%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 5 seconds. Training speed 408 pps. Validation speed 3988 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.322. Accuracy is 92.20%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.370. Accuracy is 89.39%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 5 seconds. Training speed 411 pps. Validation speed 4109 pps.\n",
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.351. Accuracy is 11.20%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.373. Accuracy is 10.64%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.405. Accuracy is 11.80%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.361. Accuracy is 9.83%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 5 seconds. Training speed 379 pps. Validation speed 4067 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.231. Accuracy is 19.00%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 2.170. Accuracy is 16.98%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 5 seconds. Training speed 373 pps. Validation speed 3938 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 2.022. Accuracy is 31.00%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.848. Accuracy is 39.30%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 5 seconds. Training speed 373 pps. Validation speed 3911 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.702. Accuracy is 47.60%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 1.476. Accuracy is 53.24%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 5 seconds. Training speed 374 pps. Validation speed 3960 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.369. Accuracy is 57.70%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 1.188. Accuracy is 72.88%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 5 seconds. Training speed 391 pps. Validation speed 3965 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.122. Accuracy is 67.70%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.987. Accuracy is 72.08%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 5 seconds. Training speed 372 pps. Validation speed 4039 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.951. Accuracy is 72.70%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.886. Accuracy is 73.06%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 5 seconds. Training speed 426 pps. Validation speed 4047 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.812. Accuracy is 76.60%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.810. Accuracy is 77.44%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 5 seconds. Training speed 404 pps. Validation speed 4065 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.732. Accuracy is 79.50%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.686. Accuracy is 78.97%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 5 seconds. Training speed 422 pps. Validation speed 4104 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.644. Accuracy is 81.60%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.612. Accuracy is 82.91%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 5 seconds. Training speed 416 pps. Validation speed 4066 pps.\n",
      "INFO:root:Initialising data providers...\n",
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Pre-Training started...\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.194. Accuracy is 23.90%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.386. Accuracy is 56.30%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.002. Accuracy is 71.20%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.803. Accuracy is 78.60%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.684. Accuracy is 82.20%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.603. Accuracy is 84.20%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.544. Accuracy is 86.40%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.498. Accuracy is 87.10%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.461. Accuracy is 87.80%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.430. Accuracy is 88.50%\n",
      "INFO:mlp.optimisers:activations 3\n",
      "INFO:mlp.optimisers:activations2 2\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.373. Accuracy is 16.30%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.923. Accuracy is 32.70%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.540. Accuracy is 50.70%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.248. Accuracy is 63.20%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.043. Accuracy is 70.70%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.898. Accuracy is 75.50%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.794. Accuracy is 78.50%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.716. Accuracy is 80.80%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.657. Accuracy is 82.10%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.610. Accuracy is 83.70%\n",
      "INFO:mlp.optimisers:activations 4\n",
      "INFO:mlp.optimisers:activations2 3\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.347. Accuracy is 13.20%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.149. Accuracy is 22.80%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.921. Accuracy is 35.70%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.672. Accuracy is 48.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.443. Accuracy is 56.50%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.259. Accuracy is 62.60%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.120. Accuracy is 66.10%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.014. Accuracy is 69.40%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.933. Accuracy is 72.40%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.869. Accuracy is 73.70%\n",
      "INFO:mlp.optimisers:activations 5\n",
      "INFO:mlp.optimisers:activations2 4\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.279. Accuracy is 26.30%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.305. Accuracy is 23.68%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.903. Accuracy is 37.50%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 1.583. Accuracy is 46.51%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 5 seconds. Training speed 404 pps. Validation speed 3983 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.357. Accuracy is 58.00%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.283. Accuracy is 53.99%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 5 seconds. Training speed 417 pps. Validation speed 4129 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.085. Accuracy is 67.00%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.955. Accuracy is 77.80%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 5 seconds. Training speed 429 pps. Validation speed 4225 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.930. Accuracy is 69.90%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.860. Accuracy is 72.87%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 5 seconds. Training speed 431 pps. Validation speed 4081 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.815. Accuracy is 75.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.764. Accuracy is 80.09%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 5 seconds. Training speed 408 pps. Validation speed 4004 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.741. Accuracy is 76.60%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.736. Accuracy is 78.22%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 5 seconds. Training speed 423 pps. Validation speed 4093 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.694. Accuracy is 76.60%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.683. Accuracy is 78.67%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 5 seconds. Training speed 414 pps. Validation speed 4164 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.635. Accuracy is 82.20%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.637. Accuracy is 82.05%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 5 seconds. Training speed 444 pps. Validation speed 4144 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.584. Accuracy is 82.80%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.629. Accuracy is 79.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 5 seconds. Training speed 411 pps. Validation speed 4059 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.560. Accuracy is 83.00%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.575. Accuracy is 83.29%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 5 seconds. Training speed 420 pps. Validation speed 4049 pps.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFor RMSProp/AdaGrad, does it have to go in optimisers since it is on weight by weight basis.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Experiment with fixed, newBob, exponential, listed\n",
    "%load Experiments/scheduler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Experiments with regularisers (5 marks)\n",
    "\n",
    "Investigate the effect of different regularisation approaches (L1, L2, dropout).  Implement the annealing dropout scheduler (mentioned in lecture 5). Do some further investigations and experiments with model structures (and regularisers) of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lab4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Experiments with pretraining (15 marks)\n",
    "\n",
    "Implement pretraining of multi-layer networks with autoencoders, denoising autoencoders, and using  layer-by-layer cross-entropy training.  \n",
    "\n",
    "Implementation tip: You could add the corresponding methods to `optimiser`, namely, `pretrain()` and `pretrain_epoch()`, for autoencoders. Simiilarly, `pretrain_discriminative()` and `pretrain_epoch_discriminative()` for cross-entropy layer-by-layer pretraining. Of course, you can modify any other necessary pieces, but include all the modified fragments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Both auto-encoders and denoising? Deep autoencoders or just initial layer?\n",
    "'''\n",
    "\n",
    "Final Layer - Sigmoid, use normal backprop of sigmoid, bce?\n",
    "Cross-entropy layer to layer - as expected\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - Experiments with data augmentation (5 marks)\n",
    "\n",
    "Using the standard MNIST training data, generate some augmented training examples (for example, using noise or rotation). Perform experiments on using this expanded training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Look at lab4 - Make for loops, to go through batch_size, as its currently just for individual images not batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 - State of the art (5 marks)\n",
    "\n",
    "Using any techniques you have learnt so far (combining any number of them), build and train the best model you can (no other constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Will come back to this after CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2. Convolutional Neural Networks (55 marks)\n",
    "\n",
    "In this part of the coursework, you are required to implement deep convolutional networks.  This includes code for forward prop, back prop, and weight updates for convolutional and max-pooling layers, and should support the stacking of convolutional + pooling layers.  You should implement all the parts relating to the convolutional layer in the mlp/conv.py module; if you decide to implement some routines in cython, keep them in mlp/conv.pyx). Attach both files in this notebook.\n",
    "\n",
    "Implementation tips: Look at [lecture 7](http://www.inf.ed.ac.uk/teaching/courses/mlp/2015/mlp07-cnn.pdf) and [lecture 8](http://www.inf.ed.ac.uk/teaching/courses/mlp/2015/mlp08-cnn2.pdf), and the introductory tutorial, [06_MLP_Coursework2_Introduction.ipynb](https://github.com/CSTR-Edinburgh/mlpractical/blob/master/06_MLP_Coursework2_Introduction.ipynb)\n",
    "\n",
    "### Task 6 -  Implement convolutional layer (20 marks)\n",
    "\n",
    "Implement linear convolutional layer, and then extend to sigmoid and ReLU transfer functions (do it in a similar way to fully-connected layers). Include all relevant code.  It is recommended that you first implement in the naive way with nested loops (python and/or cython);  optionally you may then implement in a vectorised way in numpy.  Include logs for each way you implement the convolutional layer, as timings for different implementations are of interest.  Include all relevant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(ConvLinear(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 7 - Implement max-pooling layer (10 marks)\n",
    "\n",
    "Implement a max-pooling layer. Non-overlapping pooling (which was assumed in the lecture presentation) is required. You may also implement a more generic solution with striding as well. Include all relevant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 - Experiments with convolutional networks (25 marks)\n",
    "\n",
    "Construct convolutional networks with a softmax output layer and a single fully connected hidden layer. Your first experiments should use one convolutional+pooling layer.  As a default use convolutional kernels of dimension 5x5 (stride 1) and pooling regions of 2x2 (stride 2, hence non-overlapping).\n",
    "\n",
    "*  Implement and test a convolutional network with 1 feature map\n",
    "*  Implement and test a convolutional network with 5 feature maps\n",
    "\n",
    "Explore convolutional networks with two convolutional layers, by implementing, training, and evaluating a network with two convolutional+maxpooling layers with 5 feature maps in the first convolutional layer,  and 10 feature maps in the second convolutional layer.\n",
    "\n",
    "Carry out further experiments to optimise the convolutional network architecture (you could explore kernel sizes and strides, number of feature maps, sizes and strides of pooling operator, etc. - it is up to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**This is the end of coursework 2.**\n",
    "\n",
    "Please remember to save your notebook, and submit your notebook following the instructions at the top.  Please make sure that you have executed all the code cells when you submit the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
