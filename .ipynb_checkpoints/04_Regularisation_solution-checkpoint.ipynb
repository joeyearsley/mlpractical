{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial focuses on implementation of three reqularisaion techniques, two of them are norm based approaches which are added to optimised objective and the third technique, called *droput*, is a form of noise injection by random corruption of information carried by hidden units during training.\n",
    "\n",
    "\n",
    "## Virtual environments\n",
    "\n",
    "Before you proceed onwards, remember to activate your virtual environment:\n",
    "   * If you were in last week's Tuesday or Wednesday group type `activate_mlp` or `source ~/mlpractical/venv/bin/activate`\n",
    "   * If you were in the Monday group:\n",
    "      + and if you have chosen the **comfy** way type: `workon mlpractical`\n",
    "      + and if you have chosen the **generic** way, `source` your virutal environment using `source` and specyfing the path to the activate script (you need to localise it yourself, there were not any general recommendations w.r.t dir structure and people have installed it in different places, usually somewhere in the home directories. If you cannot easily find it by yourself, use something like: `find . -iname activate` ):\n",
    "\n",
    "## Syncing the git repository\n",
    "\n",
    "Look <a href=\"https://github.com/CSTR-Edinburgh/mlpractical/blob/master/gitFAQ.md\">here</a> for more details. But in short, we recommend to create a separate branch for this lab, as follows:\n",
    "\n",
    "1. Enter the mlpractical directory `cd ~/mlpractical/repo-mlp`\n",
    "2. List the branches and check which is currently active by typing: `git branch`\n",
    "3. If you have followed our recommendations, you should be in the `coursework1` branch, please commit your local changed to the repo index by typing:\n",
    "```\n",
    "git commit -am \"finished coursework\"\n",
    "```\n",
    "4. Now you can switch to `master` branch by typing: \n",
    "```\n",
    "git checkout master\n",
    " ```\n",
    "5. To update the repository (note, assuming master does not have any conflicts), if there are some, have a look <a href=\"https://github.com/CSTR-Edinburgh/mlpractical/blob/master/gitFAQ.md\">here</a>\n",
    "```\n",
    "git pull\n",
    "```\n",
    "6. And now, create the new branch & swith to it by typing:\n",
    "```\n",
    "git checkout -b lab4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation\n",
    "\n",
    "Regularisation add some *complexity term* to the cost function. It's purpose is to put some prior on the model's parameters. The most common prior is perhaps the one which assumes smoother solutions (the one which are not able to fit training data too well) are better as they are more likely to better generalise to unseen data. \n",
    "\n",
    "A way to incorporate such prior in the model is to add some term that penalise certain configurations of the parameters -- either from growing too large ($L_2$) or the one that prefers solution that could be modelled with less parameters ($L_1$), hence encouraging some parameters to become 0. One can, of course, combine many such priors when optimising the model, however, in the lab we shall use $L_1$ and/or $L_2$ priors.\n",
    "\n",
    "They can be easily incorporated into the training objective by adding some additive terms, as follows:\n",
    "\n",
    "(1) $\n",
    " \\begin{align*}\n",
    "        E^n &= \\underbrace{E^n_{\\text{train}}}_{\\text{data term}} + \n",
    "    \\underbrace{\\beta_{L_1} E^n_{L_1}}_{\\text{prior term}} + \\underbrace{\\beta_{L_2} E^n_{L_2}}_{\\text{prior term}}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $ E^n_{\\text{train}} = - \\sum_{k=1}^K t^n_k \\ln y^n_k $,  $\\beta_{L_1}$ and $\\beta_{L_2}$ some non-negative constants specified a priori (hyper-parameters) and $E^n_{L_1}$ and $E^n_{L_2}$ norm metric specifying certain properties of parameters:\n",
    "\n",
    "(2) $\n",
    " \\begin{align*}\n",
    " E^n_{L_p}(\\mathbf{W}) = \\left ( \\sum_{i,j \\in \\mathbf{W}} |w_{i,j}|^p \\right )^{\\frac{1}{p}}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $p$ denotes the norm-order (for regularisation either 1 or 2). (TODO: explain here why we usualy skip square root for p=2)\n",
    "\n",
    "## $L_{p=2}$ (Weight Decay)\n",
    "\n",
    "(3) $\n",
    " \\begin{align*}\n",
    "        E^n &= \\underbrace{E^n_{\\text{train}}}_{\\text{data term}} + \n",
    "    \\underbrace{\\beta E^n_{L_2}}_{\\text{prior term}} = E^n_{\\text{train}} + \\beta_{L_2} \\frac{1}{2}|w_i|^2\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "(4) $\n",
    "\\begin{align*}\\frac{\\partial E^n}{\\partial w_i} &= \\frac{\\partial (E^n_{\\text{train}} + \\beta_{L_2} E_{L_2}) }{\\partial w_i} \n",
    "  = \\left( \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_2} \\frac{\\partial\n",
    "      E_{L_2}}{\\partial w_i} \\right) \n",
    "  = \\left( \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_2} w_i \\right)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "(5) $\n",
    "\\begin{align*}\n",
    "  \\Delta w_i &= -\\eta \\left( \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_2} w_i \\right) \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "where $\\eta$ is learning rate.\n",
    "\n",
    "## $L_{p=1}$ (Sparsity)\n",
    "\n",
    "(6) $\n",
    " \\begin{align*}\n",
    "        E^n &= \\underbrace{E^n_{\\text{train}}}_{\\text{data term}} + \n",
    "    \\underbrace{\\beta E^n_{L_1}}_{\\text{prior term}} \n",
    "        = E^n_{\\text{train}} + \\beta_{L_1} |w_i|\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "(7) $\\begin{align*}\n",
    "  \\frac{\\partial E^n}{\\partial w_i} =  \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_1} \\frac{\\partial E_{L_1}}{\\partial w_i}  =  \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_1}  \\mbox{sgn}(w_i)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "(8) $\\begin{align*}\n",
    "  \\Delta w_i &= -\\eta \\left( \\frac{\\partial E^n_{\\text{train}}}{\\partial w_i}  + \\beta_{L_1} \\mbox{sgn}(w_i) \\right) \n",
    "\\end{align*}$\n",
    "\n",
    "Where $\\mbox{sgn}(w_i)$ is the sign of $w_i$: $\\mbox{sgn}(w_i) = 1$ if $w_i>0$ and $\\mbox{sgn}(w_i) = -1$ if $w_i<0$\n",
    "\n",
    "One can also apply those penalty terms for biases, however, this is usually not necessary as biases have secondary impact on smoothnes of the given solution.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout, for a given layer's output $\\mathbf{h}^i \\in \\mathbb{R}^{BxH^l}$ (where $B$ is batch size and $H^l$ is the $l$-th layer output dimensionality) implements the following transformation:\n",
    "\n",
    "(9) $\\mathbf{\\hat h}^l = \\mathbf{d}^l\\circ\\mathbf{h}^l$\n",
    "\n",
    "where $\\circ$ denotes an elementwise product and $\\mathbf{d}^l \\in \\{0,1\\}^{BxH^i}$ is a matrix in which $d^l_{ij}$ element is sampled from the Bernoulli distribution:\n",
    "\n",
    "(10) $d^l_{ij} \\sim \\mbox{Bernoulli}(p^l_d)$\n",
    "\n",
    "with $0<p^l_d<1$ denoting the probability the given unit is kept unchanged (dropping probability is thus $1-p^l_d$). We ignore here edge scenarios where $p^l_d=1$ and there is no dropout applied (and the training would be exactly the same as in standard SGD) and $p^l_d=0$ where all units would have been dropped, hence the model would not learn anything.\n",
    "\n",
    "The probability $p^l_d$ is a hyperparameter (like learning rate) meaning it needs to be provided before training and also very often tuned for the given task. As the notation suggest, it can be specified separately for each layer, including scenario where $l=0$ when some random input features (pixels in the image for MNIST) are being also ommitted.\n",
    "\n",
    "### Keeping the $l$-th layer output $\\mathbf{\\hat h}^l$ (input to the upper layer) appropiately scaled at test-time\n",
    "\n",
    "The other issue one needs to take into account is the mismatch that arises between training and test (runtime) stages when dropout is applied. It is due to the fact that droput is not applied when testing hence the average input to the unit in upper layer is going to be bigger when compared to training stage (where some inputs are set to 0), in average $1/p^l_d$ times bigger. \n",
    "\n",
    "So to account for this mismatch one could either:\n",
    "\n",
    "1. When training is finished scale the final weight matrices $\\mathbf{W}^l, l=1,\\ldots,L$ by $p^{l-1}_d$ (remember, $p^{0}_d$ is the probability related to the input features)\n",
    "2. Scale the activations in equation (9) during training, that is, for each mini-batch multiply $\\mathbf{\\hat h}^l$ by $1/p^l_d$ to compensate for dropped units and then at run-time use the model as usual, **without** scaling. Make sure the $1/p^l_d$ scaler is taken into account for both forward and backward passes.\n",
    "\n",
    "\n",
    "Our recommendation is option 2 as it will make some things easier from implementation perspective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Implement L1 based regularisation\n",
    "\n",
    "Implement L1 regularisation penalty (just for weight matrices, ignore biases). Test your solution on one hidden layer model similar to the one from coursework's Task 4 (800 hidden units) but limit training data to 10 000 (random) data-points (keep validation and test sets the same). First build and train not-regularised model as a basline. Then train regularised model starting with $\\beta_{L1}$ set to 0.001 and do some grid search for better values. Plot validation accuracies as a function of epochs for each model (each $\\beta_{L1}$ you tried).\n",
    "\n",
    "Implementation tips:\n",
    "* Have a look at the constructor of mlp.optimiser.SGDOptimiser class, it has been modified to take more optimisation-related arguments.\n",
    "* The best place to implement regularisation terms is `pgrads` method of mlp.layers.Layer (sub)-classes (look at equations (5) and (8) to see why). Some modifications are also required in `train_epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "l1_weight = 0.0001\n",
    "l2_weight = 0.0\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=None,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:  Implement L2 based regularisation\n",
    "\n",
    "Implement L2 regularisation method. Follow similar steps as in Exercise 1. Start with $\\beta_{L2}$ set to 0.001 and do some grid search for better values. Plot validation accuracies as a function of epochs for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "l1_weight = 0\n",
    "l2_weight = 0.00001\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=None,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "    \n",
    "Droput applied to input features (turning on/off some random pixels) may be also viewed as a form of data augmentation -- as we effectively create images that differ in some way from training one but also model is tasked to properly classify imperfect data-points.\n",
    "\n",
    "Your task in this exercise is to pick a random digit from MNIST dataset (use MNISTDataProvider) and corrupt it pixel-wise with different levels of probabilities $p_{d} \\in \\{0.9, 0.7, 0.5, 0.2, 0.1\\}$ (reminder, dropout probability is $1-p_d$) that is, for each pixel $x_{i,j}$ in image $\\mathbf{X} \\in \\mathbb{R}^{W\\times H}$:\n",
    "\n",
    "$\\begin{align}\n",
    "d_{i,j} & \\sim\\ \\mbox{Bernoulli}(p_{d}) \\\\\n",
    "x_{i,j} &=\n",
    "\\begin{cases}\n",
    "     0     & \\quad \\text{if } d_{i,j} = 0\\\\\n",
    "     x_{i,j}       & \\quad \\text{if } d_{i,j} = 1\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Plot the solution as a 2x3 grid of images for each $p_d$ scenario, at position (0, 0) plot an original (uncorrupted) image.\n",
    "\n",
    "Tip: You may use numpy.random.binomial function to draw samples from Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10a5bb710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD7CAYAAABOi672AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/LQKwMiFKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109de58d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "train_dp.reset()\n",
    "x, t = train_dp.next()\n",
    "img = x[0].reshape(28,28)\n",
    "pds = [0.9, 0.7, 0.5, 0.2, 0.1]\n",
    "imgs = [None] * (len(pds)+1)\n",
    "imgs[0] = img\n",
    "\n",
    "for i, pd in enumerate(pds):\n",
    "    d = rng.binomial(1, pd, img.shape)\n",
    "    imgs[i + 1] = d*img\n",
    "\n",
    "fig, ax = plt.subplots(2,3)\n",
    "ax[0, 0].imshow(numpy.rot90(numpy.rot90(imgs[0])), cmap=cm.Greys_r)\n",
    "ax[0, 1].imshow(scipy.ndimage.interpolation.rotate(imgs[1], angle), cmap=cm.Greys_r)\n",
    "ax[0, 2].imshow(imgs[2], cmap=cm.Greys_r)\n",
    "ax[1, 0].imshow(imgs[3], cmap=cm.Greys_r)\n",
    "ax[1, 1].imshow(imgs[4], cmap=cm.Greys_r)\n",
    "ax[1, 2].imshow(imgs[5], cmap=cm.Greys_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Implement Dropout \n",
    "\n",
    "Implement dropout regularisation technique. Then for the same initial configuration as in Exercise 1. investigate effectivness of different dropout rates applied to input features and/or hidden layers. Start with $p_{inp}=0.5$ and $p_{hid}=0.5$ and do some search for better settings.\n",
    "\n",
    "Implementation tips:\n",
    "* Add a function `fprop_dropout` to `mlp.layers.MLP` class which (on top of `inputs` argument) takes also dropout-related argument(s) and perform dropout forward propagation through the model.\n",
    "* One also would have to introduce required modificastions to `mlp.optimisers.SGDOptimiser.train_epoch()` function.\n",
    "* Design and implemnt dropout scheduler in a similar way to how learning rates are handled (that is, allowing for some implementation dependent schedule which is kept independent of implementation in `mlp.optimisers.SGDOptimiser.train()`). \n",
    "   +  For this exercise implement only fixed dropout scheduler - `DropoutFixed`, but implementation should allow to easily add other schedules in the future. \n",
    "   +  Dropout scheduler of any type should return a tuple of two numbers $(p_{inp},\\; p_{hid})$, the first one is dropout factor for input features (data-points), and the latter dropout factor for hidden layers (assumed the same for all hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.624. Accuracy is 8.60%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:(0.59, 0.59)\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 78.279. Accuracy is 13.60%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 56.603. Accuracy is 19.69%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 4 seconds. Training speed 431 pps. Validation speed 5874 pps.\n",
      "INFO:mlp.optimisers:(0.6799999999999999, 0.6799999999999999)\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 44.062. Accuracy is 28.70%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 52.269. Accuracy is 14.91%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 4 seconds. Training speed 442 pps. Validation speed 5817 pps.\n",
      "INFO:mlp.optimisers:(0.7699999999999999, 0.7699999999999999)\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 23.618. Accuracy is 49.50%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 10.529. Accuracy is 65.44%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 4 seconds. Training speed 448 pps. Validation speed 5718 pps.\n",
      "INFO:mlp.optimisers:(0.8599999999999999, 0.8599999999999999)\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 14.398. Accuracy is 61.80%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 17.474. Accuracy is 50.59%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 4 seconds. Training speed 453 pps. Validation speed 5680 pps.\n",
      "INFO:mlp.optimisers:(0.9499999999999998, 0.9499999999999998)\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 7.397. Accuracy is 75.80%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 3.310. Accuracy is 84.96%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 4 seconds. Training speed 447 pps. Validation speed 5798 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 3.987. Accuracy is 82.80%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 4.026. Accuracy is 83.75%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 4 seconds. Training speed 463 pps. Validation speed 5801 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 2.873. Accuracy is 86.70%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 5.428. Accuracy is 80.40%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 4 seconds. Training speed 448 pps. Validation speed 5672 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 3.563. Accuracy is 84.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 5.232. Accuracy is 81.53%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 4 seconds. Training speed 470 pps. Validation speed 5812 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 1.878. Accuracy is 90.30%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 4.139. Accuracy is 85.21%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 4 seconds. Training speed 458 pps. Validation speed 5831 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 2.479. Accuracy is 89.80%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 2.988. Accuracy is 88.62%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 4 seconds. Training speed 453 pps. Validation speed 5780 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.737. Accuracy is 95.40%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 5.232. Accuracy is 82.28%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 4 seconds. Training speed 427 pps. Validation speed 5662 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.927. Accuracy is 93.50%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 3.148. Accuracy is 88.93%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 4 seconds. Training speed 466 pps. Validation speed 5804 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.462. Accuracy is 96.50%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 2.968. Accuracy is 88.64%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 4 seconds. Training speed 463 pps. Validation speed 5870 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.247. Accuracy is 97.80%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 2.610. Accuracy is 90.15%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 4 seconds. Training speed 456 pps. Validation speed 5999 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.200. Accuracy is 98.10%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 2.735. Accuracy is 89.51%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 4 seconds. Training speed 456 pps. Validation speed 6070 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.183. Accuracy is 98.40%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 3.087. Accuracy is 88.27%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 4 seconds. Training speed 455 pps. Validation speed 5901 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.237. Accuracy is 97.80%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 2.648. Accuracy is 90.13%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 4 seconds. Training speed 449 pps. Validation speed 6076 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.067. Accuracy is 99.30%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 2.926. Accuracy is 89.76%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 4 seconds. Training speed 459 pps. Validation speed 5797 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.072. Accuracy is 99.00%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 2.495. Accuracy is 90.53%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 4 seconds. Training speed 453 pps. Validation speed 5957 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.020. Accuracy is 99.70%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 2.694. Accuracy is 89.95%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 4 seconds. Training speed 461 pps. Validation speed 5647 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.028. Accuracy is 99.70%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 2.538. Accuracy is 90.62%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 4 seconds. Training speed 451 pps. Validation speed 6099 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.005. Accuracy is 99.90%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 2.583. Accuracy is 90.15%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 4 seconds. Training speed 453 pps. Validation speed 6038 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.050. Accuracy is 99.20%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 2.979. Accuracy is 88.08%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 4 seconds. Training speed 449 pps. Validation speed 6001 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.030. Accuracy is 99.30%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 2.609. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 4 seconds. Training speed 449 pps. Validation speed 5987 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 2.608. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 4 seconds. Training speed 434 pps. Validation speed 5849 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 2.607. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 4 seconds. Training speed 442 pps. Validation speed 5913 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 2.606. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 4 seconds. Training speed 462 pps. Validation speed 5850 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 2.605. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 4 seconds. Training speed 476 pps. Validation speed 5823 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 2.605. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 4 seconds. Training speed 453 pps. Validation speed 5897 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 2.604. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 4 seconds. Training speed 444 pps. Validation speed 5950 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 2.604. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 4 seconds. Training speed 444 pps. Validation speed 5938 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 2.603. Accuracy is 90.33%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 4 seconds. Training speed 442 pps. Validation speed 5910 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 2.603. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 4 seconds. Training speed 457 pps. Validation speed 5791 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 2.602. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 4 seconds. Training speed 458 pps. Validation speed 5904 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 2.602. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 4 seconds. Training speed 458 pps. Validation speed 5888 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 2.601. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 4 seconds. Training speed 458 pps. Validation speed 5889 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 2.601. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 4 seconds. Training speed 464 pps. Validation speed 5841 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 2.600. Accuracy is 90.30%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 4 seconds. Training speed 458 pps. Validation speed 6012 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 2.600. Accuracy is 90.30%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 4 seconds. Training speed 454 pps. Validation speed 5881 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 2.600. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 4 seconds. Training speed 452 pps. Validation speed 5929 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 2.599. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 4 seconds. Training speed 439 pps. Validation speed 5664 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 2.599. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 4 seconds. Training speed 456 pps. Validation speed 5843 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 2.599. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 4 seconds. Training speed 458 pps. Validation speed 5950 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 2.598. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 4 seconds. Training speed 459 pps. Validation speed 5880 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 2.598. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 4 seconds. Training speed 457 pps. Validation speed 5843 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 2.598. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 4 seconds. Training speed 452 pps. Validation speed 5943 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 2.597. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 4 seconds. Training speed 455 pps. Validation speed 5776 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 2.597. Accuracy is 90.33%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 4 seconds. Training speed 456 pps. Validation speed 6037 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 2.597. Accuracy is 90.34%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 4 seconds. Training speed 462 pps. Validation speed 5861 pps.\n",
      "INFO:mlp.optimisers:(1, 1)\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 0.000. Accuracy is 100.00%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 2.596. Accuracy is 90.34%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 4 seconds. Training speed 521 pps. Validation speed 5903 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 89.86 %, cost (ce) is 2.598\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed, DropoutAnnealed\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 50\n",
    "l1_weight = 0.0\n",
    "l2_weight = 0.0\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.09)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed, DropoutFixed\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 20\n",
    "l1_weight = 0.0\n",
    "l2_weight = 0.0\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    dp_scheduler = DropoutFixed(0.5, 0.5)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.624. Accuracy is 8.60%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 110.679. Accuracy is 11.60%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 75.982. Accuracy is 10.63%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 4 seconds. Training speed 426 pps. Validation speed 6007 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 94.152. Accuracy is 14.10%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 113.869. Accuracy is 10.58%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 4 seconds. Training speed 437 pps. Validation speed 5797 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 106.026. Accuracy is 12.50%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 124.184. Accuracy is 10.90%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 4 seconds. Training speed 430 pps. Validation speed 5765 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 100.790. Accuracy is 8.80%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 110.388. Accuracy is 9.71%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 4 seconds. Training speed 435 pps. Validation speed 5934 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 103.153. Accuracy is 10.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 132.420. Accuracy is 14.47%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 4 seconds. Training speed 434 pps. Validation speed 5866 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 101.707. Accuracy is 10.50%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 66.753. Accuracy is 18.25%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 4 seconds. Training speed 440 pps. Validation speed 5770 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 96.033. Accuracy is 10.70%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 79.745. Accuracy is 9.83%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 4 seconds. Training speed 442 pps. Validation speed 5710 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 108.743. Accuracy is 7.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 125.786. Accuracy is 10.64%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 4 seconds. Training speed 438 pps. Validation speed 5637 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 103.159. Accuracy is 9.70%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 91.004. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 4 seconds. Training speed 445 pps. Validation speed 5892 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 100.349. Accuracy is 10.20%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 58.014. Accuracy is 13.96%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 4 seconds. Training speed 437 pps. Validation speed 5876 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 99.407. Accuracy is 11.60%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 85.234. Accuracy is 9.83%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 4 seconds. Training speed 439 pps. Validation speed 5945 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 96.056. Accuracy is 9.80%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 120.467. Accuracy is 9.61%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 4 seconds. Training speed 454 pps. Validation speed 5854 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 101.521. Accuracy is 9.80%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 136.409. Accuracy is 9.61%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 4 seconds. Training speed 439 pps. Validation speed 5913 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 105.833. Accuracy is 10.60%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 82.651. Accuracy is 9.74%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 4 seconds. Training speed 432 pps. Validation speed 5940 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 105.043. Accuracy is 11.30%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 50.088. Accuracy is 10.33%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 4 seconds. Training speed 443 pps. Validation speed 5782 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 109.862. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 50.533. Accuracy is 10.74%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 4 seconds. Training speed 437 pps. Validation speed 5823 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 104.472. Accuracy is 9.30%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 108.050. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 4 seconds. Training speed 430 pps. Validation speed 5921 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 109.311. Accuracy is 8.30%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 58.957. Accuracy is 10.64%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 4 seconds. Training speed 427 pps. Validation speed 5788 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 106.734. Accuracy is 9.20%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 43.715. Accuracy is 9.82%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 4 seconds. Training speed 432 pps. Validation speed 5905 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 104.666. Accuracy is 10.00%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 96.433. Accuracy is 9.61%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 4 seconds. Training speed 447 pps. Validation speed 5858 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 100.752. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 112.070. Accuracy is 9.61%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 4 seconds. Training speed 439 pps. Validation speed 5823 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 99.751. Accuracy is 12.20%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 169.791. Accuracy is 10.30%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 4 seconds. Training speed 436 pps. Validation speed 5762 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 106.216. Accuracy is 10.40%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 61.197. Accuracy is 9.61%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 4 seconds. Training speed 428 pps. Validation speed 5924 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 104.729. Accuracy is 10.00%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 73.648. Accuracy is 9.83%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 4 seconds. Training speed 426 pps. Validation speed 5922 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 104.022. Accuracy is 10.00%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 95.764. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 4 seconds. Training speed 434 pps. Validation speed 5890 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 106.615. Accuracy is 9.30%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 72.241. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 4 seconds. Training speed 440 pps. Validation speed 5961 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 104.488. Accuracy is 10.30%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 111.289. Accuracy is 9.15%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 4 seconds. Training speed 447 pps. Validation speed 5778 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 105.208. Accuracy is 10.40%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 79.775. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 4 seconds. Training speed 437 pps. Validation speed 5812 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 106.279. Accuracy is 8.70%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 80.684. Accuracy is 10.90%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 4 seconds. Training speed 443 pps. Validation speed 5850 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 105.465. Accuracy is 9.10%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 47.978. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 4 seconds. Training speed 431 pps. Validation speed 5863 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 31: Training cost (ce) is 113.835. Accuracy is 9.60%\n",
      "INFO:mlp.optimisers:Epoch 31: Validation cost (ce) is 58.332. Accuracy is 9.67%\n",
      "INFO:mlp.optimisers:Epoch 31: Took 4 seconds. Training speed 442 pps. Validation speed 5806 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 32: Training cost (ce) is 110.904. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 32: Validation cost (ce) is 151.682. Accuracy is 9.91%\n",
      "INFO:mlp.optimisers:Epoch 32: Took 4 seconds. Training speed 450 pps. Validation speed 5852 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 33: Training cost (ce) is 100.105. Accuracy is 11.80%\n",
      "INFO:mlp.optimisers:Epoch 33: Validation cost (ce) is 98.620. Accuracy is 9.67%\n",
      "INFO:mlp.optimisers:Epoch 33: Took 4 seconds. Training speed 449 pps. Validation speed 5841 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 34: Training cost (ce) is 101.088. Accuracy is 10.60%\n",
      "INFO:mlp.optimisers:Epoch 34: Validation cost (ce) is 64.396. Accuracy is 9.67%\n",
      "INFO:mlp.optimisers:Epoch 34: Took 4 seconds. Training speed 444 pps. Validation speed 6013 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 35: Training cost (ce) is 105.338. Accuracy is 10.50%\n",
      "INFO:mlp.optimisers:Epoch 35: Validation cost (ce) is 62.684. Accuracy is 9.91%\n",
      "INFO:mlp.optimisers:Epoch 35: Took 4 seconds. Training speed 444 pps. Validation speed 5824 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 36: Training cost (ce) is 105.031. Accuracy is 9.70%\n",
      "INFO:mlp.optimisers:Epoch 36: Validation cost (ce) is 60.535. Accuracy is 10.82%\n",
      "INFO:mlp.optimisers:Epoch 36: Took 4 seconds. Training speed 458 pps. Validation speed 5942 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 37: Training cost (ce) is 107.552. Accuracy is 10.00%\n",
      "INFO:mlp.optimisers:Epoch 37: Validation cost (ce) is 72.024. Accuracy is 9.61%\n",
      "INFO:mlp.optimisers:Epoch 37: Took 4 seconds. Training speed 437 pps. Validation speed 5775 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 38: Training cost (ce) is 104.858. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 38: Validation cost (ce) is 73.190. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 38: Took 4 seconds. Training speed 435 pps. Validation speed 6087 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 39: Training cost (ce) is 103.501. Accuracy is 9.60%\n",
      "INFO:mlp.optimisers:Epoch 39: Validation cost (ce) is 114.823. Accuracy is 9.67%\n",
      "INFO:mlp.optimisers:Epoch 39: Took 4 seconds. Training speed 476 pps. Validation speed 6059 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 40: Training cost (ce) is 100.093. Accuracy is 11.20%\n",
      "INFO:mlp.optimisers:Epoch 40: Validation cost (ce) is 80.011. Accuracy is 9.61%\n",
      "INFO:mlp.optimisers:Epoch 40: Took 4 seconds. Training speed 490 pps. Validation speed 6068 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 41: Training cost (ce) is 106.238. Accuracy is 9.50%\n",
      "INFO:mlp.optimisers:Epoch 41: Validation cost (ce) is 103.262. Accuracy is 10.64%\n",
      "INFO:mlp.optimisers:Epoch 41: Took 4 seconds. Training speed 504 pps. Validation speed 6045 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 42: Training cost (ce) is 103.337. Accuracy is 9.20%\n",
      "INFO:mlp.optimisers:Epoch 42: Validation cost (ce) is 105.970. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 42: Took 4 seconds. Training speed 503 pps. Validation speed 6064 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 43: Training cost (ce) is 101.684. Accuracy is 12.10%\n",
      "INFO:mlp.optimisers:Epoch 43: Validation cost (ce) is 74.980. Accuracy is 9.66%\n",
      "INFO:mlp.optimisers:Epoch 43: Took 4 seconds. Training speed 499 pps. Validation speed 6159 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 44: Training cost (ce) is 97.929. Accuracy is 10.50%\n",
      "INFO:mlp.optimisers:Epoch 44: Validation cost (ce) is 132.385. Accuracy is 9.67%\n",
      "INFO:mlp.optimisers:Epoch 44: Took 4 seconds. Training speed 495 pps. Validation speed 5946 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 45: Training cost (ce) is 110.403. Accuracy is 9.80%\n",
      "INFO:mlp.optimisers:Epoch 45: Validation cost (ce) is 136.354. Accuracy is 10.90%\n",
      "INFO:mlp.optimisers:Epoch 45: Took 4 seconds. Training speed 485 pps. Validation speed 5801 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 46: Training cost (ce) is 100.248. Accuracy is 10.10%\n",
      "INFO:mlp.optimisers:Epoch 46: Validation cost (ce) is 90.668. Accuracy is 9.67%\n",
      "INFO:mlp.optimisers:Epoch 46: Took 4 seconds. Training speed 492 pps. Validation speed 5975 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 47: Training cost (ce) is 114.497. Accuracy is 8.40%\n",
      "INFO:mlp.optimisers:Epoch 47: Validation cost (ce) is 89.305. Accuracy is 9.91%\n",
      "INFO:mlp.optimisers:Epoch 47: Took 3 seconds. Training speed 562 pps. Validation speed 6019 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 48: Training cost (ce) is 107.145. Accuracy is 10.30%\n",
      "INFO:mlp.optimisers:Epoch 48: Validation cost (ce) is 104.459. Accuracy is 9.90%\n",
      "INFO:mlp.optimisers:Epoch 48: Took 3 seconds. Training speed 567 pps. Validation speed 5922 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 49: Training cost (ce) is 102.723. Accuracy is 11.90%\n",
      "INFO:mlp.optimisers:Epoch 49: Validation cost (ce) is 63.697. Accuracy is 8.91%\n",
      "INFO:mlp.optimisers:Epoch 49: Took 3 seconds. Training speed 568 pps. Validation speed 5771 pps.\n",
      "INFO:mlp.optimisers:(0.5, 0.5)\n",
      "INFO:mlp.optimisers:Epoch 50: Training cost (ce) is 112.086. Accuracy is 8.80%\n",
      "INFO:mlp.optimisers:Epoch 50: Validation cost (ce) is 103.943. Accuracy is 10.09%\n",
      "INFO:mlp.optimisers:Epoch 50: Took 4 seconds. Training speed 553 pps. Validation speed 5761 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 9.74 %, cost (ce) is 104.595\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed, DropoutFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 50\n",
    "l1_weight = 0.0\n",
    "l2_weight = 0.0\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer = 1\n",
    "for i in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "    dp_scheduler = DropoutFixed(0.5, 0.5)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
