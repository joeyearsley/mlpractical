{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will experiment with different schedulers, seeing which can improve the standard coursework 1 setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    # training_size should equal batch size, as that is the amount for each epoch\n",
    "    lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.8\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                      min_derror_stop=.05, scale_by=0.05, zero_rate=0.5, patience = 10)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Pre-Training started...')\n",
    "tr_stats, valid_stats = optimiser.pretrain(model, train_dp, None, 0)\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "i = 0\n",
    "inputs=[]\n",
    "\n",
    "for x,t in train_dp:\n",
    "    inputs.append(x)\n",
    "    \n",
    "print inputs[0].shape\n",
    "print len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Pre-Training started...\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.194. Accuracy is 23.90%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.386. Accuracy is 56.30%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.002. Accuracy is 71.20%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.803. Accuracy is 78.60%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.684. Accuracy is 82.20%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.603. Accuracy is 84.20%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.544. Accuracy is 86.40%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.498. Accuracy is 87.10%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.461. Accuracy is 87.80%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.430. Accuracy is 88.50%\n",
      "INFO:mlp.optimisers:activations 3\n",
      "INFO:mlp.optimisers:activations2 2\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.373. Accuracy is 16.30%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.923. Accuracy is 32.70%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.540. Accuracy is 50.70%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.248. Accuracy is 63.20%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.043. Accuracy is 70.70%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.898. Accuracy is 75.50%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.794. Accuracy is 78.50%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.716. Accuracy is 80.80%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.657. Accuracy is 82.10%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.610. Accuracy is 83.70%\n",
      "INFO:mlp.optimisers:activations 4\n",
      "INFO:mlp.optimisers:activations2 3\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.347. Accuracy is 13.20%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.149. Accuracy is 22.80%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.921. Accuracy is 35.70%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.672. Accuracy is 48.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.443. Accuracy is 56.50%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.259. Accuracy is 62.60%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.120. Accuracy is 66.10%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.014. Accuracy is 69.40%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.933. Accuracy is 72.40%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.869. Accuracy is 73.70%\n",
      "INFO:mlp.optimisers:activations 5\n",
      "INFO:mlp.optimisers:activations2 4\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.279. Accuracy is 26.30%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.305. Accuracy is 23.68%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.903. Accuracy is 37.50%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 1.583. Accuracy is 46.51%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 5 seconds. Training speed 419 pps. Validation speed 3869 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.357. Accuracy is 58.00%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.283. Accuracy is 53.99%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 5 seconds. Training speed 418 pps. Validation speed 4161 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.085. Accuracy is 67.00%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.955. Accuracy is 77.80%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 5 seconds. Training speed 411 pps. Validation speed 4011 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.930. Accuracy is 69.90%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.860. Accuracy is 72.87%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 5 seconds. Training speed 407 pps. Validation speed 4042 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.815. Accuracy is 75.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.764. Accuracy is 80.09%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 5 seconds. Training speed 408 pps. Validation speed 3886 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.741. Accuracy is 76.60%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.736. Accuracy is 78.22%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 5 seconds. Training speed 409 pps. Validation speed 4088 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.694. Accuracy is 76.60%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.683. Accuracy is 78.67%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 5 seconds. Training speed 418 pps. Validation speed 4023 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.635. Accuracy is 82.20%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.637. Accuracy is 82.05%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 5 seconds. Training speed 405 pps. Validation speed 4037 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.584. Accuracy is 82.80%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.629. Accuracy is 79.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 5 seconds. Training speed 405 pps. Validation speed 3950 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.560. Accuracy is 83.00%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.575. Accuracy is 83.29%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 5 seconds. Training speed 411 pps. Validation speed 4119 pps.\n"
     ]
    }
   ],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Pre-Training started...')\n",
    "tr_stats, valid_stats = optimiser.pretrain_discriminative(model, train_dp, None)\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load -s Linear layers.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: option -s requires argument ( allowed: \"yns:r:\" )"
     ]
    }
   ],
   "source": [
    "#Run experiments using fixed, list, newBob and exponential use different scheduler each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) for initial model is 2.570. Accuracy is 9.28%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) for initial model is 2.554. Accuracy is 9.84%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.761. Accuracy is 85.84%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.262. Accuracy is 92.61%\n",
      "INFO:mlp.schedulers:0.495024916875\n",
      "INFO:mlp.optimisers:Epoch 2: Took 30 seconds. Training speed 1770 pps. Validation speed 5826 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.269. Accuracy is 92.06%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.228. Accuracy is 93.41%\n",
      "INFO:mlp.schedulers:0.490099336653\n",
      "INFO:mlp.optimisers:Epoch 3: Took 30 seconds. Training speed 1792 pps. Validation speed 5960 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.222. Accuracy is 93.56%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.192. Accuracy is 94.68%\n",
      "INFO:mlp.schedulers:0.485222766774\n",
      "INFO:mlp.optimisers:Epoch 4: Took 28 seconds. Training speed 1873 pps. Validation speed 5779 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.189. Accuracy is 94.45%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.171. Accuracy is 95.40%\n",
      "INFO:mlp.schedulers:0.480394719576\n",
      "INFO:mlp.optimisers:Epoch 5: Took 29 seconds. Training speed 1810 pps. Validation speed 5963 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.163. Accuracy is 95.26%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.150. Accuracy is 95.98%\n",
      "INFO:mlp.schedulers:0.0\n",
      "INFO:mlp.optimisers:Epoch 5: Took 29 seconds. Training speed 1842 pps. Validation speed 5911 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 95.62 %, cost (ce) is 0.151\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.818. Accuracy is 10.19%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.795. Accuracy is 10.85%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.712. Accuracy is 85.81%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.314. Accuracy is 90.75%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 30 seconds. Training speed 1777 pps. Validation speed 5847 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.273. Accuracy is 91.94%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.221. Accuracy is 93.84%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 29 seconds. Training speed 1839 pps. Validation speed 5819 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.224. Accuracy is 93.44%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.187. Accuracy is 95.09%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 28 seconds. Training speed 1875 pps. Validation speed 6017 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.189. Accuracy is 94.65%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.167. Accuracy is 95.40%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 29 seconds. Training speed 1842 pps. Validation speed 5963 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.161. Accuracy is 95.32%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.152. Accuracy is 95.77%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 30 seconds. Training speed 1761 pps. Validation speed 6169 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 95.25 %, cost (ce) is 0.159\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) for initial model is 2.397. Accuracy is 10.44%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) for initial model is 2.392. Accuracy is 10.00%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.728. Accuracy is 85.82%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.278. Accuracy is 91.83%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 30 seconds. Training speed 1771 pps. Validation speed 5966 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.273. Accuracy is 91.91%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.219. Accuracy is 93.82%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 30 seconds. Training speed 1764 pps. Validation speed 5862 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.227. Accuracy is 93.54%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.213. Accuracy is 93.96%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 29 seconds. Training speed 1838 pps. Validation speed 5935 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.224. Accuracy is 93.63%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.212. Accuracy is 94.00%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 29 seconds. Training speed 1840 pps. Validation speed 5901 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.224. Accuracy is 93.64%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.212. Accuracy is 94.00%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 30 seconds. Training speed 1736 pps. Validation speed 5926 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 93.81 %, cost (ce) is 0.220\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.597. Accuracy is 9.77%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.582. Accuracy is 10.28%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.774. Accuracy is 86.06%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.266. Accuracy is 92.17%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 29 seconds. Training speed 1860 pps. Validation speed 5655 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.270. Accuracy is 92.10%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.226. Accuracy is 93.34%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 27 seconds. Training speed 1941 pps. Validation speed 5843 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.225. Accuracy is 93.51%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.207. Accuracy is 94.00%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 29 seconds. Training speed 1857 pps. Validation speed 5881 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.195. Accuracy is 94.42%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.177. Accuracy is 95.14%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 28 seconds. Training speed 1873 pps. Validation speed 5867 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.174. Accuracy is 95.07%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.164. Accuracy is 95.71%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 29 seconds. Training speed 1834 pps. Validation speed 6120 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.159. Accuracy is 95.49%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.152. Accuracy is 95.89%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 28 seconds. Training speed 1924 pps. Validation speed 5823 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.147. Accuracy is 95.82%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.145. Accuracy is 96.06%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 28 seconds. Training speed 1882 pps. Validation speed 5979 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.139. Accuracy is 96.08%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.141. Accuracy is 96.24%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 28 seconds. Training speed 1905 pps. Validation speed 5999 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.134. Accuracy is 96.22%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.138. Accuracy is 96.34%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 29 seconds. Training speed 1832 pps. Validation speed 5944 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.130. Accuracy is 96.38%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.135. Accuracy is 96.45%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 29 seconds. Training speed 1858 pps. Validation speed 5982 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.128. Accuracy is 96.41%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.135. Accuracy is 96.36%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 29 seconds. Training speed 1863 pps. Validation speed 6001 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 95.97 %, cost (ce) is 0.140\n"
     ]
    }
   ],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 5\n",
    "cost = CECost()\n",
    "\n",
    "#Open file to save to\n",
    "shelve_p = shelve.open(\"learningRateExperiments\")\n",
    "\n",
    "stats = []\n",
    "#Go through for each learning rate\n",
    "for rate in xrange(1, 5):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "    \n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=0.5, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 2:\n",
    "        lr_scheduler = LearningRateFixed(learning_rate=0.5, max_epochs=max_epochs)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=0.5, max_epochs=max_epochs,\\\n",
    "                                      min_derror_stop=.05, scale_by=0.05, zero_rate=0.5, patience = 10)\n",
    "    elif rate == 4:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateList([0.5,0.45,0.4,0.35,0.3,0.25,0.2,0.15,0.1,0.05,0.005],max_epochs)\n",
    "    \n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "    \n",
    "    if rate == 1:\n",
    "        shelve_p['exponential'] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "    elif rate == 2:\n",
    "        shelve_p['fixed'] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "    elif rate == 3:\n",
    "        shelve_p['newbob'] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "    elif rate == 4:\n",
    "        shelve_p['list'] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "        \n",
    "        \n",
    "shelve_p.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(2.5698940660905487, 0.092780000000000057), (0.76107186078361544, 0.85839999999999927), (0.26941209019168572, 0.92061999999999999), (0.22168318644900226, 0.93561999999999956), (0.18934968575919447, 0.94454000000000027), (0.16270716969798743, 0.95259999999999989)], [(2.5535497258761031, 0.098400000000000001), (0.26169362217099285, 0.92610000000000003), (0.22757469120098908, 0.93410000000000004), (0.19216553403288314, 0.94679999999999997), (0.17132681558478102, 0.95399999999999996), (0.15021251853962736, 0.95979999999999999)], (0.15078776552708043, 0.95620000000000005))\n"
     ]
    }
   ],
   "source": [
    "print shelve_p['exponential']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
