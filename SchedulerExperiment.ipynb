{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will experiment with different schedulers, seeing which can improve the standard coursework 1 setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    # training_size should equal batch size, as that is the amount for each epoch\n",
    "    lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.8\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                      min_derror_stop=.05, scale_by=0.05, zero_rate=0.5, patience = 10)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Pre-Training started...\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 21.023. Accuracy is 0.80%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 11.791. Accuracy is 1.40%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 8.838. Accuracy is 1.20%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 7.072. Accuracy is 1.10%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 5.876. Accuracy is 0.70%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 5.021. Accuracy is 0.80%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 4.384. Accuracy is 0.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 3.893. Accuracy is 0.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 3.502. Accuracy is 0.90%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 3.182. Accuracy is 1.00%\n",
      "INFO:mlp.optimisers:activations 3\n",
      "INFO:mlp.optimisers:activations2 2\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 16.812. Accuracy is 11.10%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 8.182. Accuracy is 27.10%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 5.006. Accuracy is 37.40%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 3.509. Accuracy is 44.20%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 2.699. Accuracy is 50.40%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 2.216. Accuracy is 51.60%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.920. Accuracy is 52.00%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.675. Accuracy is 55.00%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 1.506. Accuracy is 56.40%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 1.381. Accuracy is 57.60%\n",
      "INFO:mlp.optimisers:activations 4\n",
      "INFO:mlp.optimisers:activations2 3\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 13.397. Accuracy is 12.40%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 7.578. Accuracy is 28.10%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 4.333. Accuracy is 37.30%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 2.644. Accuracy is 47.40%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.752. Accuracy is 55.10%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.251. Accuracy is 60.80%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.954. Accuracy is 64.90%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.770. Accuracy is 68.20%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.653. Accuracy is 70.10%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.574. Accuracy is 70.70%\n",
      "INFO:mlp.optimisers:activations 5\n",
      "INFO:mlp.optimisers:activations2 4\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.407. Accuracy is 9.70%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.413. Accuracy is 10.55%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.634. Accuracy is 50.00%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 1.033. Accuracy is 79.58%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 5 seconds. Training speed 376 pps. Validation speed 4004 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.874. Accuracy is 80.70%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.758. Accuracy is 80.32%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 5 seconds. Training speed 371 pps. Validation speed 4018 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.648. Accuracy is 85.10%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.545. Accuracy is 87.19%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 5 seconds. Training speed 405 pps. Validation speed 3970 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.537. Accuracy is 87.40%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.485. Accuracy is 87.76%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 5 seconds. Training speed 398 pps. Validation speed 4019 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.468. Accuracy is 88.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.452. Accuracy is 87.67%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 5 seconds. Training speed 423 pps. Validation speed 4057 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.424. Accuracy is 89.80%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.428. Accuracy is 88.08%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 5 seconds. Training speed 417 pps. Validation speed 4028 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.393. Accuracy is 89.70%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.421. Accuracy is 87.75%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 5 seconds. Training speed 416 pps. Validation speed 4096 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.365. Accuracy is 90.80%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.413. Accuracy is 87.93%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 5 seconds. Training speed 420 pps. Validation speed 3971 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.336. Accuracy is 91.50%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.388. Accuracy is 88.67%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 5 seconds. Training speed 381 pps. Validation speed 3991 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.322. Accuracy is 92.20%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.370. Accuracy is 89.39%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 5 seconds. Training speed 417 pps. Validation speed 4065 pps.\n"
     ]
    }
   ],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Pre-Training started...')\n",
    "tr_stats, valid_stats = optimiser.pretrain(model, train_dp, None)\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "i = 0\n",
    "inputs=[]\n",
    "\n",
    "for x,t in train_dp:\n",
    "    inputs.append(x)\n",
    "    \n",
    "print inputs[0].shape\n",
    "print len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
