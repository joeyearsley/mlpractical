{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will experiment with different schedulers, seeing which can improve the standard coursework 1 setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c149b2a49bfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialising data providers...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvalid_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTDataProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/josephyearsley/Documents/Edinburgh/Machine Learning Practical/mlpractical/repo-mlp/mlp/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dset, batch_size, max_num_batches, max_num_examples, randomize, augmentation, rng, conv_reshape)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_num_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_num_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mREAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    # training_size should equal batch size, as that is the amount for each epoch\n",
    "    lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.8\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                      min_derror_stop=.05, scale_by=0.05, zero_rate=0.5, patience = 10)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Pre-Training started...')\n",
    "tr_stats, valid_stats = optimiser.pretrain(model, train_dp, None, 0)\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "i = 0\n",
    "inputs=[]\n",
    "\n",
    "for x,t in train_dp:\n",
    "    inputs.append(x)\n",
    "    \n",
    "print inputs[0].shape\n",
    "print len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Pre-Training started...')\n",
    "tr_stats, valid_stats = optimiser.pretrain_discriminative(model, train_dp, None)\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run experiments using fixed, list, newBob and exponential use different scheduler each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 20\n",
    "cost = CECost()\n",
    "\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_p = shelve.open(\"learningRateExperiments\")\n",
    "\n",
    "\n",
    "options = {1: 'Exponential', 2: 'Fixed', 3: 'NewBob', 4: 'List'}\n",
    "\n",
    "stats = []\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,3):\n",
    "    #Go through for each learning rate\n",
    "    for rate in xrange(1, 5):\n",
    "\n",
    "        #Set here in case we alter it in a layer experiment\n",
    "        learning_rate = 0.5\n",
    "\n",
    "\n",
    "        train_dp.reset()\n",
    "        valid_dp.reset()\n",
    "        test_dp.reset()\n",
    "\n",
    "        logger.info(\"Starting \" + options[rate])\n",
    "\n",
    "        #define the model\n",
    "        model = MLP(cost=cost)\n",
    "        \n",
    "        if layer >= 0:\n",
    "            odim = 800\n",
    "            model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "        if layer >= 1:\n",
    "            odim = 600\n",
    "            model.add_layer(Sigmoid(idim=800, odim=600, irange=0.2, rng=rng))\n",
    "        elif layer == 2:\n",
    "            odim = 400\n",
    "            model.add_layer(Sigmoid(idim=600, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "        #Add output layer\n",
    "        model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "        #Set rate scheduler here\n",
    "        if rate == 1:\n",
    "            lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "        elif rate == 2:\n",
    "            lr_scheduler = LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "        elif rate == 3:\n",
    "            # define the optimiser, here stochasitc gradient descent\n",
    "            # with fixed learning rate and max_epochs\n",
    "            lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "        elif rate == 4:\n",
    "            # define the optimiser, here stochasitc gradient descent\n",
    "            # with fixed learning rate and max_epochs\n",
    "            \n",
    "            #Build this up instead\n",
    "            lr_scheduler = LearningRateList(learningList,max_epochs=max_epochs)\n",
    "\n",
    "        optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "        logger.info('Training started...')\n",
    "        tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "        logger.info('Testing the model on test set:')\n",
    "        tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "        logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "        #Append stats for all test\n",
    "        stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "        #Should save rate to specific dictionairy in pickle\n",
    "        shelve_p[options[rate]+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_p.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use 20 epochs, 0.01, 0.05, 0.1, 0.2, 0.5, \n",
    "Build the list from (start_rate / epochs), to have constant decrease otherwise it would return a 0 learning rate.\n",
    "Always start from 0.5\n",
    "Use both 1 and 2 layers and 3 layers as interested in different schedulers affects on depth of network.\n",
    "Save the exponential decline, as well as list and newBobs rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Open file to save to\n",
    "shelve_p = shelve.open(\"learningRateExperiments\")\n",
    "x = range(0,21)\n",
    "\n",
    "#Go through training list and output the stats in a graph.\n",
    "#Make sure you run the above cell first.\n",
    "train_cont = ['Exponential', 'Fixed', 'NewBob' ,'List']\n",
    "\n",
    "for i in xrange(0,3):\n",
    "    for idx,lists in enumerate(train_cont):\n",
    "        train_error = []\n",
    "        for inner_list in shelve_p[lists+str(i)][0]:\n",
    "            train_error.append(100-(inner_list[1]*100.))\n",
    "        plt.plot(x[4:len(shelve_p[lists+str(i)][0])], train_error[4:len(shelve_p[lists+str(i)][0])], lw=2, label=\"Scheduler: \"+ str(train_cont[idx]))\n",
    "\n",
    "\n",
    "    plt.title(\"Error Rates v Train Epochs for different learning schedulers with \"+str(i+1)+\" hidden layer\")\n",
    "    plt.xlabel('Training Epochs')\n",
    "    plt.legend\n",
    "    plt.legend(bbox_to_anchor=(1.6, 0.8))\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search for better l1 value, how?!\n",
    "\n",
    "0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Experiments/l1Experiment.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 20\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.001\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 1\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,3):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting\")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer >= 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer >= 1:\n",
    "        odim = 600\n",
    "        model.add_layer(Sigmoid(idim=800, odim=600, irange=0.2, rng=rng))\n",
    "    elif layer == 2:\n",
    "        odim = 400\n",
    "        model.add_layer(Sigmoid(idim=600, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser =   optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle\n",
    "    shelve_r['l1'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seemingly allows for more hidden layers to be added without the added overhead of having to run for more epochs.\n",
    "If we ran for more epochs it should do better? (Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search l2 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Experiments/l2Experiment.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 20\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.001\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 1\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,3):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer >= 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer >= 1:\n",
    "        odim = 600\n",
    "        model.add_layer(Sigmoid(idim=800, odim=600, irange=0.2, rng=rng))\n",
    "    elif layer == 2:\n",
    "        odim = 400\n",
    "        model.add_layer(Sigmoid(idim=600, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser =   optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['l2'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate dropout with both normal and annealed. Start at 0.5 and experiment for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Experiments/dropNExperiment.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutFixed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 20\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = DropoutFixed(0.5, 0.5)\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 1\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,3):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer >= 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer >= 1:\n",
    "        odim = 600\n",
    "        model.add_layer(Sigmoid(idim=800, odim=600, irange=0.2, rng=rng))\n",
    "    elif layer == 2:\n",
    "        odim = 400\n",
    "        model.add_layer(Sigmoid(idim=600, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser =   optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropN'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Experiments/dropAExperiment.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 20\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = DropoutAnnealed(0.5, 0.5, 0.005)\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 1\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,3):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer >= 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer >= 1:\n",
    "        odim = 600\n",
    "        model.add_layer(Sigmoid(idim=800, odim=600, irange=0.2, rng=rng))\n",
    "    elif layer == 2:\n",
    "        odim = 400\n",
    "        model.add_layer(Sigmoid(idim=600, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser =   optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['dropA'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load Experiments/noDropExp.py\n",
    "# %load Experiments/scheduler.py\n",
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential, LearningRateFixed, LearningRateList, LearningRateNewBob, DropoutAnnealed\n",
    "\n",
    "import numpy\n",
    "import logging\n",
    "import shelve\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "max_epochs = 20\n",
    "cost = CECost()\n",
    "learning_rate = 0.5;\n",
    "learningList = []\n",
    "decrement = (learning_rate/max_epochs)\n",
    "\n",
    "#Regulariser weights\n",
    "l1_weight = 0.000\n",
    "l2_weight = 0.000\n",
    "dp_scheduler = None\n",
    "\n",
    "#Build list once so we don't have to rebuild every time.\n",
    "for i in xrange(0,max_epochs):\n",
    "    #In this order so start learning rate is added\n",
    "    learningList.append(learning_rate)\n",
    "    learning_rate -= decrement\n",
    "\n",
    "\n",
    "\n",
    "#Open file to save to\n",
    "shelve_r = shelve.open(\"regExperiments\")\n",
    "\n",
    "stats = []\n",
    "rate = 1\n",
    "\n",
    "#For each number of layers, new model add layers.\n",
    "for layer in xrange(0,3):\n",
    "    #Set here in case we alter it in a layer experiment\n",
    "    learning_rate = 0.5\n",
    "\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "\n",
    "    logger.info(\"Starting \")\n",
    "\n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "\n",
    "    if layer >= 0:\n",
    "        odim = 800\n",
    "        model.add_layer(Sigmoid(idim=784, odim=odim, irange=0.2, rng=rng))\n",
    "    if layer >= 1:\n",
    "        odim = 600\n",
    "        model.add_layer(Sigmoid(idim=800, odim=600, irange=0.2, rng=rng))\n",
    "    elif layer == 2:\n",
    "        odim = 400\n",
    "        model.add_layer(Sigmoid(idim=600, odim=odim, irange=0.2, rng=rng))\n",
    "        \n",
    "    #Add output layer\n",
    "    model.add_layer(Softmax(idim=odim, odim=10, rng=rng))\n",
    "\n",
    "    #Set rate scheduler here\n",
    "    if rate == 1:\n",
    "        lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    elif rate == 3:\n",
    "        # define the optimiser, here stochasitc gradient descent\n",
    "        # with fixed learning rate and max_epochs\n",
    "        lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                          min_derror_stop=.05, scale_by=0.05, zero_rate=learning_rate, patience = 10)\n",
    "\n",
    "    optimiser =   optimiser = SGDOptimiser(lr_scheduler=lr_scheduler, \n",
    "                             dp_scheduler=dp_scheduler,\n",
    "                             l1_weight=l1_weight, \n",
    "                             l2_weight=l2_weight)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "\n",
    "    #Append stats for all test\n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))\n",
    "\n",
    "    #Should save rate to specific dictionairy in pickle, different key so same shelving doesn't matter\n",
    "    shelve_r['noDL'+str(layer)] = (tr_stats, valid_stats, (tst_cost, tst_accuracy))\n",
    "\n",
    "logger.info('Saving Data')\n",
    "shelve_r.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
