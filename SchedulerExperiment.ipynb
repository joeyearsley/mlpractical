{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will experiment with different schedulers, seeing which can improve the standard coursework 1 setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    # training_size should equal batch size, as that is the amount for each epoch\n",
    "    lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.8\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                      min_derror_stop=.05, scale_by=0.05, zero_rate=0.5, patience = 10)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Pre-Training started...')\n",
    "tr_stats, valid_stats = optimiser.pretrain(model, train_dp, None, 0)\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "i = 0\n",
    "inputs=[]\n",
    "\n",
    "for x,t in train_dp:\n",
    "    inputs.append(x)\n",
    "    \n",
    "print inputs[0].shape\n",
    "print len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Pre-Training started...\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.194. Accuracy is 23.90%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.386. Accuracy is 56.30%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.002. Accuracy is 71.20%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.803. Accuracy is 78.60%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.684. Accuracy is 82.20%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.603. Accuracy is 84.20%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.544. Accuracy is 86.40%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.498. Accuracy is 87.10%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.461. Accuracy is 87.80%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.430. Accuracy is 88.50%\n",
      "INFO:mlp.optimisers:activations 3\n",
      "INFO:mlp.optimisers:activations2 2\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.373. Accuracy is 16.30%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.923. Accuracy is 32.70%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.540. Accuracy is 50.70%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.248. Accuracy is 63.20%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.043. Accuracy is 70.70%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.898. Accuracy is 75.50%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.794. Accuracy is 78.50%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.716. Accuracy is 80.80%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.657. Accuracy is 82.10%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.610. Accuracy is 83.70%\n",
      "INFO:mlp.optimisers:activations 4\n",
      "INFO:mlp.optimisers:activations2 3\n",
      "INFO:mlp.optimisers:Max epochs 10\n",
      "INFO:mlp.optimisers:epochs 0\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.347. Accuracy is 13.20%\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.149. Accuracy is 22.80%\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.921. Accuracy is 35.70%\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.672. Accuracy is 48.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.443. Accuracy is 56.50%\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.259. Accuracy is 62.60%\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.120. Accuracy is 66.10%\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.014. Accuracy is 69.40%\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.933. Accuracy is 72.40%\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.869. Accuracy is 73.70%\n",
      "INFO:mlp.optimisers:activations 5\n",
      "INFO:mlp.optimisers:activations2 4\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.279. Accuracy is 26.30%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.305. Accuracy is 23.68%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.903. Accuracy is 37.50%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 1.583. Accuracy is 46.51%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 5 seconds. Training speed 419 pps. Validation speed 3869 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.357. Accuracy is 58.00%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.283. Accuracy is 53.99%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 5 seconds. Training speed 418 pps. Validation speed 4161 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.085. Accuracy is 67.00%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.955. Accuracy is 77.80%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 5 seconds. Training speed 411 pps. Validation speed 4011 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.930. Accuracy is 69.90%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.860. Accuracy is 72.87%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 5 seconds. Training speed 407 pps. Validation speed 4042 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.815. Accuracy is 75.50%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.764. Accuracy is 80.09%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 5 seconds. Training speed 408 pps. Validation speed 3886 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.741. Accuracy is 76.60%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.736. Accuracy is 78.22%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 5 seconds. Training speed 409 pps. Validation speed 4088 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.694. Accuracy is 76.60%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.683. Accuracy is 78.67%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 5 seconds. Training speed 418 pps. Validation speed 4023 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.635. Accuracy is 82.20%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.637. Accuracy is 82.05%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 5 seconds. Training speed 405 pps. Validation speed 4037 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.584. Accuracy is 82.80%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.629. Accuracy is 79.90%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 5 seconds. Training speed 405 pps. Validation speed 3950 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.560. Accuracy is 83.00%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.575. Accuracy is 83.29%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 5 seconds. Training speed 411 pps. Validation speed 4119 pps.\n"
     ]
    }
   ],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=100, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob, LearningRateFixed\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.05\n",
    "max_epochs = 10\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=600, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=600, odim=500, irange=0.2, rng=rng))\n",
    "model.add_layer(Sigmoid(idim=500, odim=300, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=300, odim=10, rng=rng))\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.05, max_epochs=max_epochs)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Pre-Training started...')\n",
    "tr_stats, valid_stats = optimiser.pretrain_discriminative(model, train_dp, None)\n",
    "logger.info('Training started...')\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
