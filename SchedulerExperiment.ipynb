{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will experiment with different schedulers, seeing which can improve the standard coursework 1 setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=1000, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateExponential\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.5\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    # training_size should equal batch size, as that is the amount for each epoch\n",
    "    lr_scheduler = LearningRateExponential(start_rate=learning_rate, max_epochs=max_epochs, training_size=100)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline experiment\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 800\n",
    "learning_rate = 0.8\n",
    "max_epochs = 30\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "for layer in xrange(1, 2):\n",
    "\n",
    "    train_dp.reset()\n",
    "    valid_dp.reset()\n",
    "    test_dp.reset()\n",
    "    \n",
    "    #define the model\n",
    "    model = MLP(cost=cost)\n",
    "    model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "    for i in xrange(1, layer):\n",
    "        logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "        model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "    model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "    # define the optimiser, here stochasitc gradient descent\n",
    "    # with fixed learning rate and max_epochs\n",
    "    lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                      min_derror_stop=.05, scale_by=0.05, zero_rate=0.5, patience = 10)\n",
    "    optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "    logger.info('Training started...')\n",
    "    tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "    logger.info('Testing the model on test set:')\n",
    "    tst_cost, tst_accuracy = optimiser.validate(model, test_dp)\n",
    "    logger.info('MNIST test set accuracy is %.2f %%, cost (%s) is %.3f'%(tst_accuracy*100., cost.get_name(), tst_cost))\n",
    "    \n",
    "    stats.append((tr_stats, valid_stats, (tst_cost, tst_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%autoreload` not found.\n",
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Stacking hidden layer (2)\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Max epochs 5\n",
      "INFO:mlp.optimisers:epochs 1\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 42.314. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 32.082. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 28.234. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 26.418. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:(10, 784)\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 26.363. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:activations 3\n",
      "INFO:mlp.optimisers:activations2 2\n",
      "INFO:mlp.optimisers:Max epochs 5\n",
      "INFO:mlp.optimisers:epochs 1\n",
      "INFO:mlp.optimisers:Running\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 8.998. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.455. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.269. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.212. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:(10, 600)\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.211. Accuracy is 0.00%\n",
      "INFO:mlp.optimisers:activations 4\n",
      "INFO:mlp.optimisers:activations2 3\n"
     ]
    }
   ],
   "source": [
    "#Baseline experiment\n",
    "%autoreload\n",
    "import numpy\n",
    "import logging\n",
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info('Initialising data providers...')\n",
    "\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=10, max_num_batches=10, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=10000, max_num_batches=-10, randomize=False)\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateNewBob\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "\n",
    "#some hyper-parameters\n",
    "nhid = 600\n",
    "learning_rate = 0.8\n",
    "max_epochs = 5\n",
    "cost = CECost()\n",
    "    \n",
    "stats = []\n",
    "layer=2\n",
    "\n",
    "train_dp.reset()\n",
    "\n",
    "#define the model\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=nhid, irange=0.2, rng=rng))\n",
    "for i in xrange(1, layer):\n",
    "    logger.info(\"Stacking hidden layer (%s)\" % str(i+1))\n",
    "    model.add_layer(Sigmoid(idim=nhid, odim=nhid, irange=0.2, rng=rng))\n",
    "model.add_layer(Softmax(idim=nhid, odim=10, rng=rng))\n",
    "\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs\n",
    "lr_scheduler = LearningRateNewBob(start_rate=learning_rate, max_epochs=max_epochs,\\\n",
    "                                  min_derror_stop=.05, scale_by=0.05, zero_rate=0.5, patience = 10)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.pretrain(model, train_dp, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
